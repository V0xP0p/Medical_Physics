Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Weakly Supervised Deep Learning for COVID-19 Infection Detection and Classification From CT Images,S. Hu; Y. Gao; Z. Niu; Y. Jiang; L. Li; X. Xiao; M. Wang; E. F. Fang; W. Menpes-Smith; J. Xia; H. Ye; G. Yang,"Radiology Department, Hospital of Wuhan Red Cross Society, Wuhan, China; Institute of Biomedical Engineering, University of Oxford, Oxford, U.K; Aladdin Healthcare Technologies Ltd., London, U.K; Hangzhou Ocean’s Smart Boya Company, Ltd., Hangzhou, China; Hangzhou Ocean’s Smart Boya Company, Ltd., Hangzhou, China; Aladdin Healthcare Technologies Ltd., London, U.K; Hangzhou Ocean’s Smart Boya Company, Ltd., Hangzhou, China; Department of Clinical Molecular Biology, University of Oslo, Oslo, Norway; Aladdin Healthcare Technologies Ltd., London, U.K; Radiology Department, Shenzhen Second People’s Hospital, Shenzhen, China; PET-CT Center, Hunan Cancer Hospital, Changsha, China; NHLI, Imperial College London, London, U.K",IEEE Access,06-Jul-20,2020,8,,118869,118883,"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.",2169-3536,,10.1109/ACCESS.2020.3005510,"European Research Council Innovative Medicines Initiative on Development of Therapeutics and Diagnostics Combatting Coronavirus Infections Award ’DRAGON: rapiD and secuRe AI imaging based diaGnosis, stratification, fOllow-up, and preparedness for coronavirus paNdemics’(grant numbers:H2020-JTI-IMI2 101005122); IIAT Hangzhou; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127422,COVID-19;deep learning;weakly supervision;CT~images;classification;convolutional neural network,COVID-19;Computed tomography;Lung;Deep learning;Hospitals,biochemistry;computerised tomography;diseases;image classification;learning (artificial intelligence);medical image processing;microorganisms,COVID-19 patient diagnosis;COVID-19 patient prognosis;clinical decision;supervised deep learning strategy;infection detection;COVID-19 infection;chest CT imaging;RT-PCR testing resources;reverse transcription polymerase chain reaction;coronavirus disease;CT images,,131,,37,CCBY,29-Jun-20,,,IEEE,IEEE Journals
Deep Learning-Based Image Segmentation on Multimodal Medical Imaging,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Radiology, Massachusetts General Hospital, Boston, MA, USA; Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA; Department of Radiology, Massachusetts General Hospital, Boston, MA, USA; Department of Radiology, Massachusetts General Hospital, Boston, MA, USA",IEEE Transactions on Radiation and Plasma Medical Sciences,01-Mar-19,2019,3,2,162,169,"Multimodality medical imaging techniques have been increasingly applied in clinical practice and research studies. Corresponding multimodal image analysis and ensemble learning schemes have seen rapid growth and bring unique value to medical applications. Motivated by the recent success of applying deep learning methods to medical image processing, we first propose an algorithmic architecture for supervised multimodal image analysis with cross-modality fusion at the feature learning level, classifier level, and decision-making level. We then design and implement an image segmentation system based on deep convolutional neural networks to contour the lesions of soft tissue sarcomas using multimodal images, including those from magnetic resonance imaging, computed tomography, and positron emission tomography. The network trained with multimodal images shows superior performance compared to networks trained with single-modal images. For the task of tumor segmentation, performing image fusion within the network (i.e., fusing at convolutional or fully connected layers) is generally better than fusing images at the network output (i.e., voting). This paper provides empirical guidance for the design and application of multimodal image analysis.",2469-7303,,10.1109/TRPMS.2018.2890359,National Institute of Biomedical Imaging and Bioengineering(grant numbers:1P41EB022544-01A1); National Institute on Aging(grant numbers:1RF1AG052653-01A1); National Institutes of Health(grant numbers:C06 CA059267); China Scholarship Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599078,Computed tomography (CT);convolutional neural network (CNN);magnetic resonance imaging (MRI);multimodal image;positron emission tomography (PET),Biomedical imaging;Image segmentation;Computed tomography;Magnetic resonance imaging;Feature extraction;Tumors,biomedical MRI;computerised tomography;feature extraction;image classification;image fusion;image segmentation;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography;tumours,multimodality medical imaging techniques;ensemble learning schemes;medical applications;deep learning methods;medical image processing;supervised multimodal image analysis;feature learning level;decision-making level;image segmentation system;deep convolutional neural networks;multimodal images;magnetic resonance imaging;single-modal images;image fusion,,101,,48,IEEE,01-Jan-19,,,IEEE,IEEE Journals
The Role of Imaging in the Detection and Management of COVID-19: A Review,D. Dong; Z. Tang; S. Wang; H. Hui; L. Gong; Y. Lu; Z. Xue; H. Liao; F. Chen; F. Yang; R. Jin; K. Wang; Z. Liu; J. Wei; W. Mu; H. Zhang; J. Jiang; J. Tian; H. Li,"School of Artificial IntelligenceUniversity of Chinese Academy of Sciences; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, School of Medicine and EngineeringBeihang University; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, School of Medicine and EngineeringBeihang University; School of Artificial IntelligenceUniversity of Chinese Academy of Sciences; College of Medicine and Biological Information Engineering School, Northeastern University, Shenyang, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Shanghai United Imaging Intelligence Co Ltd, Shanghai, China; Department of Biomedical Engineering, School of Medicine, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Radiology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China; Beijing Youan Hospital, Capital Medical University, Beijing, China; School of Artificial IntelligenceUniversity of Chinese Academy of Sciences; School of Artificial IntelligenceUniversity of Chinese Academy of Sciences; School of Artificial IntelligenceUniversity of Chinese Academy of Sciences; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, School of Medicine and EngineeringBeihang University; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, School of Medicine and EngineeringBeihang University; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, School of Medicine and EngineeringBeihang University; CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Radiology, Beijing Youan Hospital, Capital Medical University, Beijing, China",IEEE Reviews in Biomedical Engineering,22-Jan-21,2021,14,,16,29,"Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.",1941-1189,,10.1109/RBME.2020.2990959,"National Natural Science Foundation of China(grant numbers:81930053,91959130,81971776,81771924,81227901,81671851,81827808,81527805,81971691); National Key R&D Program of China(grant numbers:2017YFA0205200,2017YFC1308700,2017YFC1309100,2017YFA0700401,2016YFC0103803); Beijing Natural Science Foundation(grant numbers:L182061); Youth Innovation Promotion Association(grant numbers:2017175); Hubei COVID-19 Emergency; Strategic Priority Research Program(grant numbers:XDB32030200); Scientific Instrument R&D Program(grant numbers:YJKYYQ20170075); Chinese Academy of Sciences; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079648,COVID-19;imaging;chest CT;artificial intelligence,COVID-19;Computed tomography;Diseases;Lung;Hospitals;Biomedical imaging,biomedical MRI;biomedical ultrasonics;computerised tomography;diseases;epidemics;lung;medical image processing;microorganisms;molecular biophysics;positron emission tomography;reviews,COVID-19;coronavirus disease 2019;severe acute respiratory syndrome coronavirus 2;positron emission tomography;magnetic resonance imaging;quantitative image analysis;SARS-CoV-2;lung infection;pneumonia;computed tomography;lung ultrasound;artificial intelligence,"Artificial Intelligence;COVID-19;Humans;Lung;Positron Emission Tomography Computed Tomography;SARS-CoV-2;Tomography, X-Ray Computed;Ultrasonography",99,,90,IEEE,27-Apr-20,,,IEEE,IEEE Journals
f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"Inst. of Inf. & Comput. Sci., Utrecht Univ., Netherlands; Inst. of Inf. & Comput. Sci., Utrecht Univ., Netherlands; NA",IEEE Transactions on Medical Imaging,30-Nov-04,2004,23,12,1508,1516,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",1558-254X,,10.1109/TMI.2004.836872,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1362752,multimodal image registration;mutual information,Biomedical imaging;Image registration;Mutual information;Information theory;Computed tomography;Current measurement;Radioactive decay;Magnetic resonance;Gold;Robustness,positron emission tomography;biomedical MRI;computerised tomography;medical image processing;image registration;information theory,f-information measures;medical image registration;information theory;positron emission tomography;magnetic resonance imaging;computed tomography,"Algorithms;Artificial Intelligence;Brain;Brain;Brain;Computer Simulation;Diagnostic Imaging;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Magnetic Resonance Imaging;Models, Biological;Models, Statistical;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Positron-Emission Tomography;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction Technique;Tomography, X-Ray Computed",95,2,29,,30-Nov-04,,,IEEE,IEEE Journals
Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"Departments of Computational Mathematics, Science and Engineering, and Biomedical Engineering, Michigan State University, East Lansing, MI, USA; Department of Bio and Brain Engineering, Department of Mathematical Sciences, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA",Proceedings of the IEEE,27-Dec-19,2020,108,1,86,109,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",1558-2256,,10.1109/JPROC.2019.2936204,"National Research Foundation of Korea(grant numbers:2016R1A2B3008104); National Institutes of Health(grant numbers:R01 CA214981,R01 EB023618,U01 EB018753,U01 EB026977); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844696,Compressed sensing (CS);deep learning;dictionary learning (DL);efficient algorithms;image reconstruction;machine learning;magnetic resonance imaging (MRI);multilayer models;nonconvex optimization;positron emission tomography (PET);single-photon emission computed tomography (SPECT);sparse and low-rank models;structured models;transform learning;X-ray computed tomography (CT),Image reconstruction;Computed tomography;Mathematical model;Magnetic resonance imaging;Machine learning;X-ray imaging;Data models,biomedical MRI;computerised tomography;data acquisition;Fourier transforms;image reconstruction;image resolution;inverse transforms;iterative methods;learning (artificial intelligence);medical image processing,data-adaptive methods;medical image reconstruction;analytical methods;X-ray computed tomography;magnetic resonance imaging;MRI;iterative reconstruction methods;complete models;imaging system physics;image quality;regularization models;modified data acquisition methods;mathematical image models;adaptive models;low-rank models;machine learning techniques;filtered backprojection;inverse Fourier transform;sensor statistics;data-driven models,,62,,216,IEEE,19-Sep-19,,,IEEE,IEEE Journals
A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,Meei-Ling Jan; Keh-Shih Chuang; Guo-Wei Chen; Yu-Ching Ni; S. Chen; Chih-Hsien Chang; Jay Wu; Te-Wei Lee; Ying-Kai Fu,"Inst. of Nucl. Energy Res., Hsinchu, Taiwan; NA; NA; NA; NA; NA; NA; NA; NA",IEEE Transactions on Medical Imaging,05-Jul-05,2005,24,7,886,893,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.",1558-254X,,10.1109/TMI.2005.848617,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1461524,Image fusion;micro computed tomography (CT);micro positron emission tomography (PET);micro single-photon emission computed tomography (SPECT);registration,Computed tomography;Positron emission tomography;Animal structures;X-ray imaging;Imaging phantoms;Whole-body PET;Pharmaceuticals;Testing;Humans;Diseases,positron emission tomography;single photon emission computed tomography;image registration;medical image processing;diseases;genetics;biological organs;phantoms,three-dimensional image registration;automated image fusion;micro PET-CT-SPECT whole-body images;micro positron emission tomography;X-ray computed tomography;micro single-photon emission computed tomography;small animal imaging;pharmaceuticals;human disease;gene expression;tissues;organs;lesions localization;functional images;structural images;calibration phantom;spatial transformation matrix;Concorde microPET R4 scanner;Gamma Medica X-SPECT/CT system,"Algorithms;Animals;Artificial Intelligence;Image Enhancement;Image Enhancement;Image Interpretation, Computer-Assisted;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Imaging, Three-Dimensional;Male;Mice;Mice, Inbred C57BL;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Tomography, Emission-Computed;Tomography, Emission-Computed;Tomography, X-Ray Computed;Whole-Body Counting;Whole-Body Counting",46,,16,,05-Jul-05,,,IEEE,IEEE Journals
Co-Learning Feature Fusion Maps From PET-CT Images of Lung Cancer,A. Kumar; M. Fulham; D. Feng; J. Kim,"School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Department of Molecular Imaging, Royal Prince Alfred Hospital, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia",IEEE Transactions on Medical Imaging,31-Dec-19,2020,39,1,204,217,"The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modalityspecific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modalityspecific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p <; 0.05) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.",1558-254X,,10.1109/TMI.2019.2923601,Australian Research Council(grant numbers:DP160103675); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737963,Multi-modality imaging;deep learning;fusion learning;PET-CT,Computed tomography;Tumors;Lung;Image segmentation;Biomedical imaging;Cancer,cancer;computerised tomography;image classification;image fusion;image segmentation;learning (artificial intelligence);lung;medical image processing;neural nets;positron emission tomography;tumours,fusion requirements;PET-CT images;lung cancer;CNN;fusion baselines;recent PET-CT tumor segmentation methods;fusion maps;multimodality positron emission tomography;computed tomography images;computer-aided diagnosis applications;PET-CT image analysis;image analysis task;encode different information;high abnormal PET uptake;tumor detection;physiological PET uptake;complementary information;multimodality PET-CT;multimodality medical image analysis;modalityspecific features;spatially varying fusion map;modality;spatial locations;modalityspecific feature maps;complementary multimodality information,,40,,85,IEEE,17-Jun-19,,,IEEE,IEEE Journals
mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"School of Digital Media, Jiangnan University, Wuxi, China; School of Digital Media, Jiangnan University, Wuxi, China; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA; Department of Informatics, University of Leicester, Leicester, U.K.; School of Digital Media, Jiangnan University, Wuxi, China; School of Digital Media, Jiangnan University, Wuxi, China; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA; Philips Healthcare, Cleveland, OH, USA; Department of Biomedical Engineering, Case Western Reserve University, Cleveland, OH, USA; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA; Department of Biomedical Engineering, Case Western Reserve University, Cleveland, OH, USA; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA; Department of Radiology, University Hospitals Cleveland Medical Center, Cleveland, OH, USA; Department of Radiation Oncology, University Hospitals Cleveland Medical Center, Cleveland, OH, USA; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA; Department of Radiology, University Hospitals Cleveland Medical Center, Cleveland, OH, USA; School of Digital Media, Jiangnan University, Wuxi, China; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA; Case Center for Imaging Research, Case Western Reserve University, Cleveland, OH, USA",IEEE Transactions on Medical Imaging,02-Apr-20,2020,39,4,819,832,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",1558-254X,,10.1109/TMI.2019.2935916,"National Institutes of Health(grant numbers:R01CA196687); National Natural Science Foundation of China(grant numbers:61772241,61702225); Natural Science Foundation of Jiangsu Province(grant numbers:BK20160187); Six Talent Peaks Project in Jiangsu Province(grant numbers:2016-XYDXXJS-014); Science and Technology Demonstration Project of Social Development of Wuxi(grant numbers:WX18IVJN002); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804223,Synthetic CT generation;Dixon-based MR;abdomen;attenuation correction (AC);transfer fuzzy clustering (TFC);active learning-based classification (ALC),Computed tomography;Attenuation;Abdomen;Pelvis;Bones;Biological tissues,biomedical MRI;bone;computerised tomography;data acquisition;image reconstruction;image registration;learning (artificial intelligence);medical image processing;positron emission tomography,soft tissue;air;bone;fat;modified Dixon-based synthetic CT images;PET attenuation correction;TFC-ALC method;active learning-based classification;transfer fuzzy clustering;MR-CT pairs;synthetic CT generation methods;pelvis;abdomen;modified Dixon MR data,"Abdomen;Cluster Analysis;Fuzzy Logic;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Pelvis;Positron-Emission Tomography;Support Vector Machine;Tomography, X-Ray Computed",35,,67,IEEE,16-Aug-19,,,IEEE,IEEE Journals
Medical image segmentation based on multi-modal convolutional neural network: Study on image fusion schemes,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA; Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA; University of Pittsburgh, Pittsburgh, PA; Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA; Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA",2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018),24-May-18,2018,,,903,907,"Motivated by the recent success in applying deep learning for natural image analysis, we designed an image segmentation system based on deep Convolutional Neural Network (CNN) to detect the presence of soft tissue sarcoma from multi-modality medical images, including Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and Positron Emission Tomography (PET). Multi-modality imaging analysis using deep learning has been increasingly applied in the field of biomedical imaging and brought unique value to medical applications. However, it is still challenging to perform the multi-modal analysis owing to a major difficulty that is how to fuse the information derived from different modalities. There exist varies of possible schemes which are application-dependent and lack of a unified framework to guide their designs. Aiming at lesion segmentation with multi-modality images, we innovatively propose a conceptual image fusion architecture for supervised biomedical image analysis. The architecture has been optimized by testing different fusion schemes within the CNN structure, including fusing at the feature learning level, fusing at the classifier level, and the fusing at the decision-making level. It is found from the results that while all the fusion schemes outperform the single-modality schemes, fusing at the feature level can generally achieve the best performance in terms of both accuracy and computational cost, but can also suffer from the decreased robustness due to the presence of large errors in one or more image modalities.",1945-8452,978-1-5386-3636-7,10.1109/ISBI.2018.8363717,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8363717,multi-modal imaging analysis;deep learning;image fusion,Image fusion;Computed tomography;Biomedical imaging;Image segmentation;Tumors;Testing,biomedical MRI;computerised tomography;feature extraction;feedforward neural nets;image classification;image fusion;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography,medical image segmentation;multimodal convolutional neural network;image fusion schemes;deep learning;natural image analysis;image segmentation system;deep Convolutional Neural Network;multimodality medical images;Magnetic Resonance Imaging;Computed Tomography;Positron Emission Tomography;multimodality imaging analysis;biomedical imaging;medical applications;lesion segmentation;multimodality images;conceptual image fusion architecture;supervised biomedical image analysis;feature learning level;single-modality schemes;image modalities;soft tissue sarcoma presence detection;classifier level;decision-making level;CNN structure;MRI;CT;PET,,29,,16,,24-May-18,,,IEEE,IEEE Conferences
Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Sch. of Inf. Technol., Sydney Univ., NSW; Sch. of Inf. Technol., Sydney Univ., NSW; NA; NA",IEEE Transactions on Information Technology in Biomedicine,05-Mar-07,2007,11,2,161,169,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",1558-0032,,10.1109/TITB.2006.875669,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118185,Dual-modality positron emission tomography and computed tomography (PET/CT);fuzzy $C$-means cluster analysis;interactive three-dimensional (3-D) segmentation;multi-volume rendering;real-time volume rendering,Rendering (computer graphics);Visualization;Positron emission tomography;Computed tomography;Image segmentation;Application software;Surgery;Computer aided diagnosis;Table lookup;Bandwidth,computerised tomography;data visualisation;fuzzy set theory;image fusion;image segmentation;image texture;interactive systems;medical image processing;pattern clustering;positron emission tomography;rendering (computer graphics),real-time 3-D volume rendering;dual-modality PET-CT images;interactive fuzzy thresholding segmentation technique;three-dimensional data visualization;image-guided surgery;radiotherapy planning;computer-aided diagnosis;positron emission tomography;computed tomography;fuzzy C-means cluster analysis;real-time optimization;real-time fusion;interchangeability;texture-based volume rendering algorithm;rapid transfer capability;graphic hardware,"Algorithms;Artificial Intelligence;Computer Systems;Fuzzy Logic;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Pattern Recognition, Automated;Positron-Emission Tomography;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Tomography, X-Ray Computed;User-Computer Interface",28,1,33,,05-Mar-07,,,IEEE,IEEE Journals
3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Department of Electrical and Computer Engineering, University of Iowa, Iowa City, IA; Department of Radiation Oncology, University of Iowa, Iowa City, IA; Department of Electrical and Computer Engineering, University of Iowa, Iowa City, IA; Department of Radiation Oncology, University of Iowa, Iowa City, IA; Department of Radiation Oncology, University of Iowa, Iowa City, IA; Department of Radiation Oncology, University of Iowa, Iowa City, IA; Department of Electrical and Computer Engineering, University of Iowa, Iowa City, IA",2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018),24-May-18,2018,,,228,231,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",1945-8452,978-1-5386-3636-7,10.1109/ISBI.2018.8363561,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8363561,image segmentation;lung tumor segmentation;co-segmentation;fully convolutional networks;deep learning,Tumors;Computed tomography;Image segmentation;Biomedical imaging;Three-dimensional displays;Lung,biomedical MRI;cancer;computerised tomography;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,graph cut;co-segmentation model;final tumor segmentation results;lung cancer patients;3D fully convolutional networks;PET-CT images;positron emission tomography;computed tomography;dual-modality imaging;critical diagnostic information;automated accurate tumor delineation;tumor reading;semantic segmentation framework;probability maps;PET-CT scans;cancer diagnosis,,27,,16,,24-May-18,,,IEEE,IEEE Conferences
Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"TSI Dept., Ecole Nat. Superieure des Telecommun., Paris; NA; NA; NA; NA",IEEE Transactions on Medical Imaging,29-Jan-07,2007,26,2,164,178,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",1558-254X,,10.1109/TMI.2006.889712,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4077853,Anatomical constraints;free-form deformations (FFD);nonrigid registration;oncology;thoracic and abdominal computed tomography (CT);whole-body positron emission tomography (PET,Abdomen;Computed tomography;Whole-body PET;Positron emission tomography;Biomedical imaging;Medical diagnostic imaging;Anatomical structure;Anatomy;Image segmentation;Deformable models,gradient methods;image registration;image segmentation;medical image processing;optimisation;positron emission tomography,prior anatomical information;intensity-based nonrigid registration;thoracic CT;abdominal CT;computed tomography;18-FDG whole-body emission PET images;positron emission tomography;segmentation algorithm;optimization;gradient-based technique,"Algorithms;Artificial Intelligence;Fluorodeoxyglucose F18;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Positron-Emission Tomography;Radiography, Abdominal;Radiography, Thoracic;Radiopharmaceuticals;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Tomography, X-Ray Computed;Whole Body Imaging",24,,63,,29-Jan-07,,,IEEE,IEEE Journals
A Review of Deep-Learning-Based Approaches for Attenuation Correction in Positron Emission Tomography,J. S. Lee,"Department of Nuclear Medicine, Seoul National University College of Medicine, Seoul, South Korea",IEEE Transactions on Radiation and Plasma Medical Sciences,02-Mar-21,2021,5,2,160,184,"Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. PET AC based on computed tomography (CT) frequently results in artifacts in attenuation-corrected PET images, and these artifacts mainly originate from CT artifacts and PET-CT mismatches. The AC in PET combined with a magnetic resonance imaging (MRI) scanner (PET/MRI) is more complex than PET/CT, given that MR images do not provide direct information on high-energy photon attenuation. Deep-learning (DL)-based methods for the improvement of PET AC have received significant research attention as alternatives to conventional AC methods. Many DL studies were focused on the transformation of MR images into synthetic pseudo-CT or attenuation maps. Alternative approaches that are not dependent on the anatomical images (CT or MRI) can overcome the limitations related to current CT- and MRI-based ACs and allow for more accurate PET quantification in stand-alone PET scanners for the realization of low radiation doses. In this article, a review is presented on the limitations of the PET AC in current dual-modality PET/CT and PET/MRI scanners, in addition to the current status and progress of DL-based approaches, for the realization of improved performance of PET AC.",2469-7303,,10.1109/TRPMS.2020.3009269,"National Research Foundation of Korea (NRF); Korean Ministry of Science, ICT and Future Planning(grant numbers:NRF-2016R1A2B3014645); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143173,Attenuation correction (AC);deep neural network;PET/MRI;positron emission tomography (PET),Positron emission tomography;Computed tomography;Attenuation;Photonics;Magnetic resonance imaging;Biomedical imaging;Plasmas,biomedical MRI;image reconstruction;learning (artificial intelligence);medical image processing;positron emission tomography;reviews,PET AC;deep-learning-based approaches;positron emission tomography images;attenuation-corrected PET images;CT artifacts;PET-CT scanner;magnetic resonance imaging scanner;MR images;high-energy photon attenuation;conventional AC methods;synthetic pseudoCT;MRI-based AC;deep-learning-based method;review;DL-based approaches;PET-MRI scanners,,22,,233,CCBY,17-Jul-20,,,IEEE,IEEE Journals
Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Siemens Med. Solutions, Knoxville, TN; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven; Dept. of Nucl. Med., Katholieke Univ. Leuven, Leuven",IEEE Transactions on Medical Imaging,22-Dec-08,2009,28,1,67,73,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",1558-254X,,10.1109/TMI.2008.927349,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556632,Detection;emission tomography;maximum a posteriori;observer study;penalized-likelihood;positron emission tomography/computed tomography (PET/CT),Lesions;Whole-body PET;Humans;Image reconstruction;Smoothing methods;Computed tomography;Positron emission tomography;Reconstruction algorithms;Maximum likelihood detection;Spatial resolution,computerised tomography;image reconstruction;maximum likelihood estimation;medical image processing;object detection;positron emission tomography,MAP reconstruction;hot lesion detection;whole-body PET/CT;positron emission tomography imaging;maximum likelihood reconstruction;maximum a posteriori reconstruction;quantitative analysis;complex object dependent smoothing;hot spot detection;channelized Hotelling observer;multiple alternative forced choice approach;Gaussian smoothing kernel,"Algorithms;Artifacts;Artificial Intelligence;Humans;Imaging, Three-Dimensional;Models, Statistical;Normal Distribution;Pattern Recognition, Automated;Phantoms, Imaging;Positron-Emission Tomography;Positron-Emission Tomography;Probability;ROC Curve;Signal Processing, Computer-Assisted;Subtraction Technique;Tomography, Emission-Computed;Tomography, Emission-Computed",20,,28,,02-Jul-08,,,IEEE,IEEE Journals
Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Department of Computer Science, Nuclear Medicine, Friedrich-Alexander-University of Erlangen-Nuremberg (FAU), Pattern Recognition Lab, FAU, Erlangen, Erlangen, GermanyGermany; Department of Computer Science, Nuclear Medicine, Friedrich-Alexander-University of Erlangen-Nuremberg (FAU), Pattern Recognition Lab, FAU, Erlangen, Erlangen, GermanyGermany; Department of Computer Science, Friedrich-Alexander-University of Erlangen-Nuremberg (FAU), Pattern Recognition Lab, Erlangen Graduate School in Advanced Optical Technologies (SAOT), Erlangen, Erlangen, GermanyGermany",IEEE Transactions on Medical Imaging,29-Apr-10,2010,29,5,1140,1155,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",1558-254X,,10.1109/TMI.2010.2041358,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5430984,Adaptive binning;automatic parameter estimation;coincidence weighting;normalized mutual information;Parzen-window estimation,Image registration;Histograms;Kernel;Proposals;Background noise;Noise measurement;Mutual information;Image databases;Manuals;Biomedical imaging,biomedical MRI;computerised tomography;image registration;medical image processing;parameter estimation;positron emission tomography,automatic parameter selection;multimodal image registration;similarity measures;intensity distributions;parameter configuration;data-driven estimation schemes;parameter-free implementation;coincidence weighting scheme;Max-Lloyd requantization;normalized mutual information;CT-MR image pairs;PET-MR image pairs;MR-MR image pairs;RIRE 2.0 database;target registration error;distortion-corrected magnetic resonance images,"Algorithms;Animals;Artificial Intelligence;Diagnostic Imaging;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Subtraction Technique;Tomography, X-Ray Computed",17,,40,IEEE,15-Mar-10,,,IEEE,IEEE Journals
Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Université Paris-Est, ESIEE-Paris, LIGM, CNRS, France; Université Paris-Est, ESIEE-Paris, LIGM, CNRS, France; Université de Reims Champagne-Ardenne, CReSTIC, France; The Lymphoma Academic Research Organisation (LYSARC), Lyon, France; Université Paris-Est, ESIEE-Paris, LIGM, CNRS, France",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),19-Jun-17,2017,,,174,178,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",1945-8452,978-1-5090-1172-8,10.1109/ISBI.2017.7950495,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950495,Positron Emission Tomography;lymphoma segmentation;multimodality;machine learning;mathematical morphology;component-tree;random forest,Lesions;Image segmentation;Positron emission tomography;Computed tomography;Shape;Feature extraction,image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;random processes;tumours,multicentric PET-CT images;morphological hierarchy models;component-trees modeling PET-CT images;mixed spatial-spectral space;random forest classification;machine learning;PET lesion detection;hierarchical image models;user interaction;intensity-based strategies;spectral properties;tumour tissues;quantitative image interpretation;metabolic activity;positron emission tomography;PET-CT characteristics;automated 3D lymphoma lesion segmentation,,16,,29,,19-Jun-17,,,IEEE,IEEE Conferences
Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Radiotherapy Oncology Department, Nanfang Hospital, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China",IEEE Transactions on Medical Imaging,02-Apr-18,2018,37,4,977,987,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",1558-254X,,10.1109/TMI.2018.2790962,"National Natural Science Foundation of China(grant numbers:61471187,U1501256); Natural Science Foundation of Guangdong Province(grant numbers:2015A030313280); Guangdong Provincial Key Laboratory of Medical Image Processing(grant numbers:2014B030301042); Excellent Young Teachers Program of Guangdong Colleges; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8249878,CT prediction;nonlinear descriptor;low-rank approximation;KNN regression;PET attenuation correction,Computed tomography;Attenuation;Bones;Image segmentation;Biomedical imaging,biomedical MRI;brain;cancer;computerised tomography;dosimetry;feature extraction;image matching;image reconstruction;learning (artificial intelligence);medical image processing;positron emission tomography;radiation therapy;regression analysis,paired brain MRI;feature matching;learned nonlinear local descriptors;positron-emission tomography/magnetic resonance hybrid imaging systems;MR-based radiation therapy;high-energy photon attenuation information;tomography images;high-dimensional space;explicit feature map;pseudo computed tomography image;k-nearest neighbor regression;mean absolute error;Hounsfield units;peak signal-to-noise ratio;dose relative structure volume difference;noise figure 29.72 dB to 32.02 dB,"Algorithms;Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Models, Statistical;Nonlinear Dynamics;Tomography, X-Ray Computed",16,,36,IEEE,08-Jan-18,,,IEEE,IEEE Journals
Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Departments of Radiology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Radiation Oncology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Radiation Oncology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Radiation Oncology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Radiation Oncology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Radiation Oncology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Departments of Radiology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA",2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),11-Jul-19,2019,,,846,849,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",1945-8452,978-1-5386-3641-1,10.1109/ISBI.2019.8759301,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759301,CNNs;proportional hazards model;survival analysis;rectal cancer;tumor recurrence,Tumors;Cancer;Predictive models;Feature extraction;Hazards,biological organs;cancer;computerised tomography;feature extraction;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography;radiation therapy;regression analysis;tumours,survival analysis techniques whose performance;competitive prediction performance;random survival forests;Cox proportional hazards model;hand-crafted radiomic features;survival prediction models;locally advanced rectal cancer;rectal cancer patients;simulated imaging dataset;survival regression models;deep learning method;imaging features;data samples;survival analysis studies;predictive models;existing deep learning;fusing multimodal imaging data;deep learning techniques;recent radiomic studies;deep convolutional neural networks,,15,,19,,11-Jul-19,,,IEEE,IEEE Conferences
Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"School of Software, Central South University, Changsha, China; School of Software, Central South University, Changsha, China; School of Software, Central South University, Changsha, China; PET-CT Center, The Second Xiangya Hospital of Central South University, Changsha, China; School of Software, Central South University, Changsha, China; School of Software, Central South University, Changsha, China; School of Software, Central South University, Changsha, China; School of Software, Central South University, Changsha, China",IEEE Access,06-Jan-19,2019,7,,2348,2363,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",2169-3536,,10.1109/ACCESS.2018.2886198,Major Program of National Natural Science Foundation of China(grant numbers:71633006); National Natural Science Foundation of China(grant numbers:61672540); China Postdoctoral Science Foundation(grant numbers:2017M612586); Postdoctoral Science Foundation of Central South University(grant numbers:185684); Central South University(grant numbers:2018zzts615); “Mobile Health” Ministry of Education–China Mobile Joint Laboratory; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573767,Prostate cancer;fuzzy inference logic;intelligent medical system;big medical data decision-making model;fusion of multimodal medical data;machine-assisted diagnosis,Medical diagnostic imaging;Prostate cancer;Fuzzy logic,,,,13,,38,OAPA,12-Dec-18,,,IEEE,IEEE Journals
Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Student Member, IEEE, Biomedical Engineering Dept., The Ohio State University, Columbus, OH 43210, USA; Dept. of Biomedical Engineering, Lerner Research Institute, The Cleveland Clinic, Cleveland, OH 44195, USA. e-mail: walimbe.2@osu.edu; Student Member, IEEE, Dept. of Diagnostic Radiology, University of Maryland, Baltimore, MD 21201, USA; Dept. of Electrical & Computer Engineering, University of Maryland, College Park, MD 20740, USA. email: dandekar@ieee.org; VA Maryland Health Care System, Baltimore, MD 21201, USA. email: faaiza@siddiqui.md; Member, IEEE, Dept. of Diagnostic Radiology, University of Maryland, Baltimore, MD 21201, USA. phone: 410-706-8714; fax: 410-706-8724; e-mail: rshekhar@umm.edu",2006 International Conference of the IEEE Engineering in Medicine and Biology Society,15-Dec-16,2006,,,2799,2802,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",1557-170X,1-4244-0032-5,10.1109/IEMBS.2006.259236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462377,,Neoplasms;Computed tomography;Positron emission tomography;USA Councils;Whole-body PET;Interpolation;Biomedical engineering;Radiology;Quaternions;Image converters,cancer;computerised tomography;image registration;medical image processing;pneumodynamics;positron emission tomography;tumours,automated 3D elastic registration;tumor localization;whole-body PET-CT;combined scanner;metabolic information;anatomic information;patient breathing;involuntary movement;misalignment correction;radiotherapy treatment planning applications;patient treatment-monitoring,"Algorithms;Artificial Intelligence;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Neoplasms;Pattern Recognition, Automated;Positron-Emission Tomography;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Tomography, X-Ray Computed;Whole Body Imaging",11,1,4,,15-Dec-16,,,IEEE,IEEE Conferences
Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Department of EnTC, Vishwakarma Institute of Technology, Pune, India; Department of EnTC, Vishwakarma Institute of Technology, Pune, India; Department of EnTC, Vishwakarma Institute of Technology, Pune, India",2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA),25-Apr-19,2018,,,1,4,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",,978-1-5386-5257-2,10.1109/ICCUBEA.2018.8697386,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697386,Linear Regression;Machine Learning algorithm;Alzheimer's disease symptoms;Random Forest;Gradient Boosting Algorithm;Neural Network,Machine learning algorithms;Alzheimer's disease;Support vector machines;Boosting;Classification algorithms,biomedical MRI;brain;cognition;diseases;learning (artificial intelligence);medical image processing;neurophysiology;psychology,behavioral problems;psychological problems;neuroimaging technologies;Alzheimer's disease;machine learning;medical fields;medical technologies;chronic condition;brain cells;memory enervation;cognitive mental problems;confusion;forgetfulness;neuroimaging techniques;MRI;CT;PET;EEG,,11,,9,,25-Apr-19,,,IEEE,IEEE Conferences
Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; PET-CT/MRI Center and Molecular Imaging Center, Wuhan University Renmin Hospital, Wuhan, China; MI Research and Development Division, Neusoft Medical Systems Company, Ltd., Shenyang, China; MI Research and Development Division, Neusoft Medical Systems Company, Ltd., Shenyang, China; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",IEEE Transactions on Radiation and Plasma Medical Sciences,02-Mar-21,2021,5,2,213,223,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",2469-7303,,10.1109/TRPMS.2020.3025071,"National Natural Science Foundation of China(grant numbers:61871371,81830056,61671441); Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX01); ZJLab; Shanghai Center for Brain Science and Brain-Inspired Technology, Natural Science Foundation of Liaoning Province of China(grant numbers:20170540321); Science and Technology Planning Project of Guangdong Province(grant numbers:2017B020227012,2018B010109009); Basic Research Program of Shenzhen(grant numbers:JCYJ20180507182400762); Youth Innovation Promotion Association Program of Chinese Academy of Sciences(grant numbers:2019351); NIH/NCI(grant numbers:R01CA233888,R01CA237267); NIH/NIBIB(grant numbers:R01EB026646); Scientific and Technical Innovation 2030-“New Generation Artificial Intelligence” Project(grant numbers:2020AAA0104100,2020AAA0104105); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201109,Deep learning;image quality;low-dose positron emission tomography (PET);task-specific initialization;transfer learning,Three-dimensional displays;Two dimensional displays;Convolution;Positron emission tomography;Noise reduction;Deconvolution;Biomedical imaging,computerised tomography;edge detection;image denoising;learning (artificial intelligence);medical image processing;positron emission tomography,PT-WGAN framework;low-dose PET images;PT-WGAN using trainable parameters;suppress image noise;image fidelity;parameter-transferred Wasserstein generative adversarial network;low-dose PET image denoising;PET-associated radiation dose;resultant images,,9,,46,IEEE,21-Sep-20,,,IEEE,IEEE Journals
Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI 48202, USA; Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI 48202, USA; Karmanos Cancer Institute, Wayne State University, Detroit, MI 48202, USA; Karmanos Cancer Institute, Wayne State University, Detroit, MI 48202, USA; Department of Medicine, Wayne State University, Detroit, MI 48202, USA; Karmanos Cancer Institute, Wayne State University, Detroit, MI 48202, USA; Department of Pediatrics, Wayne State University, Detroit, MI 48202, USA",2006 International Conference of the IEEE Engineering in Medicine and Biology Society,15-Dec-16,2006,,,2320,2323,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",1557-170X,1-4244-0032-5,10.1109/IEMBS.2006.259238,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462257,,Lung neoplasms;Positron emission tomography;Computed tomography;Biomedical imaging;Medical diagnostic imaging;Inspection;Computer errors;Information resources;Software prototyping;Data mining,cancer;computerised tomography;fuzzy logic;image segmentation;lung;medical diagnostic computing;positron emission tomography;tumours,computerized lung tumor detection;computer tomography;positron emission tomography;tumor diagnosis;computer aided diagnosis software;attenuation-corrected PET;optimal thresholding;multiple gray-level thresholding;fuzzy logic based approach,"Algorithms;Artificial Intelligence;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Lung Neoplasms;Pattern Recognition, Automated;Positron-Emission Tomography;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Tomography, X-Ray Computed",9,,11,,15-Dec-16,,,IEEE,IEEE Conferences
Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Electrical and Electronics Engineering Deparment, Dokuz Eylül University, Buca, Izmir",IEEE Transactions on Visualization and Computer Graphics,01-Jan-15,2015,21,2,174,187,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",1941-0506,,10.1109/TVCG.2014.2359462,The Scientific and Technological Research Council of Turkey TUBITAK(grant numbers:112E032); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6908014,Brushlets;Texture Analysis;Transfer Functions;Volume Rendering;Abdominal Imaging;Brushlets;texture analysis;transfer functions;volume rendering;abdominal imaging,Image reconstruction;Rendering (computer graphics);Transfer functions;Transforms;Histograms;Noise;Frequency-domain analysis,biological organs;biomedical MRI;computerised tomography;image texture;learning (artificial intelligence);medical image processing;positron emission tomography;rendering (computer graphics);transfer functions,brushlet based 3D textures;transfer function specification;abdominal organs;direct volume rendering;brushlet expansion;high frequency textured structures;low frequency textured structures;transform domain;expert based manual;atlas;machine learning based automatic;complex brushlet coefficients;CT;MR;PET;3D rendering quality;TF specification techniques,,9,,50,IEEE,23-Sep-14,,,IEEE,IEEE Journals
MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network,J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang,"Electronic Information School, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; School of Electronic and Information, Zhongyuan University of Technology, Zhengzhou, China",IEEE Access,26-Mar-20,2020,8,,55145,55157,"In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.",2169-3536,,10.1109/ACCESS.2020.2982016,National Natural Science Foundation of China(grant numbers:61903279); Science and Technology Innovation Team of Colleges and Universities in Henan Province(grant numbers:18IRTSTHN013); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042283,Medical image fusion;generative adversarial network;different resolutions;end-to-end;unified method,Image fusion;Image resolution;Medical diagnostic imaging;Deep learning;Generative adversarial networks;Magnetic resonance imaging,biomedical MRI;image fusion;image resolution;medical image processing;neural nets;positron emission tomography;single photon emission computed tomography,CT-SPECT;MRI-SPECT;MRI-PET;end-to-end model;medical image fusion;source images;MGMDcGAN;multigenerator multidiscriminator conditional generative adversarial network;functional information;structural information,,9,,48,CCBY,19-Mar-20,,,IEEE,IEEE Journals
Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China; Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China; Shanghai United Imaging Intelligence Co., Ltd, Shanghai, China",2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),11-Jul-19,2019,,,1096,1099,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",1945-8452,978-1-5386-3641-1,10.1109/ISBI.2019.8759529,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759529,image synthesis;unpaired data;mutual information;adversarial learning;cross modality,Computed tomography;Shape;Image generation;Bones;Biomedical imaging;Image resolution;Attenuation,biomedical MRI;computerised tomography;image matching;image reconstruction;image registration;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;radiation therapy,CT synthesis;explicit structural constrained adversarial learning;medical imaging;PET-MR attenuation correction;MRI-guided radiation therapy;CT images;tissue density properties;image synthesis techniques;superior ability;image mapping;deep-learning-based synthesis methods;paired data;cycle-consistent adversarial networks;Cycle-GAN;medical images;unpaired data;cycle consistency loss;indirect structural similarity;synthesized images;anatomical structures;synthesized results;mutual information loss;synthesized CT image;shape consistency information;synthesis result;unpaired training images,,8,,8,,11-Jul-19,,,IEEE,IEEE Conferences
Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Department of Computer Science, Technische Universität München, Munich, Germany; Department of Nuclear Medicine, Ruijin Hospital, Shanghai Jiaotong University, School of Medicine, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Nuclear Medicine, Ruijin Hospital, Shanghai Jiaotong University, School of Medicine, Shanghai, China; Department Nuclear Medicine, Lab for Artificial Intelligence & Translational Theranostics, University of Bern, Bern, Switzerland; Department of Computer Science, Technische Universität München, Munich, Germany",IEEE Journal of Biomedical and Health Informatics,03-Sep-20,2020,24,9,2599,2608,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",2168-2208,,10.1109/JBHI.2020.2972694,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8988186,Coarse-to-fine adversarial network;multi-zone uncertainty estimate;medical image segmentation;Monte Carlo dropout,Uncertainty;Lesions;Image segmentation;Computed tomography;Diseases;Semantics;Biomedical imaging,backpropagation;cancer;image segmentation;learning (artificial intelligence);medical image processing;Monte Carlo methods;positron emission tomography;statistical analysis;tumours,CT-PET Images;NK-T-cell lymphoma segmentation;state-of-the-art deep-learning based segmentation model;effective lesion-zone;uncertainty criteria;medical image segmentation;deep networks;Monte Carlo dropout technique;global features;adversarial segmentation network;segmentation neural network;coarse-to-fine approach;ENKL segmentation problem;computer-aided diagnosis systems;treatment planning;clinical decision support;ENKL lesions;low survival rate;rare disease;zone-based uncertainty analysis;coarse-to-fine adversarial networks,"Algorithms;Humans;Image Processing, Computer-Assisted;Lymphoma, T-Cell;Neural Networks, Computer;Positron Emission Tomography Computed Tomography;Uncertainty",8,,39,IEEE,10-Feb-20,,,IEEE,IEEE Journals
Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),16-Jun-16,2016,,,743,746,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",1945-8452,978-1-4799-2349-6,10.1109/ISBI.2016.7493373,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493373,CT prediction;local descriptor learning;low-rank approximation;PET attenuation correction,Computed tomography;Magnetic resonance imaging;Training;Attenuation;Learning systems;Manifolds,biomedical MRI;computerised tomography;learning (artificial intelligence);medical image processing;radiation therapy;regression analysis,CT image prediction;MRI data;KNN regression;learned local descriptors;attenuation correction;PET-MR hybrid imaging systems;dose planning;MR based radiation therapy;supervised descriptor learning;low rank approximation;manifold regularization,,8,,13,,16-Jun-16,,,IEEE,IEEE Conferences
Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"University of Trento & Fondazione Bruno Kessler, Trento, Italy; University of Trento & Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy",2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB),08-Aug-19,2019,,,1,8,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",,978-1-7281-1462-0,10.1109/CIBCB.2019.8791473,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8791473,Radiomics;Deep Learning;Integration,Feature extraction;Computed tomography;Cancer;Pipelines;Data models,biological organs;cancer;computerised tomography;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);medical image processing;positron emission tomography;tumours,CT-PET cancer dataset;RADLER;tumor mass;convolutional neural network layers;deep learning;feature extractors;positron emission tomography imaging;computed tomography;head-and-neck squamous cell carcinoma;pattern recognition;image mode;internal transfer learning;neck tumor stage diagnosis;softmax classification layer;automatic image interpretation;machine learning framework;bioimaging diagnostic systems;ophthalmic imaging;histo-pathology slides;radiology scans;image streams;artificial intelligence;cancer bioimaging;radiomics features,,8,,54,,08-Aug-19,,,IEEE,IEEE Conferences
Predicting Lymph Node Metastasis in Head and Neck Cancer by Combining Many-objective Radiomics and 3-dimensioal Convolutional Neural Network through Evidential Reasoning,Z. Zhou; L. Chen; D. Sher; Q. Zhang; J. Shah; N. -L. Pham; S. Jiang; J. Wang,"University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA; State Key Lab of Biotherapy and Cancer, West China Hospital, Sichuan University, Chengdu, China; University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas Southwestern Medical Center, Dallas, TX, 75390, USA",2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),28-Oct-18,2018,,,1,4,"Lymph node metastasis (LNM) is a significant prognostic factor in patients with head and neck cancer, and the ability to predict it accurately is essential for treatment optimization. PET and CT imaging are routinely used for LNM identification. However, uncertainties of LNM always exist especially for small size or reactive nodes. Radiomics and deep learning are the two preferred imaging-based strategies for node malignancy prediction. Radiomics models are built based on handcrafted features, and deep learning can learn the features automatically. We proposed a hybrid predictive model that combines many-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural network (3D-CNN) through evidential reasoning (ER) approach. To build a more reliable model, we proposed a new many-objective radiomics model. Meanwhile, we designed a 3D-CNN that fully utilizes spatial contextual information. Finally, the outputs were fused through the ER approach. To study the predictability of the two modalities, three models were built for PET, CT, and PET& CT. The results showed that the model performed best when the two modalities were combined. Moreover, we showed that the quantitative results obtained from the hybrid model were better than those obtained from MO-radiomics and 3D-CNN.",1558-4615,978-1-5386-3646-6,10.1109/EMBC.2018.8513070,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513070,,Predictive models;Computed tomography;Feature extraction;Cancer;Lymph nodes;Erbium;Training,cancer;computerised tomography;convolution;feature extraction;feedforward neural nets;image classification;inference mechanisms;learning (artificial intelligence);medical image processing;optimisation;tumours,LNM identification;deep learning;node malignancy prediction;handcrafted features;hybrid predictive model;evidential reasoning approach;many-objective radiomics model;3D-CNN;lymph node metastasis;neck cancer;treatment optimization;CT imaging;convolutional neural network;prognostic factor;imaging-based strategies;PET imaging;head cancer,"Head and Neck Neoplasms;Humans;Lymph Nodes;Lymphatic Metastasis;Neural Networks, Computer;Positron Emission Tomography Computed Tomography",8,,17,IEEE,28-Oct-18,,,IEEE,IEEE Conferences
Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Max Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Department of Engineering Science, University of Oxford, OX1 3PJ, UK; Max Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany",2009 IEEE Conference on Computer Vision and Pattern Recognition,18-Aug-09,2009,,,186,193,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",1063-6919,978-1-4244-3992-8,10.1109/CVPR.2009.5206840,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206840,,Image registration;Pixel;Histograms;Biomedical imaging;Computed tomography;Mutual information;Bones;Maximum likelihood estimation;Biology;Cybernetics,image registration;learning (artificial intelligence);medical image processing,multimodal 3D image registration;medical imaging;imaging devices;floating image;reference image;a priori fixed similarity criterion;max-margin structured output learning method;standard rigid registration algorithm;multimodal medical image registration,,7,2,13,,18-Aug-09,,,IEEE,IEEE Conferences
Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung Tumor Segmentation,X. Fu; L. Bi; A. Kumar; M. Fulham; J. Kim,"School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; Department of Molecular Imaging, Royal Prince Alfred Hospital, Camperdown, NSW, Australia; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia",IEEE Journal of Biomedical and Health Informatics,03-Sep-21,2021,25,9,3507,3516,"Multimodal positron emission tomography-computed tomography (PET-CT) is used routinely in the assessment of cancer. PET-CT combines the high sensitivity for tumor detection of PET and anatomical information from CT. Tumor segmentation is a critical element of PET-CT but at present, the performance of existing automated methods for this challenging task is low. Segmentation tends to be done manually by different imaging experts, which is labor-intensive and prone to errors and inconsistency. Previous automated segmentation methods largely focused on fusing information that is extracted separately from the PET and CT modalities, with the underlying assumption that each modality contains complementary information. However, these methods do not fully exploit the high PET tumor sensitivity that can guide the segmentation. We introduce a deep learning-based framework in multimodal PET-CT segmentation with a multimodal spatial attention module (MSAM). The MSAM automatically learns to emphasize regions (spatial areas) related to tumors and suppress normal regions with physiologic high-uptake from the PET input. The resulting spatial attention maps are subsequently employed to target a convolutional neural network (CNN) backbone for segmentation of areas with higher tumor likelihood from the CT image. Our experimental results on two clinical PET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma (STS) validate the effectiveness of our framework in these different cancer types. We show that our MSAM, with a conventional U-Net backbone, surpasses the state-of-the-art lung tumor segmentation approach by a margin of 7.6% in Dice similarity coefficient (DSC).",2168-2208,,10.1109/JBHI.2021.3059453,"Australian Research Council(grant numbers:DP170104304,IC170100022); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354983,"Convolutional Neural Network (CNN), Multimodal Image Segmentation, Positron Emission Tomography;Computed Tomography (PET-CT)",Tumors;Computed tomography;Image segmentation;Positron emission tomography;Feature extraction;Imaging;Sensitivity,cancer;computerised tomography;image segmentation;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,clinical PET-CT datasets;state-of-the-art lung tumor segmentation approach;multimodal spatial attention module;multimodal PET-CT lung tumor segmentation;multimodal positron emission tomography-computed tomography;high PET tumor sensitivity;convolutional neural network;soft tissue sarcoma;conventional U-Net backbone;Dice similarity coefficient,"Attention;Carcinoma, Non-Small-Cell Lung;Humans;Lung Neoplasms;Neural Networks, Computer;Positron Emission Tomography Computed Tomography",6,,41,IEEE,16-Feb-21,,,IEEE,IEEE Journals
Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu, Japan; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu, Japan; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu, Japan",IEEE Access,14-Apr-20,2020,8,,63077,63089,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",2169-3536,,10.1109/ACCESS.2020.2984804,National Natural Science Foundation of China(grant numbers:61872075); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9052732,PET/CT images;3D image registration;unsupervised registration;metabolic constraint;convolutional neural network,Three-dimensional displays;Anatomical structure;Computed tomography;Image registration;Bladder;Mutual information;Cancer,biochemistry;brain;cancer;computerised tomography;convolutional neural nets;image registration;liver;medical image processing;positron emission tomography;unsupervised learning,displacement vector field;3D unsupervised registration network;unsupervised 3D PET-CT image registration;3D CT image patches;liver;hypermetabolic regions;registration accuracy;bladder;brain;early stage cancer diagnosis;blurred PET images;anatomical structure;high-resolution CT images;metabolic constraint function;MC-MDS Net;whole-body PET-CT images;loss function;frequency domain similarity;PET image patches;convolutional neural networks;hypermetabolic anatomical structures;heart,,6,,30,CCBY,01-Apr-20,,,IEEE,IEEE Journals
Deep learning-guided attenuation and scatter correction without using anatomical images in brain PET/MRI,K. Bortolin; H. Arabi; H. Zaidi,"Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211",2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),09-Apr-20,2019,,,1,3,"Attenuation correction (AC) is essential component for quantitative PET imaging. However, in PET/MR imaging and dedicated brain PET devices, the attenuation map either suffers from a number of limitations or is not readily available in the absence of CT or transmission scan. To tackle this issue, a deep convolutional neural networks is proposed to perform joint attenuation and scatter correction in the image domain on the non-attenuation corrected PET images (PET-nonAC). The deep con-volutional neural network used in this work benefits from dilated convolutions and residual connections to establish an end-to-end PET attenuation correction (PET-DirAC). For the training phase, data of 30 patients who underwent brain <sup>18</sup>F-FDG PET/CT scans were used to generate reference PET-CTAC and PET-nonAC images. A five-fold cross-validation scheme was used for training/evaluation of the proposed algorithm. The quantitative accuracy of the proposed method was evaluated against the commercial segmentation-based method (2-class AC map referred to as MRAC). For quantitative analysis, tracer uptake estimated from PET-DirAC and PET-MRAC was compared to PET-CTAC. The relative SUV bias was calculated for bone, soft-tissue, air cavities and the entire head, separately. The proposed approach resulted in a mean relative absolute error (MRAE) of 4.1±7.5% and 5.8±10.4% for the entire head and bone regions, respectively. Conversely, MRAC led to a MRAE of 8.1±10.2% and 17.2±6.1% for these two regions, respectively. A mean SUV difference of 0.3±0.6 was achieved when using the direct method (DirAC) while the MRAC approach led to a mean SUV difference of -0.5±0.7. The quantitative analysis demonstrated the superior performance of the proposed deep learning-based AC approach over MRI segmentation-based method. The proposed approach seems promising to improve the quantitative accuracy of PET/MRI without the need for concurrent anatomical imaging.",2577-0829,978-1-7281-4164-0,10.1109/NSS/MIC42101.2019.9059943,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9059943,,Attenuation;Image segmentation;Magnetic resonance imaging;Computed tomography;Bones;Biomedical imaging,biomedical MRI;bone;brain;computerised tomography;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography,concurrent anatomical imaging;deep learning-guided attenuation;anatomical images;PET imaging;attenuation map;transmission scan;deep convolutional neural networks;joint attenuation;image domain;nonattenuation corrected PET images;convolutional neural network;PET attenuation correction;PET-DirAC;PET-CTAC;PET-nonAC images;PET-MRAC;deep learning-based AC approach;MRI segmentation-based method;SUV difference;AC map;brain PET devices,,6,,11,,09-Apr-20,,,IEEE,IEEE Conferences
SVM based lung cancer diagnosis using multiple image features in PET/CT,N. Guo; R. Yen; G. E. Fakhri; Q. Li,"Center for Advanced Medical Imaging Science, Division of Nuclear Medicine, Radiology, Massachusetts General Hospital and Harvard Medical School, Boston 02114, USA; Department of Nuclear Medicine, National Taiwan University Hospital and National Taiwan University College of Medicine, Taipei, Taiwan; Center for Advanced Medical Imaging Science, Division of Nuclear Medicine, Radiology, Massachusetts General Hospital and Harvard Medical School, Boston 02114, USA; Center for Advanced Medical Imaging Science, Division of Nuclear Medicine, Radiology, Massachusetts General Hospital and Harvard Medical School, Boston 02114, USA",2015 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),06-Oct-16,2015,,,1,4,"In this project, we assessed the clinical value of tumor heterogeneity measured with <sup>18</sup>F-FLT as a biomarker for lung cancer diagnosis and staging, then compared its performance to traditional image features using final pathology as gold standard. We also proposed to apply support vector machine (SVM) to train a vector of image features including heterogeneity extracted from PET image and CT texture features to improve the diagnosis and staging for lung cancer. Thirty-two subjects with lung nodules (19 M, 13 F, age 70 ± 9 y) who underwent <sup>18</sup>F-FLT PET/CT scans were included in our study. We applied the global Moran I(d) analysis to characterize the intra-tumor heterogeneity on PET images 1h post-injection. Other than texture analysis that widely used in heterogeneity prediction, I(d) statistic is a measure of spatial autocorrelation characterized by the correlation among 3D neighboring voxels. Other image features including SUV and CT texture were extracted from PET/CT images. Then we trained and applied a SVM based statistical machine learning tool to fuse the features and test the SVM performance in classifying patient groups: benign/early malignant and early/advanced malignant. Heterogeneity derived from <sup>18</sup>F-FLT images significantly differentiated benign (0.24 ± 0.09, N = 9) from early stage malignancy (0.40 ± 0.09, N = 10; P = 0.002), as well as early stage from advanced stage malignancy (0.50 ± 0.07, N = 13, P = 0.005). Other image features, SUVmean and CT texture, didn't demonstrated similar capability. Intra-tumor heterogeneity showed superior performance than other traditional image features when single feature was applied to staging. Furthermore, the SVM classification showed that best performance of staging was achieved when all image features are combined in the SVM training. In conclusion, we obtained a novel measurement of intra-tumor heterogeneity which has promising performance for diagnosis and staging of lung cancer. We demonstrated the feasibility of performing SVM based cancer staging using multiple image features in PET/CT. SVM analysis and classification with combination of effective features has great potential to augment diagnostic accuracy and improve tumor staging in oncological practice.",,978-1-4673-9862-6,10.1109/NSSMIC.2015.7582234,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582234,,Support vector machines;Cancer;Tumors;Positron emission tomography;Computed tomography;Training;Lungs,cancer;feature extraction;image classification;image texture;learning (artificial intelligence);lung;medical image processing;positron emission tomography;support vector machines;tumours,SVM based lung cancer diagnosis;multiple image feature extraction;tumor heterogeneity;biomarker;support vector machine;heterogeneity extraction;lung nodules;18F-FLT PET-CT scans;global Moran analysis;intratumor heterogeneity;spatial autocorrelation;3D neighboring voxels;PET-CT images;SVM based statistical machine learning tool;SUV texture;SVM classification;SVM based cancer staging;tumor staging;positron emission tomography;computed tomography,,5,,6,,06-Oct-16,,,IEEE,IEEE Conferences
Deep multi-modality collaborative learning for distant metastases predication in PET-CT soft-tissue sarcoma studies,Y. Peng; L. Bi; Y. Guo; D. Feng; M. Fulham; J. Kim,"School of Computer Science, University of Sydney, Australia; School of Computer Science, University of Sydney, Australia; Biomedical Engineering, Shanghai Jiao Tong University, China; School of Computer Science, University of Sydney, Australia; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Australia; School of Computer Science, University of Sydney, Australia",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),07-Oct-19,2019,,,3658,3688,"Soft-tissue Sarcomas (STS) are a heterogeneous group of malignant neoplasms with a relatively high mortality rate from distant metastases. Early prediction or quantitative evaluation of distant metastases risk for patients with STS is an important step which can provide better-personalized treatments and thereby improve survival rates. Positron emission tomography-computed tomography (PET-CT) image is regarded as the imaging modality of choice for the evaluation, staging and assessment of STS. Radiomics, which refers to the extraction and analysis of the quantitative of high-dimensional mineable data from medical images, is foreseen as an important prognostic tool for cancer risk assessment. However, conventional radiomics methods that depend heavily on hand-crafted features (e.g. shape and texture) and prior knowledge (e.g. tuning of many parameters) therefore cannot fully represent the semantic information of the image. In addition, convolutional neural networks (CNN) based radiomics methods present capabilities to improve, but currently, they are mainly designed for single modality e.g., CT or a particular body region e.g., lung structure. In this work, we propose a deep multi-modality collaborative learning to iteratively derive optimal ensembled deep and conventional features from PET-CT images. In addition, we introduce an end-to-end volumetric deep learning architecture to learn complementary PET-CT features optimised for image radiomics. Our experimental results using public PET-CT dataset of STS patients demonstrate that our method has better performance when compared with the state-of-the-art methods.",1558-4615,978-1-5386-1311-5,10.1109/EMBC.2019.8857666,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857666,,,biological tissues;cancer;computerised tomography;convolutional neural nets;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,quantitative evaluation;survival rates;positron emission tomography-computed tomography image;high-dimensional mineable data;medical images;prognostic tool;cancer risk assessment;hand-crafted features;convolutional neural networks based radiomics methods;optimal ensembled deep features;PET-CT images;end-to-end volumetric deep learning architecture;complementary PET-CT features;image radiomics;public PET-CT dataset;STS patients;distant metastases predication;PET-CT soft-tissue sarcoma studies;malignant neoplasms;mortality rate;deep multimodality collaborative learning;lung structure,"Deep Learning;Humans;Neural Networks, Computer;Positron Emission Tomography Computed Tomography;Sarcoma",5,,14,,07-Oct-19,,,IEEE,IEEE Conferences
Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Jiangsu Key Laboratory of Media Design and Software Technology, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, Jiangsu, P. R. China; Jiangsu Key Laboratory of Media Design and Software Technology, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, Jiangsu, P. R. China; Jiangsu Key Laboratory of Media Design and Software Technology, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, Jiangsu, P. R. China; Jiangsu Key Laboratory of Media Design and Software Technology, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, Jiangsu, P. R. China; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign, IL, USA; Department of Radiology and Case Center for Imaging Research, University Hospitals, Case Western Reserve University, Cleveland, OH, USA; Department of Biomedical Engineering, Department of Internal Medicine, Louis Stokes Cleveland VA Medical Center, Case Western Reserve University, Cleveland, OH, USA; Department of Radiology and Case Center for Imaging Research, University Hospitals, Case Western Reserve University, Cleveland, OH, USA; Department of Radiation Oncology, University Hospitals Cleveland Medical Center, Cleveland, OH, USA; Philips Healthcare, Cleveland, OH, USA; Philips Healthcare, Cleveland, OH, USA; Philips Research North America, Cambridge, MA, USA; Case Center for Imaging Research and Department of Radiation Oncology, Case Western Reserve University, Cleveland, OH, USA; Department of Radiology and Case Center for Imaging Research, University Hospitals, Case Western Reserve University, Cleveland, OH, USA",IEEE/ACM Transactions on Computational Biology and Bioinformatics,03-Feb-21,2021,18,1,70,82,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",1557-9964,,10.1109/TCBB.2020.2979841,"National Natural Science Foundation of China(grant numbers:61772241,61702225); Natural Science Foundation of Jiangsu Province(grant numbers:BK20160187); Science and Technology demonstration project of social development of Wuxi(grant numbers:WX18IVJN002); National Institutes of Health(grant numbers:R01CA196687); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9034480,Abdomen;intelligent transformation;machine learning;medical images;MR;pelvis;synthetic CT,Computed tomography;Bones;Biological tissues;Abdomen;Pelvis;Fats;Feature extraction,biomedical MRI;bone;computerised tomography;edge detection;fats;feature extraction;fuzzy set theory;image classification;medical image processing;pattern clustering;radiation therapy;supervised learning;support vector machines,LapSVM;SCT-PK-PS method;CT images;intelligent transformation;UTE-mDixon MR abdomen-pelvis images;partial supervision;PET attenuation correction;radiation treatment planning;soft tissue contrast;ionizing radiation;photon absorption information;ultrashort echo time;large-field-of-view images;span abdomen;Laplacian support vector machine;prior knowledge-referenced KL-TFCM clustering;subsequent semisupervised classification;computed tomography;MR-based synthetic CT generation;MR-only RTP;MR pulse sequence;modified Dixon;knowledge-leveraged transfer fuzzy c-means;soft tissue;fat;bone marrow;tissue-recognizers;edge detector filter based feature extraction;UTE-mDixon MR images;mean absolute prediction deviation;all-water method;external-air;internal-air;four-cluster-partitioning method,"Abdomen;Humans;Image Processing, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Pelvis;Tomography, X-Ray Computed",5,,80,IEEE,12-Mar-20,,,IEEE,IEEE Journals
Pseudo CT Estimation using Patch-based Joint Dictionary Learning,Y. Lei; H. K. Shu; S. Tian; T. Wang; T. Liu; H. Mao; H. Shim; W. J. Curran; X. Yang,"Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiology and Imaging Sciences and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology, Department of Radiology and Imaging Sciences, and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, 30322, USA",2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),28-Oct-18,2018,,,5150,5153,"Magnetic resonance (MR) simulators have recently gained popularity; it avoids the unnecessary radiation exposure associated with Computed Tomography (CT) when used for radiation therapy planning. We propose a method for pseudo CT estimation from MR images based on joint dictionary learning. Patient-specific anatomical features were extracted from the aligned training images and adopted as signatures for each voxel. The most relevant and informative features were identified to train the joint dictionary learning-based model. The well-trained dictionary was used to predict the pseudo CT of a new patient. This prediction technique was validated with a clinical study of 12 patients with MR and CT images of the brain. The mean absolute error (MAE), peak signal-to-noise ratio (PSNR), normalized cross correlation (NCC) indexes were used to quantify the prediction accuracy. We compared our proposed method with a state-of-the-art dictionary learning method. Overall our proposed method significantly improves the prediction accuracy over the state-of-the-art dictionary learning method. We have investigated a novel joint dictionary Iearning-based approach to predict CT images from routine MRIs and demonstrated its reliability. This CT prediction technique could be a useful tool for MRI-based radiation treatment planning or attenuation correction for quantifying PET images for PET/MR imaging.",1558-4615,978-1-5386-3646-6,10.1109/EMBC.2018.8513475,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513475,,Computed tomography;Feature extraction;Machine learning;Magnetic resonance imaging;Dictionaries;Training;Planning,biomedical MRI;brain;cancer;computerised tomography;image reconstruction;image resolution;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;radiation therapy,pseudoCT estimation;patch-based joint dictionary learning;magnetic resonance simulators;unnecessary radiation exposure;radiation therapy planning;patient-specific anatomical features;aligned training images;relevant features;informative features;joint dictionary learning-based model;well-trained dictionary;peak signal-to-noise ratio;prediction accuracy;state-of-the-art dictionary;novel joint dictionary;CT images;CT prediction technique;attenuation correction;PET images;computed tomography;PET-MR imaging,"Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Reproducibility of Results;Tomography, X-Ray Computed",5,,37,,28-Oct-18,,,IEEE,IEEE Conferences
Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, USA; Center for Gamma-Ray Imaging, University of Arizona, Tucson, AZ, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, USA",IEEE Transactions on Radiation and Plasma Medical Sciences,02-Mar-21,2021,5,2,193,201,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",2469-7303,,10.1109/TRPMS.2020.2989073,"National Science Foundation(grant numbers:1828993); NIH(grant numbers:R01EB019465,3R01EB01946504S1); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075292,Deep learning;generative adversarial network (GAN);Image translation and segmentation;MR-based attenuation map;Positron emission tomography systems combined with magnetic resonance imaging (PET/MR),Generators;Decoding;Attenuation;Task analysis;Bones;Image segmentation;Computed tomography,biomedical MRI;bone;brain;computerised tomography;deep learning (artificial intelligence);image reconstruction;image segmentation;medical image processing;neural nets;positron emission tomography,MR image segmentation;empirical attenuation coefficients;PET image reconstruction;MR-based attenuation correction;generative adversarial network;residual blocks;image translation;pseudoCT image synthesis;brain PET;photon attenuation correction;positron emission tomography;magnetic resonance imaging;deep learning;PET-MR systems;structural similarity index,,4,,38,IEEE,21-Apr-20,,,IEEE,IEEE Journals
An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu-shi, Japan; Software College, Northeastern University, Shenyang, China; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu-shi, Japan; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu-shi, Japan; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA",IEEE Access,06-Jan-20,2020,8,,1135,1145,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",2169-3536,,10.1109/ACCESS.2019.2961268,"National Natural Science Foundation of China(grant numbers:61872075); Ministry of Education, Culture, Sports, Science and Technology(grant numbers:26108005); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937543,PET/CT registration;unsupervised learning;two-level similarity measure;deformation regularization,Three-dimensional displays;Computed tomography;Image registration;Task analysis;Strain;Biomedical imaging;Training,computerised tomography;deformation;image fusion;image registration;image segmentation;liver;medical image processing;positron emission tomography,euclidean distance;deep learning registration models;liver bounding box;test set;Hu patches;Hounsfield Unit patches;SUV;registration result;image registration stage;loss function;deformation regularization;two-level similarity measure;3D displacement vector field;DenseRegNet;unsupervised end-to-end method;medical image registration;medical image analysis;computed tomography;positron emission tomography;medical images;functional information;anatomical information;fusion image;DVF smoothness;distribution similarity;optimized registration method;time 10.0 s,,4,,29,CCBY,20-Dec-19,,,IEEE,IEEE Journals
Combining Superpixels and Deep Learning Approaches to Segment Active Organs in Metastatic Breast Cancer PET Images,C. Fourcade; L. Ferrer; G. Santini; N. Moreau; C. Rousseau; M. Lacombe; C. Guillerminet; M. Colombié; M. Campone; D. Mateus; M. Rubeaux,"Ecole Centrale de Nantes,LS2N, UMR CNRS,Nantes,France,6004; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; Keosys Medical Imaging,Nantes,France; Keosys Medical Imaging,Nantes,France; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; ICO Cancer Center,Nantes - Angers,France; ICO Cancer Center,Nantes - Angers,France; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; Ecole Centrale de Nantes,LS2N, UMR CNRS,Nantes,France,6004; Keosys Medical Imaging,Nantes,France",2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),27-Aug-20,2020,,,1536,1539,"Semi-automatic measurements are performed on <sup>18</sup>FDG PET-CT images to monitor the evolution of metastatic sites in the clinical follow-up of metastatic breast cancer patients. Apart from being time-consuming and prone to subjective approximation, semi-automatic tools cannot make the difference between cancerous regions and active organs, presenting a high <sup>18</sup>FDG uptake.In this work, we combine a deep learning-based approach with a superpixel segmentation method to segment the main active organs (brain, heart, bladder) from full-body PET images. In particular, we integrate a superpixel SLIC algorithm at different levels of a convolutional network. Results are compared with a deep learning segmentation network alone. The methods are cross-validated on full-body PET images of 36 patients and tested on the acquisitions of 24 patients from a different study center, in the context of the ongoing EPICUREseinmeta study. The similarity between the manually defined organ masks and the results is evaluated with the Dice score. Moreover, the amount of false positives is evaluated through the positive predictive value (PPV).According to the computed Dice scores, all approaches allow to accurately segment the target organs. However, the networks integrating superpixels are better suited to transfer knowledge across datasets acquired on multiple sites (domain adaptation) and are less likely to segment structures outside of the target organs, according to the PPV.Hence, combining deep learning with superpixels allows to segment organs presenting a high <sup>18</sup>FDG uptake on PET images without selecting cancerous lesion, and thus improves the precision of the semi-automatic tools monitoring the evolution of breast cancer metastasis.Clinical relevance— We demonstrate the utility of combining deep learning and superpixel segmentation methods to accurately find the contours of active organs from metastatic breast cancer images, to different dataset distributions.",2694-0604,978-1-7281-1990-8,10.1109/EMBC44109.2020.9175683,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175683,,Image segmentation;Lesions;Positron emission tomography;Training;Machine learning;Breast cancer;Three-dimensional displays,biological organs;brain;cancer;image classification;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;tumours,different study center;ongoing EPICUREseinmeta study;manually defined organ masks;Dice score;positive predictive value;computed Dice scores;target organs;multiple sites;segment structures;segment organs;cancerous lesion;semiautomatic tools;breast cancer metastasis;superpixel segmentation method;metastatic breast cancer images;dataset distributions;deep learning approaches;segment active organs;metastatic breast cancer PET;semiautomatic measurements;metastatic sites;metastatic breast cancer patients;cancerous regions;deep learning-based approach;main active organs;full-body PET images;superpixel SLIC algorithm;convolutional network;deep learning segmentation network;18FDG PET-CT images;18FDG uptake,Algorithms;Brain;Breast Neoplasms;Deep Learning;Humans;Neoplasm Metastasis;Positron Emission Tomography Computed Tomography,4,,19,IEEE,27-Aug-20,,,IEEE,IEEE Conferences
Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia; Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia; Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia",2012 9th IEEE International Symposium on Biomedical Imaging (ISBI),12-Jul-12,2012,,,1775,1778,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",1945-8452,978-1-4577-1858-8,10.1109/ISBI.2012.6235925,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235925,image retrieval;distance metric;regression;sparsity;classification,Measurement;Training;Vectors;Image retrieval;Biomedical imaging;Optimization;Lungs,computerised tomography;feature extraction;image classification;image retrieval;learning (artificial intelligence);medical image processing;positron emission tomography;regression analysis;visual databases,regression based distance metric learning;classification based distance metric learning;medical image retrieval;medical imaging databases;reference dataset;diagnosis;learning-based distance metric design;weight learning approach;similar-dissimilar data samples;optimization;sparsity-constraint regression algorithm;feature selection;clinical PET-CT images,,4,,14,,12-Jul-12,,,IEEE,IEEE Conferences
Skull Segmentation in MRI by a Support Vector Machine Combining Local and Global Features,J. Sjölund; A. E. Järlideni; M. Andersson; H. Knutsson; H. Nordström,"Dept. of Biomed. Eng., Linkoping Univ., Linkoping, Sweden; Elekta Instrum. AB, Stockholm, Sweden; Dept. of Biomed. Eng., Linkoping Univ., Linkoping, Sweden; Dept. of Biomed. Eng., Linkoping Univ., Linkoping, Sweden; Elekta Instrum. AB, Stockholm, Sweden",2014 22nd International Conference on Pattern Recognition,06-Dec-14,2014,,,3274,3279,"Magnetic resonance (MR) images lack information about radiation transport-a fact which is problematic in applications such as radiotherapy planning and attenuation correction in combined PET/MR imaging. To remedy this, a crude but common approach is to approximate all tissue properties as equivalent to those of water. We improve upon this using an algorithm that automatically identifies bone tissue in MR. More specifically, we focus on segmenting the skull prior to stereotactic neurosurgery, where it is common that only MR images are available. In the proposed approach, a machine learning algorithm known as a support vector machine is trained on patients for which both a CT and an MR scan are available. As input, a combination of local and global information is used. The latter is needed to distinguish between bone and air as this is not possible based only on the local image intensity. A whole skull segmentation is achievable in minutes. In a comparison with two other methods, one based on mathematical morphology and the other on deformable registration, the proposed method was found to yield consistently better segmentations.",1051-4651,978-1-4799-5209-0,10.1109/ICPR.2014.564,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977276,,Bones;Computed tomography;Image segmentation;Magnetic resonance imaging;Support vector machines;Training;Positron emission tomography,biomedical MRI;image registration;image segmentation;learning (artificial intelligence);medical image processing;support vector machines,skull segmentation;MRI;support vector machine;magnetic resonance imaging;radiotherapy planning;attenuation correction;combined PET-MR imaging;stereotactic neurosurgery;machine learning algorithm;local image intensity;mathematical morphology;deformable registration,,4,1,17,,06-Dec-14,,,IEEE,IEEE Conferences
Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks,F. Munawar; S. Azmat; T. Iqbal; C. Grönlund; H. Ali,"Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, Pakistan; Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, Pakistan; Department of Medicine, National University of Ireland Galway, Galway, Ireland; Department of Radiation Sciences, Biomedical Engineering, Umeå University, Umeå, Sweden; Department of Electrical and Computer Engineering, COMSATS University Islamabad, Abbottabad Campus, Abbottabad, Pakistan",IEEE Access,28-Aug-20,2020,8,,153535,153545,"Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",2169-3536,,10.1109/ACCESS.2020.3017915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171249,Deep learning;generative adversarial networks;lung segmentation;medical imaging,Lung;Image segmentation;Generative adversarial networks;X-ray imaging;Computed tomography;Generators;Diseases,diagnostic radiography;diseases;image segmentation;learning (artificial intelligence);lung;medical image processing,chest X-ray image;generative adversarial networks;medical imaging;GAN;lung segmentation;segmented mask;adversarial loss measure;ground truth masks;CXR datasets,,4,,54,CCBY,19-Aug-20,,,IEEE,IEEE Journals
Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Center for Biomedical Image Computing and AnalyticsUniversity of Pennsylvania; Center for Biomedical Image Computing and AnalyticsUniversity of Pennsylvania; Center for Biomedical Image Computing and AnalyticsUniversity of Pennsylvania; Center for Biomedical Image Computing and AnalyticsUniversity of Pennsylvania; Department of Radiation OncologyUniversity of Pennsylvania; Department of Radiation OncologyUniversity of Pennsylvania; Department of Radiation OncologyUniversity of Pennsylvania; Department of Radiation OncologyUniversity of Pennsylvania; Center for Biomedical Image Computing and Analytics, University of Pennsylvania, Philadelphia, PA, USA",IEEE Transactions on Biomedical Engineering,17-Sep-20,2020,67,10,2735,2744,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",1558-2531,,10.1109/TBME.2020.2969839,"National Institutes of Health(grant numbers:CA223358,EB022573); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970503,"Sparsity;collaborative clustering;unsupervised learning;nonnegative matrix tri-factorization,radiomics",Radiomics;Collaboration;Dimensionality reduction;Matrix decomposition;Feature extraction;Cancer;Dictionaries,Bayes methods;cancer;computerised tomography;feature extraction;image representation;learning (artificial intelligence);matrix decomposition;medical image processing;pattern classification;pattern clustering;positron emission tomography,discriminative low-dimensional representations;dimensionality reduction techniques;radiomic studies;distinct radiomic features;simultaneous feature dimensionality reduction;cluster patients;robust collaborative clustering method,"Bayes Theorem;Cluster Analysis;Humans;Image Processing, Computer-Assisted;Neoplasms;Positron Emission Tomography Computed Tomography;Prognosis",4,,68,IEEE,28-Jan-20,,,IEEE,IEEE Journals
PET-Train: Automatic Ground Truth Generation from PET Acquisitions for Urinary Bladder Segmentation in CT Images using Deep Learning,C. Gsaxner; B. Pfarrkirchner; L. Lindner; A. Pepe; P. M. Roth; J. Egger; J. Wallner,"Inst. of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Inst. of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Inst. of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Inst. of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Inst. of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Inst. of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Department of Maxillofacial Surgery, Medical University of Graz, Graz, Austria",2018 11th Biomedical Engineering International Conference (BMEiCON),13-Jan-19,2018,,,1,5,"In this contribution, we propose an automatic ground truth generation approach that utilizes Positron Emission Tomography (PET) acquisitions to train neural networks for automatic urinary bladder segmentation in Computed Tomography (CT) images. We evaluated different deep learning architectures to segment the urinary bladder. However, deep neural networks require a large amount of training data, which is currently the main bottleneck in the medical field, because ground truth labels have to be created by medical experts on a time-consuming slice-by-slice basis. To overcome this problem, we generate the training data set from the PET data of combined PET/CT acquisitions. This can be achieved by applying simple thresholding to the PET data, where the radiotracer accumulates very distinct in the urinary bladder. However, the ultimate goal is to entirely skip PET imaging and its additional radiation exposure in the future, and only use CT images for segmentation.",2334-3052,978-1-5386-5724-9,10.1109/BMEiCON.2018.8609954,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8609954,Deep Learning;Medical Imaging;Segmentation;PET/CT;Urinary Bladder,Image segmentation;Bladder;Computed tomography;Positron emission tomography;Neural networks;Training;Deep learning,biological organs;computerised tomography;image segmentation;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography;radioactive tracers,deep neural networks;training data;medical field;ground truth labels;medical experts;time-consuming slice-by-slice basis;PET data;PET imaging;CT images;PET acquisitions;automatic ground truth generation approach;automatic urinary bladder segmentation;segment;computed tomography images;positron emission tomography acquisitions;deep learning architectures;PET-train;combined PET-CT acquisitions,,3,,20,,13-Jan-19,,,IEEE,IEEE Conferences
Deep Learning based Respiratory Pattern Classification and Applications in PET/CT Motion Correction,Y. Guo; N. Dvornek; Y. Lu; Y. -J. Tsai; J. Hamill; M. Casey; C. Liu,"Yale University,Department of Biomedical Engineering,New Haven,CT,USA,06511; Yale University,Department of Biomedical Engineering and the Department of Radiology and Biomedical Imaging,New Haven,CT,USA,06511; Yale University,Department of Radiology and Biomedical Imaging,New Haven,CT,USA,06511; Yale University,Department of Radiology and Biomedical Imaging,New Haven,CT,USA,06511; James Hamill and Michael Casey are with the Siemens Medical Solutions,Knoxville,TN,USA,37932; James Hamill and Michael Casey are with the Siemens Medical Solutions,Knoxville,TN,USA,37932; Yale University,Department of Biomedical Engineering and the Department of Radiology and Biomedical Imaging,New Haven,CT,USA,06511",2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),09-Apr-20,2019,,,1,5,"Respiratory motion has to be corrected in PET/CT imaging for precise tumor detection and quantification. The optimal motion correction methods for regular breathers and irregular breathers could be different. In this study, we developed deep learning based methods to automatically classify patient breathing patterns and investigated the impact of breathing pattern variability on gating performance. We implemented a hybrid neural network consisting of convolutional (Conv) layers, recurrent layers (LSTM, long short-term memory) and a linear classifier to differentiate breathing patterns. 1295 respiratory traces collected using RPM (Real-time Position Management) system were used for training and testing, as well as additional traces acquired using the Anzai system. We optimized the deep neural network with respect to data preprocessing, augmentation, weighted loss function and generalization capability. The results showed that the proposed deep learning model has reached a high prediction accuracy, with a sensitivity of 92.0% and a specificity of 91.8%. Using phase gating approach, for regular breathers, end-expiration phase gating can effectively reduce the respiratory motion. In contrast, for irregular breathers, larger amount of intra-gate motion was present in the gated PET/CT images and more sophisticated motion correction methods are required.",2577-0829,978-1-7281-4164-0,10.1109/NSS/MIC42101.2019.9059783,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9059783,,Neural networks;Computed tomography;Machine learning;Tumors;Training;Image reconstruction,convolutional neural nets;image classification;image motion analysis;learning (artificial intelligence);medical image processing;pneumodynamics;positron emission tomography;recurrent neural nets;tumours,respiratory motion;precise tumor detection;optimal motion correction methods;regular breathers;irregular breathers;patient breathing patterns;breathing pattern variability;gating performance;hybrid neural network;convolutional layers;recurrent layers;linear classifier;respiratory traces;RPM system;real-time position management;Anzai system;deep neural network;weighted loss function;generalization capability;deep learning model;phase gating approach;end-expiration phase gating;intragate motion;sophisticated motion correction methods;deep learning based respiratory pattern classification;PET-CT motion correction;PET-CT imaging;data preprocessing;computed tomography;positron emission tomography,,3,,17,,09-Apr-20,,,IEEE,IEEE Conferences
Deep learning approaches for bone and bone lesion segmentation on 18FDG PET/CT imaging in the context of metastatic breast cancer,N. Moreau; C. Rousseau; C. Fourcade; G. Santini; L. Ferrer; M. Lacombe; C. Guillerminet; M. Campone; M. Colombié; M. Rubeaux; a. N. Normand,"Université de Nantes,CNRS, LS2N,Nantes,France,F-44000; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; Keosys Medical Imaging,Nantes,France; Keosys Medical Imaging,Nantes,France; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; ICO Cancer Center,Nantes - Angers,France; ICO Cancer Center,Nantes - Angers,France; University of Nantes,CRCINA, INSERM UMR1232, CNRS-ERL6001,Nantes,France; ICO Cancer Center,Nantes - Angers,France; Keosys Medical Imaging,Nantes,France; Université de Nantes,CNRS, LS2N,Nantes,France,F-44000",2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),27-Aug-20,2020,,,1532,1535,"<sup>18</sup>FDG PET/CT imaging is commonly used in diagnosis and follow-up of metastatic breast cancer, but its quantitative analysis is complicated by the number and location heterogeneity of metastatic lesions. Considering that bones are the most common location among metastatic sites, this work aims to compare different approaches to segment the bones and bone metastatic lesions in breast cancer.Two deep learning methods based on U-Net were developed and trained to segment either both bones and bone lesions or bone lesions alone on PET/CT images. These methods were cross-validated on 24 patients from the prospective EPICURE<inf>seinmeta</inf> metastatic breast cancer study and were evaluated using recall and precision to measure lesion detection, as well as the Dice score to assess bones and bone lesions segmentation accuracy.Results show that taking into account bone information in the training process allows to improve the precision of the lesions detection as well as the Dice score of the segmented lesions. Moreover, using the obtained bone and bone lesion masks, we were able to compute a PET bone index (PBI) inspired by the recognized Bone Scan Index (BSI). This automatically computed PBI globally agrees with the one calculated from ground truth delineations.Clinical relevance— We propose a completely automatic deep learning based method to detect and segment bones and bone lesions on <sup>18</sup>FDG PET/CT in the context of metastatic breast cancer. We also introduce an automatic PET bone index which could be incorporated in the monitoring and decision process.",2694-0604,978-1-7281-1990-8,10.1109/EMBC44109.2020.9175904,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175904,,Bones;Lesions;Breast cancer;Image segmentation;Machine learning;Indexes,biological organs;bone;cancer;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;tumours,bone lesions;automatic PET bone index;bone lesion segmentation;bone metastatic lesions;seinmeta metastatic breast cancer study;measure lesion detection;account bone information;lesions detection;segmented lesions;bone lesion masks;segment bones;recognized bone scan index,"Breast Neoplasms;Deep Learning;Fluorodeoxyglucose F18;Humans;Positron Emission Tomography Computed Tomography;Prospective Studies;Tomography, X-Ray Computed",3,,16,IEEE,27-Aug-20,,,IEEE,IEEE Conferences
PET-CT based automated lung nodule detection,N. Zsoter; P. Bandi; G. Szabo; Z. Toth; R. A. Bundschuh; J. Dinges; L. Papp,"Mediso Medical Imaging Systems Ltd., Baross str. 91-95, Budapest, Hungary; Mediso Medical Imaging Systems Ltd., Baross str. 91-95, Budapest, Hungary; Mediso Medical Imaging Systems Ltd., Baross str. 91-95, Budapest, Hungary; Scanomed Ltd., Budapest, Hungary; University of Wuerzburg, Germany; Technical University of Munich, Germany; Mediso Medical Imaging Systems Ltd., Baross str. 91-95, Budapest, Hungary",2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,10-Nov-12,2012,,,4974,4977,"An automatic method is presented in order to detect lung nodules in PET-CT studies. Using the foreground and background mean ratio independently in every nodule, we can detect the region of the nodules properly. The size and intensity of the lesions do not affect the result of the algorithm, although size constraints are present in the final classification step. The CT image is also used to classify the found lesions built on lung segmentation. We also deal with those cases when nearby and similar nodules are merged into one by a split-up post-processing step. With our method the time of the localization can be decreased from more than one hour to maximum five minutes. The method had been implemented and validated on real clinical cases in Interview Fusion clinical evaluation software (Mediso). Results indicate that our approach is very effective in detecting lung nodules and can be a valuable aid for physicians working in the daily routine of oncology.",1558-4615,978-1-4577-1787-1,10.1109/EMBC.2012.6347109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6347109,,Lungs;Positron emission tomography;Lesions;Computed tomography;Image segmentation;Cancer;Muscles,cancer;image classification;image fusion;lung;medical image processing;positron emission tomography;tumours,PET-computerised tomography;automated lung nodule detection;foreground mean ratio;background mean ratio;final classification step;lung segmentation;split-up post-processing step;Interview Fusion clinical evaluation software;oncology,"Algorithms;Artificial Intelligence;Humans;Image Interpretation, Computer-Assisted;Lung Neoplasms;Multimodal Imaging;Pattern Recognition, Automated;Positron-Emission Tomography;Reproducibility of Results;Sensitivity and Specificity;Software;Solitary Pulmonary Nodule;Tomography, X-Ray Computed",3,,12,,10-Nov-12,,,IEEE,IEEE Conferences
Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"Luoyang Central Hospital Affiliated to Zhengzhou University, Luoyang, China; Luoyang Central Hospital Affiliated to Zhengzhou University, Luoyang, China; Luoyang Central Hospital Affiliated to Zhengzhou University, Luoyang, China",IEEE Access,10-Jun-20,2020,8,,104445,104457,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",2169-3536,,10.1109/ACCESS.2020.2989819,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076675,Convolutional neural network;Itti model;GrabCut algorithm;heart segmentation;image saliency;stacking noise reduction self-coding network,Image segmentation;Heart;Visualization;Medical diagnostic imaging;Feature extraction,biomedical ultrasonics;cardiology;computerised tomography;convolutional neural nets;image classification;image segmentation;image sequences;medical image processing;rendering (computer graphics),volume rendering;surface rendering;3D visual reconstruction;4D heart images;cardiac CT image;PET image;image saliency detection algorithm;cardiac segmentation;visual saliency techniques;deep neural network model;segmented heart image;CT image sequence;medical image recognition;image data;fast segmentation algorithm;effective segmentation algorithm;ultrasound heart image;fully automatic segmentation algorithm;segmentation efficiency;artificial segmentation results;heart tissue;convolutional neural network;image segmentation effect;improved GrabCut image segmentation algorithm;grayscale image,,3,,34,CCBY,23-Apr-20,,,IEEE,IEEE Journals
Towards an Automatic Imaging Biopsy of Non-Small Cell Lung Cancer,E. D'Arnese; G. W. di Donato; E. del Sozzo; M. D. Santambrogio,"Politecnico di Milano, Milan, Italy; Politecnico di Milano, Milan, Italy; Politecnico di Milano, Milan, Italy; Politecnico di Milano, Milan, Italy",2019 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),12-Sep-19,2019,,,1,4,"Because of the high aggressiveness and lethality of lung cancer, its early detection and accurate characterization are among the most investigated challenges in the last years. Biomedical imaging is an important technology in lung cancer assessment, strongly impacting decision making in clinical practice, also employed as a provider of predictive imaging biomarkers. In this context, radiomics approach, which consists of mining vast arrays of quantitative features derived from digital images, has shown promising application perspectives, but suboptimal standardization and controversial results emerged. This work presents the design of a completely automated pipeline for the non invasive in-vivo characterization of Non-Small Cell Lung Cancer (NSCLC), devised to be a support for radiologists and physicians, and to speed up the diagnostic process. Our pipeline exploits data from routinely acquired PET and CT images in order to automatically obtain a reliable segmentation of the tumor lesion: accurate textural features are computed in the detected Volume of Interest (VOI), thus providing data for the characterization of lung lesion through machine learning algorithms. We evaluated our pipeline on real datasets supplied by a private hospital. Our approach reached a mean accuracy of 94.2±5.0% for the VOI segmentation, and it showed the potential of PET/CT features in differentiating both between primary and metastatic lung lesions and between primary lung cancer subtypes.",2641-3604,978-1-7281-0848-3,10.1109/BHI.2019.8834485,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834485,,Lung;Cancer;Image segmentation;Computed tomography;Feature extraction;Pipelines,cancer;computerised tomography;decision making;feature extraction;image segmentation;image texture;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,lung lesion;metastatic lung lesions;automatic imaging biopsy;biomedical imaging;lung cancer assessment;predictive imaging biomarkers;digital images;noninvasive in-vivo characterization;automated pipeline;radiomics approach;CT images;PET images,,3,,15,,12-Sep-19,,,IEEE,IEEE Conferences
Automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of planning CT and FDG-PET/CT images,H. Arimura; Z. Jin; Y. Shioyama; K. Nakamura; T. Magome; M. Sasaki,"Department of Health Sciences, Kyushu University, Fukuoka 812-8582, Japan; Department of Health Sciences, Kyushu University, Fukuoka 812-8582, Japan; Department of Clinical Radiology, Kyushu University, Fukuoka 812-8582, Japan; Department of Clinical Radiology, Kyushu University, Fukuoka 812-8582, Japan; Department of Health Sciences, Kyushu University, Fukuoka 812-8582, Japan; Department of Health Sciences, Kyushu University, Fukuoka 812-8582, Japan",2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),26-Sep-13,2013,,,2988,2991,"We have developed an automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of treatment planning computed tomography (CT) and 18F-fluorodeoxyglucose (FDG)-positron emission tomography (PET)/CT images. First, the PET images were registered with the treatment planning CT images through the diagnostic CT images of PET/CT. Second, six voxel-based features including voxel values and magnitudes of image gradient vectors were derived from each voxel in the planning CT and PET /CT image data sets. Finally, lung tumors were extracted by using a support vector machine (SVM), which learned 6 voxel-based features inside and outside each true tumor region determined by radiation oncologists. The results showed that the average DSCs for 3 and 6 features for three cases were 0.744 and 0.899, and thus the SVM may need 6 features to learn the distinguishable characteristics. The proposed method may be useful for assisting treatment planners in delineation of the tumor region.",1558-4615,978-1-4577-0216-7,10.1109/EMBC.2013.6610168,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610168,,Computed tomography;Positron emission tomography;Tumors;Planning;Support vector machines;Image segmentation;Lungs,computerised tomography;gradient methods;learning (artificial intelligence);lung;medical image processing;positron emission tomography;support vector machines;tumours,lung tumor;machine learning classifier;radiation oncologist;FDG-PET/CT images;treatment planning computed tomography;18F-fluorodeoxyglucose;positron emission tomography;diagnostic CT image;voxel-based feature;image gradient vector;support vector machine;SVM,"Algorithms;Artificial Intelligence;Automation;Fluorodeoxyglucose F18;Humans;Image Processing, Computer-Assisted;Knowledge;Lung Neoplasms;Multimodal Imaging;Positron-Emission Tomography;Radiopharmaceuticals;Support Vector Machine;Tomography, X-Ray Computed;Tumor Burden",2,,18,,26-Sep-13,,,IEEE,IEEE Conferences
PET/CT Radiomic Sequencer for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients,I. Shiri; H. Maleki; G. Hajianfar; H. Abdollahi; S. Ashrafinia; M. G. Oghli; M. Hatt; M. Oveisi; A. Rahmim,"Biomedical and Health Informatics Department, Rajaie Cardiovascular Medical and Research Center, Tehran, Iran; Biomedical and Health Informatics Department, Rajaie Cardiovascular Medical and Research Center, Tehran, Iran; Biomedical and Health Informatics Department, Rajaie Cardiovascular Medical and Research Center, Tehran, Iran; Biomedical and Health Informatics Department, Rajaie Cardiovascular Medical and Research Center, Tehran, Iran; Electrical and Computer Engineering Department, Johns Hopkins University, Baltimore, MD, USA; Biomedical and Health Informatics Department, Rajaie Cardiovascular Medical and Research Center, Tehran, Iran; Biomedical and Health Informatics Department, Rajaie Cardiovascular Medical and Research Center, Tehran, Iran; INSERM, UMR 1101, LaTIM, Univ Brest, Brest, F-29238, France; Radiology and Physics & Astronomy Departments, University of British Columbia, Vancouver, Canada",2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC),05-Sep-19,2018,,,1,4,"The aim of this study was to develop radiomic models using PET/CT radiomic features with different machine learning approaches for finding best predictive epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) mutation status. Patient's images including PET and CT [diagnostic (CTD) and low dose CT (CTA)] were pre-processed using wavelet (WAV), Laplacian of Gaussian (LOG) and 64 bin discretization (BIN) (alone or in combinations) and several features from images were extracted. The prediction performance of model was checked using the area under the receiver operator characteristic (ROC) curve (AUC). Results showed a wide range of radiomic model AUC performances up to 0.75 in prediction of EGFR and KRAS mutation status. Combination of K-Best and variance threshold feature selector with logistic regression (LREG) classifier in diagnostic CT scan led to the best performance in EGFR (CTD-BIN+B-KB+LREG, AUC: 0.75±0.10) and KRAS (CTD-BIN-LOG-WAV+B-VT+LREG, AUC: 0.75±0.07) respectively. Additionally, incorporating PET, kept AUC values at ~0.74. When considering conventional features only, highest predictive performance was achieved by PET SUVpeak (AUC: 0.69) for EGFR and by PET MTV (AUC: 0.55) for KRAS. In comparison with conventional PET parameters such as standard uptake value, radiomic models were found as more predictive. Our findings demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients.",2577-0829,978-1-5386-8494-8,10.1109/NSSMIC.2018.8824469,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824469,,Feature extraction;Predictive models;Computed tomography;Cancer;Lung,cancer;computerised tomography;feature extraction;image sequences;learning (artificial intelligence);medical image processing;positron emission tomography;regression analysis;sensitivity analysis;tumours,standard uptake value;bin discretization;Laplacian of Gaussian;Kirsten rat sarcoma viral oncogene mutation status;epidermal growth factor receptor;PET-CT radiomic sequencer;AUC values;area under the receiver operator characteristic curve;machine learning approaches;reliable radiomics analysis;conventional PET parameters;diagnostic CT scan;logistic regression classifier;variance threshold feature selector;radiomic model AUC;prediction performance;low dose CT;NSCLC patients;KRAS mutation status;EGFR,,2,,16,,05-Sep-19,,,IEEE,IEEE Conferences
CT-guided PET Image Denoising using Deep Neural Network without Prior Training Data,J. Cui; K. Gong; N. Guo; X. Meng; K. Kim; H. Liu; Q. Li,"State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, Hangzhou, Zhejiang, 310027, China; Department of Radiology, Massachusetts General Hospital, Boston, MA, 02114, USA; Department of Radiology, Massachusetts General Hospital, Boston, MA, 02114, USA; Gordon Center for Medical Imaging, Massachusetts General Hospital; State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, Hangzhou, Zhejiang, 310027, China; Department of Radiology, Massachusetts General Hospital, Boston, MA, 02114, USA; State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, Hangzhou, Zhejiang, 310027, China",2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC),05-Sep-19,2018,,,1,3,Deep neural network have been a powerful tool for computer vision tasks and generally used in medical image [1]-[4].,2577-0829,978-1-5386-8494-8,10.1109/NSSMIC.2018.8824397,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824397,,Neural networks;Biomedical imaging;Computed tomography;Noise measurement;Lesions;Noise reduction;Optical filters,computer vision;computerised tomography;image denoising;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography,medical image;computer vision tasks;deep neural network;CT-guided PET image denoising,,2,,6,,05-Sep-19,,,IEEE,IEEE Conferences
Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia; Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia; Biomedical and Multimedia Information Technology (BMIT) Research Group, School of Information Technologies, University of Sydney, Australia",2012 9th IEEE International Symposium on Biomedical Imaging (ISBI),12-Jul-12,2012,,,482,485,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",1945-8452,978-1-4577-1858-8,10.1109/ISBI.2012.6235589,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235589,PET-CT;abnormality;global contexts;max-margin;detection,Lungs;Context;Tumors;Computed tomography;Positron emission tomography;Thorax;Feature extraction,cancer;feature extraction;image classification;inference mechanisms;learning (artificial intelligence);lung;medical image processing;positron emission tomography,global context inference;adaptive abnormality detection;lung cancers;computer-based abnormality detection;thorax;max-margin learning-based method;local feature classification;clinical PET-CT images,,2,,11,,12-Jul-12,,,IEEE,IEEE Conferences
Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Department of Diagnostic and Interventional Radiology, University Hospital of Tübingen, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Germany; Department of Diagnostic and Interventional Radiology, University Hospital of Tübingen, Germany; Department of Diagnostic and Interventional Radiology, University Hospital of Tübingen, Germany","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",13-Sep-18,2018,,,1065,1069,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2379-190X,978-1-5386-4658-8,10.1109/ICASSP.2018.8462188,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462188,CT;PET;Radiomics;machine learning;support vector machine,Computed tomography;Tumors;Positron emission tomography;Feature extraction;Support vector machines;Training,computerised tomography;learning (artificial intelligence);medical image processing;positron emission tomography;tumours,PET image;PET-CT scan;metabolic activity;machine learning approach;FDG uptake region automated detection;textural variations;tumour;treatment planning,,2,,20,,13-Sep-18,,,IEEE,IEEE Conferences
Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"Electrical Engineering Department, Vanderbilt University, Nashville, TN 37235 USA (Telephone: 615-202-6454, e-mail: zheng.fu@vanderbilt.edu; Vanderbilt University Institute of Image Science, Nashville, TN 37232 USA.; Member IEEE, Vanderbilt University Institute of Image Science, Nashville, TN 37232 USA. Telephone: 615-322-2648 e-mail: todd.e.peterson@vanderbilt.edu",2006 IEEE Nuclear Science Symposium Conference Record,07-May-07,2006,5,,2708,2712,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",1082-3654,1-4244-0560-2,10.1109/NSSMIC.2006.356439,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4179596,,Independent component analysis;Blood;Animals;Positron emission tomography;Mice;Heart;Image segmentation;Data mining;Spatial resolution;Image analysis,biomedical imaging;blood;image resolution;image segmentation;independent component analysis;learning (artificial intelligence);positron emission tomography,ensemble learning;independent component analysis;blood input function;FDG-PET images;small animal PET imaging;blood time-activity curve;spatial resolution;partial volume effects;cardiac motion;Bayesian method;image segmentation;EL-ICA;myocardium;blood pool component,,2,,8,,07-May-07,,,IEEE,IEEE Conferences
Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Department of Nuclear Medicine, Key Laboratory of Carcinogenesis and Translational Research (Ministry of Education), Peking University Cancer Hospital and Institute, Beijing, China; Department of Nuclear Medicine, Key Laboratory of Carcinogenesis and Translational Research (Ministry of Education), Peking University Cancer Hospital and Institute, Beijing, China; Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Department of Nuclear Medicine, Key Laboratory of Carcinogenesis and Translational Research (Ministry of Education), Peking University Cancer Hospital and Institute, Beijing, China; Central Research Institute, UIH Group, Beijing, China; Central Research Institute, UIH Group, Beijing, China; College of Quality and Technical Supervision, Hebei University, Baoding, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China; Shanghai Research Center for Brain Science and Brain-Inspired Technology, Shanghai, China; Department of Nuclear Medicine, Key Laboratory of Carcinogenesis and Translational Research (Ministry of Education), Peking University Cancer Hospital and Institute, Beijing, China; Institute of Medical Technology, Peking University Health Science Center, Peking University, Beijing, China; Shanghai Research Center for Brain Science and Brain-Inspired Technology, Shanghai, China",IEEE Transactions on Medical Imaging,02-Feb-22,2022,41,2,407,419,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",1558-254X,,10.1109/TMI.2021.3113365,Beijing Natural Science Foundation(grant numbers:Z210008); Shenzhen Science and Technology Program(grant numbers:1210318663); Hebei Natural Science Foundation(grant numbers:H2019201378); Shenzhen Nanshan Innovation and Business Development Grant; Shanghai Municipal of Science and Technology Project(grant numbers:20JC1419500); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540600,Deep learning;low dose;CT;MR;PET;image restoration,Biomedical imaging;Noise reduction;Image denoising;Computed tomography;Generative adversarial networks;Task analysis;Image reconstruction,image denoising;learning (artificial intelligence);medical image processing,medical imaging denoising;image domain;innovative deep learning strategies;content-noise complementary learning strategy;deep learning predictors;image dataset;medical image denoising pipeline;CNCL strategy;medical imaging datasets;medical image denoising tasks,"Algorithms;Image Processing, Computer-Assisted;Signal-To-Noise Ratio;Tomography, X-Ray Computed",2,,52,IEEE,16-Sep-21,,,IEEE,IEEE Journals
Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA; Center for Biomedical Image Computing and Analysis, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, 19104, USA",2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),11-Jul-19,2019,,,1303,1306,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",1945-8452,978-1-5386-3641-1,10.1109/ISBI.2019.8759512,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759512,Collaborative clustering;unsupervised learning;nonnegative matrix tri-factorization;radiomics;patient stratification;rectal cancer,Cancer;Predictive models;Collaboration;Principal component analysis;Feature extraction;Dimensionality reduction,cancer;computerised tomography;feature extraction;learning (artificial intelligence);matrix decomposition;medical image processing;pattern clustering;positron emission tomography,effective low-dimensional feature representations;FDG-PET-CT dataset;distinct survival patterns;radiomic feature clusters;highly correlated radiomic features;rectal cancer patients;clinical outcomes,,2,,19,,11-Jul-19,,,IEEE,IEEE Conferences
DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks,V. S. S. Kandarpa; A. Bousse; D. Benoit; D. Visvikis,"LaTIM, INSERM, UMR 1101, Université de Bretagne Occidentale, Brest, France; LaTIM, INSERM, UMR 1101, Université de Bretagne Occidentale, Brest, France; LaTIM, INSERM, UMR 1101, Université de Bretagne Occidentale, Brest, France; LaTIM, INSERM, UMR 1101, Université de Bretagne Occidentale, Brest, France",IEEE Transactions on Radiation and Plasma Medical Sciences,30-Dec-20,2021,5,1,44,53,"This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.",2469-7303,,10.1109/TRPMS.2020.3033172,French Ministry of Education and Research through a Ph.D. Scholarship; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9235522,Deep learning;generative adversarial networks (GANs);medical image reconstruction,Image reconstruction;Deep learning;Computer architecture;Image segmentation;Biomedical imaging;Noise reduction;Positron emission tomography,computerised tomography;convolutional neural nets;image denoising;image reconstruction;image resolution;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;positron emission tomography,DUG-RECON;direct image reconstruction;convolutional generative networks;iterative reconstruction algorithms;medical image reconstruction;projection domain data;image domain;trained deep learning networks;direct reconstruction framework;deep learning architectures;denoising reconstruction;U-Net generator;sinogram-to-image transformation;computed tomography images;positron emission tomography;PET;CT images,,2,,32,IEEE,22-Oct-20,,,IEEE,IEEE Journals
Cross-Modality Medical Image Retrieval with Deep Features,A. Mbilinyi; H. Schuldt,"University of Basel,Department of Mathematics and Computer Science,Basel,Switzerland; University of Basel,Department of Mathematics and Computer Science,Basel,Switzerland",2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),13-Jan-21,2020,,,2632,2639,"In medical imaging, modality refers to the technique and process used to create visual representations of a particular part of the body, organs, or tissues. Conventional modalities include X-ray, CT-scan, Magnetic Resonance Imaging (MRI), Ultrasound, and Positron Emission Tomography (PET). Depending on the modality used, the same disease can be detected differently, making a modality an essential filter in evaluating the relevance of search results when retrieving similar medical images. Traditionally, texture features have been used for content-based medical image retrieval. However, texture features are limited in capturing the semantic similarity between medical images, let alone their modalities. This paper explores deep features (features extracted by deep convolutional neural networks (CNN)) and analyzes their effectiveness in retrieving similar medical images, semantic-wise and modality-wise, from a collection with different medical image modalities. We have examined CNNs of different architectures pre-trained in natural images and CNNs we fine-tuned and fully-trained from scratch in medical images to extract deep features. Based on retrieval performance evaluation, we show that deep features, even though extracted by CNN pre-trained in natural images, still outperform texture features. On the other end, we show that deep features extracted by a smaller, simpler, and yet computationally efficient CNN we trained in medical images can compete with large and complex ImageNet CNNs fine-tuned or fully trained in medical images.",,978-1-7281-6215-7,10.1109/BIBM49941.2020.9313211,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313211,Content-Based Medical Image Retrieval;Cross-Modality;Deep Features,Feature extraction;Medical diagnostic imaging;X-ray imaging;Computed tomography;Task analysis;Image retrieval;Magnetic resonance imaging,biomedical MRI;biomedical ultrasonics;computerised tomography;convolutional neural nets;diseases;feature extraction;image retrieval;image texture;learning (artificial intelligence);medical image processing;positron emission tomography,deep CNN;cross-modality medical image retrieval;texture feature extraction;organs;tissues;X-ray;CT-scan;magnetic resonance imaging;MRI;positron emission tomography;PET;ultrasound;deep convolutional neural networks;deep feature extraction,,2,,44,,13-Jan-21,,,IEEE,IEEE Conferences
Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"University of Texas, Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas, Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas, Southwestern Medical Center, Dallas, TX, 75390, USA; School of Electronic and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; University of Texas, Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas, Southwestern Medical Center, Dallas, TX, 75390, USA; University of Texas, Southwestern Medical Center, Dallas, TX, 75390, USA",2019 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),12-Sep-19,2019,,,1,4,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2641-3604,978-1-7281-0848-3,10.1109/BHI.2019.8834658,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834658,Lymph node metastasis;Head & neck cancer;Automated multi-objective learning (AutoMO);Multi-objective optimization;Evidential reasoning,Reliability;Cancer;Predictive models;Computed tomography;Training;Positron emission tomography;Sensitivity and specificity,cancer;computerised tomography;image fusion;image reconstruction;learning (artificial intelligence);medical image processing;Pareto optimisation;positron emission tomography;tumours,lymph node metastasis;head cancer;neck cancer;single-modality-based models;lymph node metastasis;positron emission tomography;evidential reasoning approach;ER fusion;automated multiobjective model;Pareto-optimal models;AutoMO;multiobjective learning model;LNM status;computed tomography,,2,,14,,12-Sep-19,,,IEEE,IEEE Conferences
A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"VNR Vignana Jyothi Institute of Engineering and Technology,Department of ECE,Hyderabad,TS,India; VNR Vignana Jyothi Institute of Engineering and Technology,Department of ECE,Hyderabad,TS,India",2020 IEEE Bangalore Humanitarian Technology Conference (B-HTC),31-Dec-20,2020,,,1,6,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",,978-1-7281-8794-5,10.1109/B-HTC50970.2020.9297936,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9297936,Retinal Imaging;Biomarkers;Health Risk Factors;Cardiovascular diseases;Cerebrovascular diseases,Retina;Diseases;Biomarkers;Stroke (medical condition);Kidney;Imaging;Heart,biomedical MRI;biomedical optical imaging;biomedical ultrasonics;blood vessels;cancer;diseases;health care;health hazards;kidney;laser applications in medicine;medical image processing;optical tomography;patient treatment;positron emission tomography;reviews,cerebrovascular diseases;retinal vascular imaging;retinal images;disorders;hypertension;diabetes;pulmonary diseases;cerebrovascular diseases;chronicle kidney diseases;magnetic resonance imaging;positron emission tomography;ultrasonography scans;computer tomography;artificial intelligence,,2,,66,,31-Dec-20,,,IEEE,IEEE Conferences
3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"QuantIF-LITIS - Equipe Quantification en Imagerie Fonctionnelle LITIS, University of Rouen, France; CHB Hospital, Rue d’Amiens, Rouen, France; QuantIF-LITIS - Equipe Quantification en Imagerie Fonctionnelle LITIS, University of Rouen, France; GREYC, University of Caen, France; QuantIF-LITIS - Equipe Quantification en Imagerie Fonctionnelle LITIS, University of Rouen, France","2018 Eighth International Conference on Image Processing Theory, Tools and Applications (IPTA)",13-Jan-19,2018,,,1,5,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",2154-512X,978-1-5386-6428-5,10.1109/IPTA.2018.8608129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8608129,Positron Emission Tomography (PET);Lymphoma detection;supervoxel;Conditional random fields,Image segmentation;Three-dimensional displays;Positron emission tomography;Computed tomography;Medical services;Mathematical model;Sensitivity,biological organs;cancer;computerised tomography;diseases;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography,computed tomography scan;machine learning technique;learning database;standard database;medical field;supervoxel;anatomic atlas;lymphoma regions;lymphoma detection method;positron-emission tomography;conditional random fields;image PET-CT images,,1,,11,,13-Jan-19,,,IEEE,IEEE Conferences
Classification of Infiltrated Injections During PET/CT Imaging Applying Deep Learning Technique,T. R. Tumpa; S. N. Acuff; D. R. Osborne,"University of Tennessee, United States; University of Tennessee, United States; University of Tennessee, United States",2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),13-Feb-20,2019,,,1781,1785,"Objective: Injected dose infiltration can negatively impact quantitative evaluation of Positron Emission Tomography (PET) data by leading to inaccurate calculation of Standardized Uptake Value (SUV) measurements and limiting bioavailability of the tracer in the patient. Recently developed topical gamma scintillation sensors provide a way to monitor Time Activity Curves (TACs) and determine the presence of activity remaining at the injection site after injecting dose. However, TAC analysis and visual inspection by physician of static PET images differ in many cases which has been a recent research concern. In this work, a deep learning (DL) based classification was implemented to study whether this approach can be a viable solution to classify the injection data based on their quality. Method: A supervised machine learning technique was adopted and TACs obtained from the sensors were fed as input to a neural network. The network was trained to classify two classes of data i.e. good quality and poor quality injections. The performance of the network was tested on the basis of 3-fold cross-validation. Result: The network could label the good quality (92.39% data) and poor quality (7.61% data) injection data with around <sub>~</sub>98% and <sub>~</sub>86% accuracy respectively with an overall accuracy of <sub>~</sub>97%. Conclusion: The objective of this work was to examine the feasibility of implementing a DL approach for PET dose injection quality monitoring.",2375-0197,978-1-7281-3798-8,10.1109/ICTAI.2019.00267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995254,"Positron Emission Tomography/Computed Tomography (PET/CT), FDG dose infiltration, Dose extravasation, Injection quality classification, Time Activity Curve (TAC), Deep Learning (DL), 1-Dimensional Convolutional Neural Network (1D CNN), LENET",,computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography,PET-CT imaging;injected dose infiltration;standardized uptake value measurements;supervised machine learning;deep learning based classification;static PET images;TAC analysis;injecting dose;injection site;time activity curves;topical gamma scintillation sensors;positron emission tomography data;infiltrated injections;PET dose injection quality monitoring;neural network,,1,,18,,13-Feb-20,,,IEEE,IEEE Conferences
Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Department of Radiation Oncology, University Hospitals Cleveland Medical Center, Cleveland, OH, USA; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Department of Biomedical Engineering, Case Western Reserve University, Cleveland, OH, USA; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Department of Radiology, School of Medicine, Case Western Reserve University, Cleveland, OH, USA; Department of Radiology, School of Medicine, Case Western Reserve University, Cleveland, OH, USA; Department of Radiation Oncology, School of Medicine, Case Western Reserve University, Cleveland, OH, USA; Department of Radiation Oncology, Penn State Cancer Institute, Hershey, PA, USA; Department of Radiation Oncology, University Hospitals Cleveland Medical Center, Cleveland, OH, USA; Philips Healthcare, Cleveland, OH, USA; Department of Radiology, School of Medicine, Case Western Reserve University, Cleveland, OH, USA; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Department of Radiation Oncology, Penn State Cancer Institute, Hershey, PA, USA; Department of Biomedical Engineering, Case Western Reserve University, Cleveland, OH, USA",IEEE Access,29-Jan-21,2021,9,,17208,17221,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",2169-3536,,10.1109/ACCESS.2021.3049781,"National Cancer Institute of the National Institute of Health, USA(grant numbers:R01CA196687); YES Award through the Department of Radiology, School of Medicine, Case Western Reserve University, Cleveland, OH, USA(grant numbers:R25CA221718); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316666,Cervical cancer;computed tomography;deep learning;generative adversarial network;magnetic resonance imaging;U-Net,Computed tomography;Cervical cancer;Gallium nitride;Biomedical applications of radiation;Attenuation;Planning;Pelvis,biological organs;biomedical MRI;cancer;computerised tomography;deep learning (artificial intelligence);feature extraction;image registration;medical image processing;positron emission tomography;radiation therapy,turbo spin echo single shot MR images;U-Net;deep learning;feature extraction;FDG-PET attenuation correction;electron density;18F-labelled 2-fluoro-2-deoxy-D-glucose positron emission tomography;pelvis;MR-CT registration;sU-cGAN;computed tomography images;magnetic resonance images;sU-Net model;CT generation;conditional generative adversarial network;cervical cancer diagnosis;MR-CT image translation;radiation therapy planning,,1,,66,CCBY,08-Jan-21,,,IEEE,IEEE Journals
Artificial Intelligence in Radiation Therapy,Y. Fu; H. Zhang; E. D. Morris; C. K. Glide-Hurst; S. Pai; A. Traverso; L. Wee; I. Hadzic; P. -I. L&#x00F8;nne; C. Shen; T. Liu; X. Yang,"Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, USA; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Radiation Oncology, University of California at Los Angeles, Los Angeles, CA, USA; Department of Human Oncology, School of Medicine and Public Health, University of Wisconsin&#x2013;Madison, Madison, WI, USA; Department of Radiotherapy (MAASTRO), Maastricht University Medical Centre, Maastricht, The Netherlands; Department of Radiotherapy (MAASTRO), Maastricht University Medical Centre, Maastricht, The Netherlands; Department of Radiotherapy (MAASTRO), Maastricht University Medical Centre, Maastricht, The Netherlands; Department of Radiotherapy (MAASTRO), Maastricht University Medical Centre, Maastricht, The Netherlands; Department of Medical Physics, Oslo University Hospital, Nydalen, Oslo, Norway; Department of Radiation Oncology, University of Texas Southwestern Medical Center, Dallas, TX, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, USA; Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA, USA",IEEE Transactions on Radiation and Plasma Medical Sciences,02-Feb-22,2022,6,2,158,181,"Artificial intelligence (AI) has great potential to transform the clinical workflow of radiotherapy. Since the introduction of deep neural networks (DNNs), many AI-based methods have been proposed to address challenges in different aspects of radiotherapy. Commercial vendors have started to release AI-based tools that can be readily integrated to the established clinical workflow. To show the recent progress in AI-aided radiotherapy, we have reviewed AI-based studies in five major aspects of radiotherapy, including image reconstruction, image registration, image segmentation, image synthesis, and automatic treatment planning. In each section, we summarized and categorized the recently published methods, followed by a discussion of the challenges, concerns, and future development. Given the rapid development of AI-aided radiotherapy, the efficiency and effectiveness of radiotherapy in the future could be substantially improved through intelligent automation of various aspects of radiotherapy.",2469-7303,,10.1109/TRPMS.2021.3107454,"Dutch Research Council NWO (STW-Perspectief STRaTegy 14930, Indo-Dutch Projects BIONIC 629.002.205 and TRAIN 629.002.212); Queen Wilhemina Foundation KWF (ProTRaIT); Hanarth Foundation; NIH, National Cancer Institute of the National Institutes of Health(grant numbers:R01CA204189); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521554,Artificial intelligence (AI);image reconstruction;image registration;image segmentation;image synthesis;radiotherapy;treatment planning,Image reconstruction;Computed tomography;Artificial intelligence;Tumors;Planning;Magnetic resonance imaging;Imaging,biomedical MRI;computerised tomography;deep learning (artificial intelligence);image reconstruction;image registration;image segmentation;medical image processing;positron emission tomography;radiation therapy,AI-aided radiotherapy;image reconstruction;image registration;image segmentation;image synthesis;intelligent automation;artificial intelligence;deep neural networks;automatic treatment planning;CT image;MRI image;PET image,,1,,307,IEEE,24-Aug-21,,,IEEE,IEEE Journals
Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Department of Biomedical Engineering, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Internal Medicine (Cardiology), Yale University, New Haven, CT, USA; Department of Internal Medicine (Cardiology), Yale University, New Haven, CT, USA; Department of Internal Medicine (Cardiology), Yale University, New Haven, CT, USA; Department of Biomedical Engineering and the Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA",IEEE Transactions on Medical Imaging,30-Nov-21,2021,40,12,3293,3304,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1558-254X,,10.1109/TMI.2021.3082578,"American Heart Association(grant numbers:18PRE33990138); NIH(grant numbers:R01CA224140,R01EB025468,R01HL154345,R03EB027209); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438949,PET;myocardial perfusion;motion correction;deep learning,Dynamics;Myocardium;Imaging;Training;Blood;Positron emission tomography;Motion detection,biomedical MRI;cardiology;computerised tomography;haemorheology;image reconstruction;image registration;learning (artificial intelligence);medical image processing;motion estimation;pneumodynamics;positron emission tomography,simulated motion;motion ground-truths;65 patient scans;20 patient scans;additional 10 patient datasets;conventional registration-based methods;motion estimation;realistic patient motion;automatic inter-frame patient motion correction;dynamic cardiac PET;dynamic PET imaging;myocardial blood flow estimation;substantial tracer distribution change;different dynamic frames;image registration-based motion correction;early dynamic frames;automatic deep learning-based motion correction method;inter-frame rigid translational motion;voluntary body movement;pattern change;respiratory motion;4D dynamic image data;motion detection;motion-free patient scans,"Deep Learning;Humans;Image Processing, Computer-Assisted;Motion;Movement;Positron-Emission Tomography",1,,41,CCBY,21-May-21,,,IEEE,IEEE Journals
Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China",IEEE Access,10-Jun-21,2021,9,,81818,81827,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",2169-3536,,10.1109/ACCESS.2021.3085601,"National Natural Science Foundation of China(grant numbers:81871437,12026601); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019A1515011104,2020A1515110683,2021A1515011676); China Postdoctoral Science Foundation(grant numbers:2020M682792); Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme, in 2018; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446049,Peri-tumor;PET/CT;radiomics;head&neck cancer;prognosis,Feature extraction;Training;Predictive models;Licenses;Radiomics;Data models;Quantization (signal),biological organs;cancer;feature extraction;medical image processing;patient treatment;positron emission tomography;tumours,neck cancer;peri-tumoral radiomics features;CT radiomics features;clinical features;clinical-PET-CT;head cancer;feature extraction;intra-tumoral region,,1,,55,CCBY,03-Jun-21,,,IEEE,IEEE Journals
Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, 94305, USA; Center for Gamma-Ray Imaging, University of Arizona, Tucson, AZ, 85721, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, 94305, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, 94305, USA",2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC),05-Sep-19,2018,,,1,3,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",2577-0829,978-1-5386-8494-8,10.1109/NSSMIC.2018.8824444,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824444,PET/MR;MR-based attenuation map;attenuation correction;deep learning;generative adversarial network (GAN);conditional GAN,Generators;Attenuation;Training;Gallium nitride;Generative adversarial networks;Bones;Head,biomedical MRI;bone;computerised tomography;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography,PET/CT based photon attenuation map;bone delineation;photon attenuation map;MR-based attenuation map;head and neck areas;PET image reconstruction;empirical attenuation coefficients;MR image segmentation;conditional adversarial networks;pixel prediction error;basic autoencoder network;image pairs;discriminator loss;generator loss;conditional GAN;fake examples;deep learning network;image translation task;generative adversarial network;electron volt energy 511.0 keV,,1,,5,,05-Sep-19,,,IEEE,IEEE Conferences
A novel convolutional neural network for predicting full dose from low dose PET scans,A. Sanaat; H. Arabi; H. Zaidi,"Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211",2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),09-Apr-20,2019,,,1,3,"The use of radiolabeled tracers in PET imaging raises concerns owing to potential risks from radiation exposure. Therefore, to reduce this potential risk in diagnostic PET imaging, efforts have been made to decrease the amount of radiotracer administered to the patient. However, decreasing the injected activity reduces the signal-to-noise Ratio (SNR) and deteriorates image quality, thus adversely impacting clinical diagnosis. Previously proposed techniques are complicated and slow, yet they yield satisfactory results at significantly low dose. In this work, we propose a deep learning algorithm to reconstruct full-dose (FD) from low-dose (LD) PET images using a fully convolutional encoder-decoder deep neural network model. The goal is to train a model to learn to reconstruct from images with only 5% of the counts to produce images corresponding to 100% of the dose. Brain PET/CT images of 140 patients acquired on the Siemens Biograph mCT with a standard injected activity of <sup>18</sup>F-FDG (205 ± 10 MBq). Images were acquired for about 20 min. The sinograms of each scan were used to produce a low-dose sinogram by randomly selecting only 1/20<sup>th</sup> of the counts. To avoid over fitting, data augmentation was used. A modified 3D U-Net, was developed to predict standard-dose sinogram (PSS) from their corresponding LD sinogram. Detailed quantitative and qualitative comparison demonstrated the proposed method can generate artefact-free diagnostic quality images that preserve internal structures without noise amplification. The structural similarity index (SSIM) and peak signal to noise ratio (PSNR) were used as quantitative metrics for assessment. For instance, the PSNR and SSIM in selected slices were 37.30±0.71 and 0.97±0.02, respectively. The proposed algorithm operates in the projection space and is capable of producing diagnostic quality images with only 5% of the standard injected activity.",2577-0829,978-1-7281-4164-0,10.1109/NSS/MIC42101.2019.9059962,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9059962,,Image reconstruction;Three-dimensional displays;Machine learning;Image quality;Training;Imaging;Convolution,brain;computerised tomography;convolutional neural nets;dosimetry;image reconstruction;learning (artificial intelligence);medical image processing;positron emission tomography,structural similarity index;modified 3D U-Net;data augmentation;brain PET-CT images;Siemens Biograph mCT;LD sinogram;artefact-free diagnostic quality images;standard-dose sinogram;low-dose sinogram;fully convolutional encoder-decoder deep neural network model;low-dose PET images;deep learning algorithm;image quality;signal-to-noise ratio;diagnostic PET imaging;radiation exposure;radiolabeled tracers;low dose PET scans,,1,,6,,09-Apr-20,,,IEEE,IEEE Conferences
Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Department of Statistics, School of Mathematical Sciences, University College Cork, Cork, Ireland; Department of Statistics, School of Mathematical Sciences, University College Cork, Cork, Ireland; School of Medicine, Royal College of Surgeons in Ireland, Dublin, Ireland; Department of Medical Epidemiology and Biostatistics, Karolinska Institutet, Solna, Sweden; PET/CT Unit (Alliance Medical), Cork University Hospital, Cork, Ireland; Department of Statistics, School of Mathematical Sciences, University College Cork, Cork, Ireland; Department of Radiology, Cork University Hospital, Cork, Ireland",IEEE Transactions on Radiation and Plasma Medical Sciences,01-Jul-19,2019,3,4,421,433,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",2469-7303,,10.1109/TRPMS.2019.2912433,"Science Foundation Ireland(grant numbers:SFI-PI 11/1027); National Cancer Institute(grant numbers:NCI R33-CA225310,P01-CA042045); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698323,FDG-PET;heterogeneity;machine learning;metabolic gradient;nonsmall cell lung cancer (NSCLC);prognosis;radiomics;spatial modeling;texture,Tumors;Machine learning;Lung cancer;Positron emission tomography;Radiomics;Image texture analysis,cancer;computerised tomography;image texture;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,traditional radiomics sense;improved baseline multivariate models;statistical significance;prognostic variable selection;NSCLC patients;structural variables;two-year survival status;logistic models;stepwise selection;prognostic model selection;conventional texture features;tumour metabolism;variables capture aspects;local metabolic gradients;statistical summaries;heterogeneity variables;structural features;intratumoral voxels;radial position;univariate uptake profile function;best-fitting ellipsoidal pattern;3-D FDG-PET uptake;spatial distribution;alternative model-based approach;nonsmall cell lung cancer;texture analyses;prognostic utility;volumetric FDG-PET uptake;textural assessments,,1,,55,OAPA,24-Apr-19,,,IEEE,IEEE Journals
Three-dimensional shape completion using deep convolutional neural networks: Application to truncation compensation and metal artifact reduction in PET/MRI attenuation correction,H. Arabi; H. Zaidi,"Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland,CH-1211",2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),09-Apr-20,2019,,,1,3,"Accurate attenuation correction (AC) of PET data is a prerequisite for quantitative PET/MR imaging. However, MR images are susceptible to metal artifacts leading to void MR signal around metallic implants. Moreover, for large patients, exceeding the scanner's field-of-view (FOV), MR images of the body will be truncated, mostly in the arms, thus hampering the accurate delineation of the body contour. Both metal-induced artifacts and body truncation affect PET AC by causing segmentation errors in MRI-based attenuation map generation. In this work, a deep learning convolutional neural network-based algorithm is proposed for the completion of MR images affected by body truncation or metal-induced artifacts. The core of the network utilizes dilated convolutions and residual connections to render an end-to-end 3D shape completion. The training of the network was performed using co-registered PET, CT and MR whole-body images of 15 patients. The evaluation of the proposed method was carried out on 10 patients with severe metal-induced artifacts and truncated MR images. Body contours from the corresponding non-attenuation corrected PET (non-AC PET) or CT images were segmented to estimate the amount of truncation in MR images. The estimated truncated volumes were later used as reference to assess the efficiency of the proposed method to recover the truncated or metal artifact affected areas. Moreover, the impact of the truncation compensation and metal-induced artifact reduction was investigated in the context of segmentation-based PET/MRI attenuation correction. The activity recovery in the affected areas was estimated before and after application of the shape completion method. The body truncation affected 11.1±2.3% of the body volume and consequently the MRI segmentation-based attenuation maps of 10 patients. After shape completion using the proposed method, the amount of truncated volume dropped to 0.7±0.2%. The SUV bias in the truncated area improved from -44.5±10% to -10.5±3% considering PET-CT AC as reference. Likewise, 8.5±1.9% of the head volume was affected by the metal-induced artifact leading to SUV bias of -59.5±11%. These were reduced to 0.3±0.1% and -23.5±9%, respectively, after shape completion. It was concluded that the proposed algorithm exhibited promising results towards the completion of MRI affected by truncation and metal-induced artifacts in whole-body PET/MRI.",2577-0829,978-1-7281-4164-0,10.1109/NSS/MIC42101.2019.9059660,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9059660,,Attenuation;Magnetic resonance imaging;Computed tomography;Shape;Image segmentation;Metals;Head,biomedical MRI;computerised tomography;convolutional neural nets;image denoising;image registration;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography,metal-induced artifact;truncation compensation;metal artifact reduction;metallic implants;body truncation;MRI-based attenuation map generation;deep learning convolutional neural network-based algorithm;whole-body images;nonattenuation corrected PET;truncated volume;truncated area;three-dimensional shape completion;quantitative PET-MR imaging;field-of-view;body contour;segmentation errors;dilated convolutions;residual connections;end-to-end 3D shape completion;segmentation-based PET-MRI attenuation correction;metal artifact affected areas;activity recovery;body volume;MRI segmentation-based attenuation maps;head volume,,1,,7,,09-Apr-20,,,IEEE,IEEE Conferences
Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Shri Mata Vaishno Devi University,School of Computer Science and Engineering,Katra,Jammu and Kashmir,India,182320; Shri Mata Vaishno Devi University,School of Computer Science and Engineering,Katra,Jammu and Kashmir,India,182320",2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),12-Apr-21,2021,,,198,202,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",,978-1-7281-9537-7,10.1109/ICAIS50930.2021.9396016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396016,Medical Imaging;Medical modalities;CT segmentation;Thoracic organs;Heart;esophagus;trachea;aorta;Automatic segmentation;Deep learning,Deep learning;Heart;Image segmentation;Task analysis;Single photon emission computed tomography;Medical diagnostic imaging;Esophagus,biological organs;biomedical MRI;computerised tomography;image segmentation;learning (artificial intelligence);medical image processing,deep learning-based techniques;automatic segmentation;thoracic computed tomography images;medical images;medical diagnosis;medical field;crucial image-related application;organs segmentation,,1,,15,,12-Apr-21,,,IEEE,IEEE Conferences
Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Faculdade de Ciências da Universidade do Porto (FCUP),Porto,Portugal; Medical Physics, Radiobiology and Radiation Protection Group IPO Porto Research Centre (CI-IPOP); Portuguese Institute of Oncology of Porto (IPO-Porto),Radioncology Department,Porto,Portuga; Medical Physics, Radiobiology and Radiation Protection Group, IPO Porto Research Centre (CI-IPOP),Nuclear Medicine Department, IPO-Porto; Portuguese Institute of Oncology of Porto (IPO-Porto) Instituto de Ciências Biomédicas Abel Salazar da Universidade do Porto,Medical Physics Department,Porto,Portugal",2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE),16-Dec-20,2020,,,746,751,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",2471-7819,978-1-7281-9574-2,10.1109/BIBE50027.2020.00127,European Regional Development Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288025,Oesophagic cancer;PET;CT;Classification;Deep Learning;Traditional Learning,Deep learning;Learning systems;Computer vision;Two dimensional displays;Surgery;Feature extraction;Cancer,cancer;computer vision;computerised tomography;convolutional neural nets;feature extraction;feature selection;image classification;learning (artificial intelligence);medical image processing;positron emission tomography;surgery,computer vision;biomedical imaging;deep learning;oesophageal cancer;feature selection;decision level fusion;image features;traditional learning classifiers;oesophagic early-stage cancers classification;CT scans;PET scans;surgery;feature extraction;convolutional neural networks,,1,,22,,16-Dec-20,,,IEEE,IEEE Conferences
Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"A*STAR-NUS, Clinical Imaging Research Centre, Singapore; A*STAR-NUS, Clinical Imaging Research Centre, Singapore; A*STAR-NUS, Clinical Imaging Research Centre, Singapore; Molecular Imaging, Siemens Medical Solutions USA, Inc.; Department of Radiation Oncology, National University Cancer Institute and A*STAR-NUS; Department of Diagnostic Radiology, National University Hospital, Singapore; Department of Diagnostic Imaging, National University Hospital, Singapore; Department of Diagnostic Imaging, National University Hospital, Singapore; Centre for Medical Image Computing and Department of Computer Science, University College of London, UK; A*STAR-NUS, Clinical Imaging Research Centre, Singapore",2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC),05-Sep-19,2018,,,1,3,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",2577-0829,978-1-5386-8494-8,10.1109/NSSMIC.2018.8824292,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824292,Image Quality Transfer;Lesion Detection;Lung Cancer;Machine Learning;Positron Emission Tomography,Lesions;Lung;Standards;Cancer;Positron emission tomography;Image quality,cancer;image reconstruction;learning (artificial intelligence);lung;medical image processing;positron emission tomography;regression analysis,machine learning algorithm;lung cancer;disease screening;disease monitoring;image quality transfer;random regression-forest model;single nonlinear regression tree;lung lesion detection;low dose images;standard dose images;low dose PET scans;high-quality images;low dose protocols;low dose positron emission tomography images,,1,,3,,05-Sep-19,,,IEEE,IEEE Conferences
Deep learning for MRI-based CT synthesis: a comparison of MRI sequences and neural network architectures,A. Larroza; L. Moliner; J. M. Álvarez-Gómez; S. Oliver; H. Espinós-Morató; M. Vergara-Díaz; M. J. Rodríguez-Álvarez,"Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022; Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022; Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022; Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022; Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022; Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022; Instituto de Instrumentación para Imagen Molecular (I3M). Universitat Politècnica de València (UPV)-Consejo Superior de Investigaciones Científicas (CSIC),Valencia,Spain,46022",2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),09-Apr-20,2019,,,1,4,"Synthetic computed tomography (CT) images derived from magnetic resonance images (MRI) are of interest for radiotherapy planning and positron emission tomography (PET) attenuation correction. In recent years, deep learning implementations have demonstrated improvement over atlas-based and segmentation-based methods. Nevertheless, several open questions remain to be addressed, such as which is the best of MRI sequences and neural network architectures. In this work, we compared the performance of different combinations of two common MRI sequences (T1- and T2-weighted), and three state-of-the-art neural networks designed for medical image processing (Vnet, HighRes3dNet and ScaleNet). The experiments were conducted on brain datasets from a public database. Our results suggest that T1 images perform better than T2, but the results further improve when combining both sequences. The lowest mean average error over the entire head (MAE = 101.76 ± 10.4 HU) was achieved combining T1 and T2 scans with HighRes3dNet. All tested deep learning models achieved significantly lower MAE (p <; 0.01) than a well-known atlas-based method.",2577-0829,978-1-7281-4164-0,10.1109/NSS/MIC42101.2019.9060051,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060051,,Magnetic resonance imaging;Computed tomography;Biological neural networks;Head;Machine learning;Convolution,biomedical MRI;brain;computerised tomography;image segmentation;learning (artificial intelligence);medical image processing;neural net architecture,deep learning models;brain datasets;T1-weighted MRI sequences;T2-weighted MRI sequences;positron emission tomography attenuation correction;segmentation-based methods;deep learning implementations;radiotherapy planning;magnetic resonance images;synthetic computed tomography images;neural network architectures;MRI-based CT synthesis;atlas-based method;HighRes3dNet;medical image processing;state-of-the-art neural networks,,1,,16,,09-Apr-20,,,IEEE,IEEE Conferences
A novel approach for medical image fusion using fuzzy logic type-2,H. R. Ramya; B. K. Sujatha,"Dept. of Telecommunication, M.S. Ramaiah Institute of Technology, Bengaluru, India; Dept. of Telecommunication, M.S. Ramaiah Institute of Technology, Bengaluru, India","2016 International Conference on Circuits, Controls, Communications and Computing (I4C)",02-Oct-17,2016,,,1,5,"Medical image fusion is used to integrate the essential features present in different medical images into a single image to improve the clinical accuracy to take better decisions. Multimodal medical image fusion combines the images obtained from different modalities like Positron Emission Tomography (PET), Computed Tomography (CT), Magnetic Resonance Imaging (MRI) and others. CT scan provides detailed information on bony structures whereas MRI scan provides details on soft tissues. Fusion of these images is useful for doctors to diagnose and plan treatment for patients. Various methods have been proposed during recent years for medical image fusion but these methods suffer from the issue of fusion performance due to different sensor modalities. In this paper, we validate that for such images, fusion performance can be improved using a learning based fusion scheme which uses training and testing phase by considering pixel wise processing of image. In order to improve image fusion performance, here we implement fuzzy logic type-2 based approaches for medical image fusion for CT and MRI images. Final fused image is achieved by applying rule based method which includes fuzzification inference, type reduction, and defuzzification of the input image. Experiments are conducted by applying fuzzy logic type-1, neural network, Neuro-fuzzy approach and fuzzy logic type-2 based fusion scheme. Performance of image fusion based on Fuzzy Logic Type-2 is compared with other state-of-art techniques using various performance metrics such as Mutual information, standard deviation and edge based similarity. Experimental results shows that fuzzy logic Type-2 performs better when compared to neural network based approach.",,978-1-5090-5369-8,10.1109/CIMCA.2016.8053286,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053286,Fuzzy logic;neuro-fuzzy;image fusion,Image fusion;Fuzzy logic;Computed tomography;Medical diagnostic imaging;Transforms,biomedical MRI;computerised tomography;fuzzy logic;fuzzy neural nets;fuzzy set theory;image fusion;learning (artificial intelligence);medical image processing;patient diagnosis;patient treatment;positron emission tomography,learning based fusion scheme;MRI images;fuzzy logic type-2 based fusion scheme;multimodal medical image fusion;positron emission tomography;PET;computed tomography;CT;magnetic resonance imaging;soft tissues;patient treatment;patient diagnosis;pixel wise processing;fuzzy logic type-1 based fusion scheme;neural network,,1,,21,,02-Oct-17,,,IEEE,IEEE Conferences
"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"Dept. of Radiat. Oncology (MAASTRO), Univ. Med. Centre Maastricht, Maastricht, Netherlands; Dept. of Radiat. Oncology (MAASTRO), Univ. Med. Centre Maastricht, Maastricht, Netherlands; Dept. of Radiat. Oncology (MAASTRO), Univ. Med. Centre Maastricht, Maastricht, Netherlands; Dept. of Radiat. Oncology (MAASTRO), Univ. Med. Centre Maastricht, Maastricht, Netherlands; Dept. of Knowledge Eng., Maastricht Univ., Maastricht, Netherlands",2011 10th International Conference on Machine Learning and Applications and Workshops,09-Feb-12,2011,2,,220,224,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",,978-1-4577-2134-2,10.1109/ICMLA.2011.139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147677,Non-small cell lung cancer;radiation-induced lung damage;individualized dose treatment planning;clinical prognostic models,Lungs;Computed tomography;Positron emission tomography;Cancer;Tumors;Three dimensional displays;Machine learning,biological tissues;cancer;cellular biophysics;computerised tomography;decision making;dosimetry;learning (artificial intelligence);lung;medical image processing;positron emission tomography;radiation therapy;sensitivity analysis;tumours,radiation-induced normal-tissue toxicity;radiation dosimetry;normal tissues;dose-limiting factor;radiotherapy;radiation-induced lung toxicity;tumor growth;dose treatment planing;low-toxicity-risk patients;clinical-demographic patient characteristics;radiotherapy;functional-metabolic information;PET scanner imaging;CT scanner imaging;machine learning techniques;post-radiation lung damage;deterioration-of-dyspnea patients;machine-learning binary classifiers;ROC curve;decision making processing;nonsmall cell lung cancer patients,,1,,9,,09-Feb-12,,,IEEE,IEEE Conferences
Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"Bharath Corporate, India; Faculty of Engineering, Karpagam University Indian, Coimbatore, TN, India; Faculty of Engineering, Karpagam University Indian, Coimbatore, TN, India; Faculty of Engineering, Karpagam University Indian, Coimbatore, TN, India; MAL, Department of CSA, CEDT, Institute of Sciences, Bengaluru, Karnataka, India","2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing",07-Feb-13,2012,,,822,827,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",,978-1-4673-2925-5,10.1109/PDGC.2012.6449929,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449929,Alzheimer's Disease -AD;Tumors;Artificial Neural Network - ANN;Discrete Wavelet Transform - DWT;Daubechies Wavelet - DW;Competitive Learning;Self Organizing Maps - SOM;Self -Organized Feature Maps -SOFM;Linear Vector Quantization - LVQ,,biomedical MRI;discrete wavelet transforms;diseases;feature extraction;image classification;image denoising;learning (artificial intelligence);medical image processing;patient diagnosis;self-organising feature maps;tumours;vector quantisation,wavelets based decomposition;diseased fMRI brain image classification;inter racial disease types;Alzheimer;tumors;SOFM;LVQ neural networks;inter racial classification;brain image detection;nondiseased syndrome types;functional magnetic resonance images;disease identification;cyst;blurred vision;nausea;unbearable pain;memory loss;abnormal behaviour;motion capturing;featuring detection;feature vector calculation;training;testing;Daubechies wavelet transform;smoothening filter;frequency feature component denoising;frequency feature component decomposition;frequency feature component extraction;median filter;Daubechies discrete wavelet decomposition;competitive learning;self-organized maps;self-organized feature maps;linear vector quantization;SOM,,1,,35,,07-Feb-13,,,IEEE,IEEE Conferences
Towards Extreme-Resolution Image Registration with Deep Learning,A. Nazib; C. Fookes; D. Perrin,"School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, Australia; School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, Australia; School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, Australia",2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),11-Jul-19,2019,,,512,516,"Image registration plays an important role in comparing images. It is particularly important in analysing medical images like CT, MRI and PET, to quantify different biological samples, to monitor disease progression, and to fuse different modalities to support better diagnosis. The recent emergence of tissue clearing protocols enable us to take images at cellular level resolution. Image registration tools developed for other modalities are currently unable to manage images of entire organs at such resolution. The popularity of deep learning based methods in the computer vision community justifies a rigorous investigation of deep-learning based methods on tissue cleared images along with their traditional counterparts. In this paper, we investigate and compare the performance of a deep learning based registration method with traditional optimization based methods on samples from tissue-clearing methods. From the comparative results it is found that a deep-learning based method outperforms all traditional registration tools in terms of registration time and has achieved promising registration accuracy.",1945-8452,978-1-5386-3641-1,10.1109/ISBI.2019.8759291,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759291,Tissue Clearing;Image Registration;Optimization,Tools;Image registration;Image resolution;Deep learning;Biomedical imaging;Magnetic resonance imaging;Optimization,biological tissues;biomedical MRI;biomedical optical imaging;computer vision;computerised tomography;diseases;image registration;image resolution;learning (artificial intelligence);medical image processing;positron emission tomography,disease progression;tissue clearing protocols;cellular level resolution;image registration tools;deep-learning based method;tissue cleared images;deep learning based registration method;tissue-clearing methods;traditional registration tools;registration time;registration accuracy;analysing medical images;biological samples;extreme-resolution image registration,,1,,16,,11-Jul-19,,,IEEE,IEEE Conferences
IEEE Access Special Section Editorial: Deep Learning for Computer-Aided Medical Diagnosis,Y. -D. Zhang; Z. Dong; S. -H. Wang; C. Cattani,"School of Informatics, University of Leicester, Leicester, U.K.; Translational Imaging Division, Columbia University Medical Center, New York, NY, USA; School of Architecture Building and Civil Engineering, Loughborough University, Loughborough, U.K.; Engineering School, University of Tuscia, Viterbo, Italy",IEEE Access,02-Jun-20,2020,8,,96804,96810,"As neuroimaging scanners grow in popularity in hospitals and institutes, the tasks of radiologists are increasing. Emotion, fatigue, and other factors may influence the manual interpretation of results. This manual interpretation suffers from inter- and intra-radiologist variance. Computer-aided medical diagnosis (CAMD) are procedures in medicine that assist radiologists and doctors in the interpretation of medical images, which may come from CT, X-ray, ultrasound, thermography, MRI, PET, SPECT, etc. In practice, CAMD can help radiologists to interpret medical images within seconds. Conventional CAMD tools are built on top of handcrafted features. Recent progress on deep learning opens a new era in which features can be automatically built from a large amount of data. Many important medical projects were launched during the last decade (Human brain project, Blue brain project, Brain Initiative, etc.) that provide massive amounts of data. This emerging big medical data can support the use of deep learning.",2169-3536,,10.1109/ACCESS.2020.2996690,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106898,,,,,,1,,0,CCBY,02-Jun-20,,,IEEE,IEEE Journals
Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"University of Bern,Dept. Nuclear Medicine,Bern,Switzerland; University of Bern,Dept. Nuclear Medicine,Bern,Switzerland; Shanghai Jiao Tong University School of Medicine,Dept. Nuclear Medicine Ruijin Hospital,Shanghai,China; Siemens Healthcare AG,Advanced Clinical Imaging Technology,Lausanne,Switzerland; University of Bern,Dept. Nuclear Medicine,Bern,Switzerland; University of Bern,Dept. Nuclear Medicine,Bern,Switzerland; Shanghai Jiao Tong University School of Medicine,Dept. Nuclear Medicine Ruijin Hospital,Shanghai,China; University of Bern,Dept. Nuclear Medicine,Bern,Switzerland",2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),09-Dec-21,2021,,,4120,4122,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",2694-0604,978-1-7281-1179-7,10.1109/EMBC46164.2021.9630590,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9630590,total-body PET;CT-free;scatter correction;attenuation correction;deep learning,Deep learning;Training;Nuclear medicine;PSNR;Biographies;Computed tomography;Transfer learning,artificial intelligence;computerised tomography;image reconstruction;learning (artificial intelligence);medical image processing;positron emission tomography,CT-free AI-correction;long axial FOV PET scanner;CT-free correction;ultra-long axial field;view PET scanner;low-dose positron emission tomography imaging;high sensitivity long axial field;tomography scanners;noncorrected PET images;body PET images;digital regular FOV PET scanner;long axial FOV scanner;Biograph Vision Quadra;developed deep learning method,"Artificial Intelligence;China;Deep Learning;Humans;Image Processing, Computer-Assisted;Positron-Emission Tomography;Tomography, X-Ray Computed",,,16,,09-Dec-21,,,IEEE,IEEE Conferences
Novel deep learning-based CT synthesis algorithm for MRI-guided PET attenuation correction in brain PET/MR imaging,H. Arabi; G. Zeng; G. Zheng; H. Zaidi,"Division of Nuclear Medicine & Molecular Imaging, Geneva University Hospital, Geneva, CH-1211, Switzerland; Institute for Surgical Technology and Biomechanics, University of Bern, Bern, CH-3014, Switzerland; Institute for Surgical Technology and Biomechanics, University of Bern, Bern, CH-3014, Switzerland; Division of Nuclear Medicine and Molecular Imaging, Geneva University Hospital, Geneva, CH-1211, Switzerland",2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC),05-Sep-19,2018,,,1,3,"MRI-guided synthetic CT (sCT) generation is one of the main challenges hampering quantitative PET/MR imaging as well as MRI-only radiation planning. Deep learning-based approaches have recently gained momentum in a variety of medical imaging applications. In this work, a novel synthetic CT generation algorithm based on deep convolutional neural network is proposed for MRI-guided attenuation correction in PET/MRI. The proposed algorithm (AsCT) exploits adversarial semantic structure learning implemented as a CT segmentation approach to constrain the adversarial synthetic CT generation process. The proposed technique was trained using 50 pairs of CT and MR brain scans under a two-fold<sup>1</sup> cross validation scheme. The AsCT method was compared to an atlas-based method (Bone-Atl), previously developed for MRI-only radiation planning, as well as the commercial segmentation-based approach (2-class) implemented on the Philips TF PET/MRI system. The evaluation was performed using clinical brain studies of 40 patients who have undergone PET/CT and MRI scanning. The accuracy of the CT value estimation and cortical bone identification were assessed for the three different methods taking CT images as reference. Bias of tracer uptake (SUV) was measured on attenuation corrected PET images using the three techniques taking CT-based attenuation corrected PET as reference. Bone-Atl and AsCT exhibited similar cortical bone extraction (using an intensity threshold of 600 Hounsfield Unit (HU)) resulting in Dice coefficient (DSC) of 0.78±0.07 and 0.77±0.07, respectively. Bone-Atl method performed slightly better in terms of accuracy of CT value estimation where a mean absolute error of 123±40 (HU) was obtained for the whole head region while AsCT and 2-class methods led to 141±40 and 230±33 (HU), respectively. Quantitative analysis of brain PET images demonstrated competitive performance of AsCT and Bone-Atl methods where mean relative errors of 1.2±13.8% and 1.0±9.9% were achieved in bony structures, respectively, while the 2-class approach led to a mean SUV error of -14.7±8.9%. The proposed AsCT algorithm showed competitive performance with respect to the atlas-based method and outperformed the segmentation-based (2-class) method with clinically tolerable errors.",2577-0829,978-1-5386-8494-8,10.1109/NSSMIC.2018.8824733,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824733,,Computed tomography;Attenuation;Magnetic resonance imaging;Image segmentation;Bones;Biomedical imaging;Estimation,biomedical MRI;bone;brain;computerised tomography;convolutional neural nets;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;positron emission tomography,segmentation-based approach;deep learning-based CT synthesis algorithm;brain PET-MR imaging;quantitative PET-MR imaging;synthetic CT generation algorithm;adversarial semantic structure learning;two-fold cross validation scheme;Philips TF PET-MRI system;cortical bone extraction;tracer uptake;CT-based attenuation corrected PET;Dice coefficient;Hounsfield Unit;whole head region;bony structures;MRI-guided attenuation correction;deep convolutional neural network;medical imaging applications;MRI-guided synthetic CT generation;MRI-guided PET attenuation correction;AsCT algorithm;brain PET images;2-class methods;Bone-Atl method;attenuation corrected PET images;CT images;cortical bone identification;CT value estimation;clinical brain studies;MRI-only radiation planning;atlas-based method;AsCT method;adversarial synthetic CT generation process;CT segmentation approach,,,,12,,05-Sep-19,,,IEEE,IEEE Conferences
Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea; Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea; Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea; Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea; Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea; Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea; Korea Institutes of Radiological and Medical Sciences (KIRAMS), Seoul, South Korea",2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC),05-Sep-19,2018,,,1,3,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",2577-0829,978-1-5386-8494-8,10.1109/NSSMIC.2018.8824278,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824278,Contrast CT;Conditional generative adversarial network;Lymph node;CT;Adaptive histogram equalization,Computed tomography;Histograms;Biomedical imaging;Adaptive equalizers;Lymph nodes;Deep learning;Generative adversarial networks,blood vessels;cancer;computerised tomography;image enhancement;image segmentation;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography;tumours,anatomical information;tumor S/L;CT scan;generated contrast CT image;histogram equalized CT;image preprocessing;G-CCT image;original contrast CT image;generator;conditional generative adversarial network;deep learning approach;contrast enhanced CT image;blood vessels;contrast CT imaging;contrast CT image generation model,,,,11,,05-Sep-19,,,IEEE,IEEE Conferences
A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"School of Engineering, Cardiff University,Cardiff,UK,CF24 3AA; Wales Research and Diagnostic PET Imaging Centre,Cardiff,UK,CF14 4XN; Wales Research and Diagnostic PET Imaging Centre,Cardiff,UK,CF14 4XN; Wales Research and Diagnostic PET Imaging Centre,Cardiff,UK,CF14 4XN; School of Engineering, Cardiff University,Cardiff,UK,CF24 3AA",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,3,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507846,"King Abdulaziz University, Jeddah, Saudi Arabia(grant numbers:# KAU1938); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507846,PET;Artificial Intelligence;Self-Organizing Map;Texture Analysis;Radiomics;Machine Learning;Cancer,Self-organizing feature maps;Image segmentation;Redundancy;Medical treatment;Tools;Predictive models;Feature extraction,cancer;computerised tomography;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);medical image processing;positron emission tomography;self-organising feature maps;tumours,texture features;texture measurements;different imaging centers;Kohonen self-organizing feature map;performing texture analysis;self-organizing map;radiomic analysis;texture distributions;PET-CT images;post injection imaging time;texture parameters;end point information;self organizing map;exploratory analysis;PET radiomic features,,,,17,,12-Aug-21,,,IEEE,IEEE Conferences
Deep Analysis of Dementia Disorder Using Artificial Intelligence to Improve Healthcare Services,D. Kavitha; A. Murugan; M. Sathiyanarayanan,"SRM Institute of Science and Technology,Department of CSE,Chennai,India; SRM Institute of Science and Technology,Department of CSE,Chennai,India; Innovation & Research,MIT Square,London",2021 International Conference on COMmunication Systems & NETworkS (COMSNETS),17-Feb-21,2021,,,378,380,"Dementia is a worldwide concern and early discovery of dementia is substantial for the administration of mental illness and healthy living. The data acquired from Magnetic resonance imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) are used in identifying dementia. Albeit numerous non-AI and AI techniques have been implemented to understand the reasons behind dementia there is very little success to predict at an early stage. Considering the self-determination medicine, a new healthcare model that could be delivered by 5G, there is a pressing need to diagnose dementia patients in early stages using deep learning techniques and that can constantly improve assessment and diagnostic tools for distinguishing people with normal brain aging from those who will develop mild cognitive impairment. We will conduct state-of-the-art research to identify various deep learning algorithms and approaches that can be used for the diagnosis of dementia and aid in predicting at an early stage. We will develop and evaluate an intelligent algorithm that will predict and distinguish people with normal brain aging from those with mild cognitive impairment on a constant basis, which will be a good fit to the future healthcare model that will be delivered by 5G.",2155-2509,978-1-7281-9127-0,10.1109/COMSNETS51098.2021.9352897,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9352897,Dementia;Alzheimer’s disease (AD);Magnetic resonance Imaging (MRI);Positron emission tomography (PET);deep learning;machine learning;artificial intelligence,Deep learning;Magnetic resonance imaging;Computed tomography;Medical services;Prediction algorithms;Positron emission tomography;Dementia,biomedical MRI;brain;cognition;computerised tomography;deep learning (artificial intelligence);diseases;health care;medical image processing;neurophysiology;positron emission tomography,positron emission tomography;computed tomography;AI techniques;dementia patients;deep learning;normal brain aging;mild cognitive impairment;dementia disorder;artificial intelligence;healthcare services;magnetic resonance imaging;self-determination medicine;diagnostic tools,,,,11,,17-Feb-21,,,IEEE,IEEE Conferences
An image segmentation method of lung lymphatic tumors based on PET-CT images,Z. Dong; T. Xu; Y. Kang; J. Lian; B. Shi,"Lanzhou Jiaotong University, School of EleCTronic and Information Engineering,Lanzhou,Gansu,P.R.China,730070; Gansu Provincial Hospital, Medical Intelligence Laboratory,Lanzhou,China,730000; Lanzhou Jiaotong University, School of EleCTronic and Information Engineering,Lanzhou,Gansu,P.R.China,730070; Lanzhou Jiaotong University, School of EleCTronic and Information Engineering,Lanzhou,Gansu,P.R.China,730070; Gansu Provincial Hospital, Medical Intelligence Laboratory,Lanzhou,China,730000","2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",03-Feb-22,2021,2,,799,804,"To address the problem from lymphatic tumor segmentation from PET-CT images, this paper presents an image segmentation method based on pulse-coupled neural network (PCNN). PET-CT image detection can provide more detailed tumor information for later diagnosis and treatment, while our adjustable fire-controlled MSPCNN model (AFC-MSPCNN) can segment lung tumors effectively. Related experiment results show that our proposed method can improve the image segmentation accuracy than other competitive methods.",,978-1-6654-2877-4,10.1109/ICIBA52610.2021.9688245,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688245,PET-CT;AFC-MSPCNN;image;segmentation,Image segmentation;Image edge detection;Corrosion;Neural networks;Lung;Manuals;Mathematical models,cancer;computerised tomography;image segmentation;lung;medical image processing;neural nets;positron emission tomography;tumours,PET-CT images;lymphatic tumor segmentation;image segmentation method;pulse-coupled neural network;PET-CT image detection;detailed tumor information;adjustable fire-controlled MSPCNN model;segment lung tumors;image segmentation accuracy;lung lymphatic tumors,,,,11,IEEE,03-Feb-22,,,IEEE,IEEE Conferences
"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Molecular Imaging, Siemens Medical Solutions USA,Knoxville,TN,37932; Molecular Imaging, Siemens Medical Solutions USA,Knoxville,TN,37932; Molecular Imaging, Siemens Medical Solutions USA,Knoxville,TN,37932; Molecular Imaging, Siemens Medical Solutions USA,Knoxville,TN,37932; Molecular Imaging, Siemens Medical Solutions USA,Knoxville,TN,37932",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,3,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9508020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9508020,Attenuation Correction;Denoising;Deep Learning;Scatter Correction;Positron Emission Tomography (PET),Deep learning;Image quality;Radioactive materials;Computed tomography;Noise reduction;Medical services;Attenuation,computerised tomography;image reconstruction;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography,simultaneous attenuation correction;scatter correction;PET imaging;low radiation dose;delivered dose originates;CT scans;PET radioisotopes;scatter corrections;PET image formation;standard PET dose;image quality;low dose PET images;fully corrected standard dose PET images;low dose PET data;CT scan unnecessary,,,,9,,12-Aug-21,,,IEEE,IEEE Conferences
Can Deep Learning Detect Esophageal Lesions In PET-CT Scans?,I. Ackerley; R. Smith; J. Scuffham; M. Halling-Brown; E. Lewis; E. Spezi; V. Prakash; K. Wells,"University of Surrey,Centre for Vision Speech & Signal Processing,Guildford,UK,GU2 7XH; University Hospital of Wales,Cardiff,UK,CF14 4XW; University of Surrey,Department of Physics,Guildford,UK,GU2 7XH; Royal Surrey County Hospital,Department of Medical Physics,Guildford,Surrey,UK,GU2 7XX; Royal Surrey County Hospital,Department of Medical Physics,Guildford,Surrey,UK,GU2 7XX; Cardiff University,School of Engineering,Cardiff,UK,CF14 4XW; Royal Surrey County Hospital,Department of Nuclear Medicine,Guildford,Surrey,UK,GU2 7XX; Centre for Vision Speech & Signal Processing, University of Surrey,Medical Imaging Group,Guildford,UK,GU2 7XH",2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),09-Apr-20,2019,,,1,4,"PET-CT scans using <sup>18</sup>F-FDG with a co-registered CT scan are increasingly used to detect cancer. This paper compares deep learning-based lesion detection tools trained on PET, CT and combined modality data. 486 pre-contoured scans were used from a retrospective cohort study into esophageal cancer. Scans were partitioned into training, validation and test sets with an 80:10:10 ratio. 1000 image segments were generated from each scan, with tumor present segments located on the contoured lesion and tumor absent segments distributed randomly within the patient but excluding the tumor. PET and CT image segments were used to train a separate dedicated 5-layer convolutional neural networks (CNN). Testing on segments from unseen scans resulted in an accuracy of greater than 95% for the PET data, and greater than 90% for CT data.",2577-0829,978-1-7281-4164-0,10.1109/NSS/MIC42101.2019.9059833,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9059833,,Tumors;Computed tomography;Data models;Solid modeling;Positron emission tomography;Three-dimensional displays;Heating systems,biological organs;cancer;computerised tomography;convolutional neural nets;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;tumours,dedicated 5-layer convolutional neural networks;tumor absent segments;precontoured scans;deep learning-based lesion detection tools;18F-FDG;esophageal lesions;CT data;PET data;CT image segments;contoured lesion;tumor present segments;esophageal cancer;PET-CT scans,,,,9,,09-Apr-20,,,IEEE,IEEE Conferences
Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Stanford University,Department of Electrical Engineering,Stanford,CA,USA,94305; Molecular Imaging Program at Stanford (MIPS),Stanford,CA,USA,94305",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,3,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507903,National Science Foundation(grant numbers:1828993); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507903,PET/MR;machine learning;conditional generative adversarial network;MR-based attenuation map;attenuation correction;PET quantification,Deep learning;Image segmentation;Head;Computed tomography;Conferences;Training data;Attenuation,biological tissues;biomedical MRI;brain;computerised tomography;deep learning (artificial intelligence);image reconstruction;image registration;image segmentation;medical image processing;neural nets;positron emission tomography,PET image reconstruction;PET-MR attenuation correction;MR images segmentation;conditional generative adversarial network;continuous attenuation coefficients;pseudoCT images;CT test images;reconstructed PET image;MR-based attenuation map;zero-time echo;Dixon sequences;deep learning;cGAN network;registered training data;straightforward bilinear conversion;brain;CT-based attenuation correction;head-and-neck region,,,,7,,12-Aug-21,,,IEEE,IEEE Conferences
Robust MR-free Grey Matter Extraction in Amyloid PET/CT Studies with Deep Learning,L. Presotto; C. Bezzi; G. Vanoli; C. Muscio; F. Tagliavini; D. Perani; V. Bettinardi,"Nuclear Medicine Unit of IRCCS Ospedale San Raffaele,Milano,(MI),Italy; Nuclear Medicine Unit of IRCCS Ospedale San Raffaele,Milano,(MI),Italy; Nuclear Medicine Unit of IRCCS Ospedale San Raffaele,Milano,(MI),Italy; Fondazione Istituto di Ricovero e Cura a Carattere Scientifico Istituto Neurologico Carlo Besta,Milano,Italy; Fondazione Istituto di Ricovero e Cura a Carattere Scientifico Istituto Neurologico Carlo Besta,Milano,Italy; Università Vita-Salute San Raffaele and the in vivo human molecular and structural neuroimaging Unit, IRCCS San Raffaele Scientific Institute,Division of Neuroscience,Milan,Italy; Nuclear Medicine Unit of IRCCS Ospedale San Raffaele,Milano,(MI),Italy",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,2,"Quantification of amyloid PET studies is most accurate if regions of interest (ROIs) are not affected by the presence of cerebrospinal fluid. Patients with high amyloid load often have great atrophy, therefore, the use of atlas-based ROIs, instead of patient specific anatomy, can underestimate amyloid load, leading to a bias. Traditionally, this can be overcome only using MR anatomical sequences, which are burdensome and might not be ideal to be performed for each patient in the clinical routine. In this work, we propose to overcome this issue by using a method based on deep learning. As CT scans provide anatomical information, even at the very low doses used for PET attenuation correction, we propose the use of such a scan, together with the PET one, for a U-NET based segmentation. The approach achieves a median DICE score of 77% on a validation cohort of N=20 patients, even when using only N=14 patients in the training dataset. A dedicated data augmentation strategy is used, and the individual contribution of each modality is analyzed. We find that the joint effect of PET and CT is beneficial (median DICE: PET only 73.0%, CT only 74%). A near perfect correlation with MR-based quantification was also found.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507836,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507836,,Deep learning;Training;Image segmentation;Correlation;Computed tomography;Conferences;Grey matter,biomedical MRI;brain;computerised tomography;deep learning (artificial intelligence);diseases;feature extraction;image segmentation;medical image processing;neurophysiology;positron emission tomography,U-NET based segmentation;median DICE score;deep learning;cerebrospinal fluid;atlas-based ROI;MR anatomical sequences;MR-free grey matter extraction;amyloid PET-CT scans;PET attenuation correction;data augmentation,,,,1,,12-Aug-21,,,IEEE,IEEE Conferences
Deep Learning-based Automated Delineation of Head and Neck Malignant Lesions from PET Images,H. Arabi; I. Shiri; E. Jenabi; M. Becker; H. Zaidi,"Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland; Research Centre for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences,Tehran,Iran; Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland; Geneva University Neurocenter, Geneva University,Geneva,Switzerland",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,3,"Accurate delineation of the gross tumor volume (GTV) is critical for treatment planning in radiation oncology. This task is very challenging owing to the irregular and diverse shapes of malignant lesions. Manual delineation of the GTVs on PET images is not only time-consuming but also suffers from inter- and intra-observer variability. In this work, we developed deep learning-based approaches for automated GTV delineation on PET images of head and neck cancer patients. To this end, V-Net, a fully convolutional neural network for volumetric medical image segmentation, and HighResNet, a 20-layer residual convolutional neural network, were adopted. <sup>18</sup>F-FDG-PET/CT images of 510 patients presenting with head and neck cancer on which manually defined (reference) GTVs were utilized for training, evaluation and testing of these algorithms. The input of these networks (in both training or evaluation phases) were 12×12×12 cm sub-volumes of PET images containing the whole volume of the tumors and the neighboring background radiotracer uptake. These networks were trained to generate a binary mask representing the GTV on the input PET subvolume. Standard segmentation metrics, including Dice similarity and precision were used for performance assessment of these algorithms. HighResNet achieved automated GTV delineation with a Dice index of 0.87±0.04 compared to 0.86±0.06 achieved by V-Net. Despite the close performance of these two approaches, HighResNet exhibited less variability among different subjects as reflected in the smaller standard deviation and significantly higher precision index (0.87±0.07 versus 0.80±0.10). Deep learning techniques, in particular HighResNet algorithm, exhibited promising performance for automated GTV delineation on head and neck PET images. Incorporation of anatomical/structural information, particularly MRI, may result in higher segmentation accuracy or less variability among the different subjects.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507977,"Swiss National Science Foundation(grant numbers:320030 176052,320030 173091); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507977,Head and Neck Cancer;Segmentation;PET;Deep Learning,Training;Deep learning;Image segmentation;Head;Magnetic resonance imaging;Neck;Lesions,cancer;computerised tomography;convolutional neural nets;deep learning (artificial intelligence);image segmentation;medical image processing;positron emission tomography;radiation therapy;radioactive tracers;tumours,gross tumor volume;manual delineation;automated GTV delineation;head and neck cancer patients;fully convolutional neural network;volumetric medical image segmentation;20-layer residual convolutional neural network;input PET subvolume;deep learning-based automated delineation;head and neck malignant lesions;treatment planning;radiation oncology;V-Net;18F-FDG-PET-CT images;background radiotracer uptake;standard segmentation metrics;Dice similarity;Dice index;HighResNet algorithm;head and neck PET images;MRI,,,,17,,12-Aug-21,,,IEEE,IEEE Conferences
Brain PET Attenuation Correction without CT: An Investigation,M. Dewan; Y. Zhan; G. Hermosillo; B. Jian; X. S. Zhou,"Siemens Med. Solutions USA, Malvern, PA, USA; Siemens Med. Solutions USA, Malvern, PA, USA; Siemens Med. Solutions USA, Malvern, PA, USA; Siemens Med. Solutions USA, Malvern, PA, USA; Siemens Med. Solutions USA, Malvern, PA, USA",2013 International Workshop on Pattern Recognition in Neuroimaging,19-Sep-13,2013,,,110,113,"In the last decade, Brain PET Imaging has taken big strides in becoming an effective diagnostic tool for dementia and epilepsy disorders, particularly Alzheimer's. CT is often used to provide information for PET attenuation correction. However, for dementia patients, which often require multiple follow-ups, the elimination of CT is desirable to reduce the radiation dose. In this paper, we present a robust algorithm for PET attenuation correction without CT. The algorithm involves building a database of non-attenuation corrected (NAC) PET and CT pairs (model scans). Given a new patient's NAC PET, a learning-based algorithm is used to detect key landmarks, which are then used to select the most similar model scans. Deformable registration is then employed to warp the model CTs to the subject space, followed by a fusion step to obtain the virtual CT for attenuation correction. Besides comparing the normalized AC values with ground truth, we also use a diagnostic tool to evaluate the solution. In addition, a diagnostic evaluation is conducted by a trained nuclear medicine physician, all with promising results.",,978-0-7695-5061-9,10.1109/PRNI.2013.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603569,attenuation correction;brain PET Imaging,Positron emission tomography;Computed tomography;Attenuation;Dementia;Deformable models;Brain modeling,brain;diagnostic radiography;diseases;learning (artificial intelligence);medical disorders;medical image processing;positron emission tomography,brain PET attenuation correction;brain PET imaging;diagnostic tool;epilepsy disorders;Alzheimer;dementia patients;radiation dose;nonattenuation corrected PET;NAC PET;learning-based algorithm;deformable registration;virtual CT;diagnostic evaluation;nuclear medicine physician,,,,11,,19-Sep-13,,,IEEE,IEEE Conferences
Automating Lung Cancer Identification in PET/CT Imaging,E. D'Arnese; E. Del Sozzo; A. Chiti; T. Berger-Wolf; M. D. Santambrogio,"Politecnico di Milano, Dept. of Electronics, Information and Bioengineering; Politecnico di Milano, Dept. of Electronics, Information and Bioengineering; Humanitas University and Humanitas Research Hospital, Milano; University of Illinois at Chicago, Dept. of Computer Science; Politecnico di Milano, Dept. of Electronics, Information and Bioengineering",2018 IEEE 4th International Forum on Research and Technology for Society and Industry (RTSI),29-Nov-18,2018,,,1,6,"Early and accurate diagnosis of lung cancer is one of the most investigated open challenges in the last decades. The diagnosis for this cancer type is usually lethal if not detected in early stages. For these reasons it is clear the need of creating an automated diagnostic tool that requires less time for the identification and does not require a cross-validation of the results by different radiologist, being in this way cheaper and less error prone. The aim of this work is to implement a completely automated pipeline that starting from the current imaging technologies, such as Computed Tomography (CT) and Positron Emission Tomography (PET), will identify lung cancer to be employed for the staging; moreover, it will be a suitable starting point for a machine learning based classification procedure. In particular, this project proposes both a methodology and the related software tool that taking as input Digital Imaging and COmmunications in Medicine (DICOM®) files of chest PET and CT and by exploiting the characteristics of both of them is capable of automatically identify the lungs and the eventually presence of tumor lesions. A validation of the image processing pipeline has been done by computing the execution time and the reached accuracy. The obtained accuracy varies between 89-97% on the analyzed dataset with a significant reduction of the analysis time.",,978-1-5386-6282-3,10.1109/RTSI.2018.8548388,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548388,Medical image processing;Automatic segmentation;PET/CT;Lung cancer,Cancer;Lung;Computed tomography;Tumors;Positron emission tomography;Image segmentation,cancer;computerised tomography;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,DICOM files;digital imaging and communications in medicine;input digital imaging;lungs PET;radiologist;PET-CT;positron emission tomography;computed tomography;cross-validation;automated diagnostic tool;cancer type;investigated open challenges;accurate diagnosis;automating lung cancer identification;image processing pipeline;chest PET;related software tool;suitable starting point;current imaging technologies;completely automated pipeline,,,,22,,29-Nov-18,,,IEEE,IEEE Conferences
A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Tehran University of Medical Sciences,Department of Medical Physics and Biomedical Engineering,Tehran,Iran; Tehran University of Medical Sciences,Department of Medical Physics and Biomedical Engineering,Tehran,Iran; Tehran University of Medical Sciences,Department of Medical Physics and Biomedical Engineering,Tehran,Iran; National Research Institute of Tuberculosis and Lung Diseases (NRITLD), Shahid Beheshti University of Medical Sciences,Chronic Respiratory Diseases Research Center,Tehran,Iran; Geneva University Hospital,Division of Nuclear Medicine and Molecular Imaging,Geneva,Switzerland,CH1211; Tehran University of Medical Sciences,Department of Medical Physics and Biomedical Engineering,Tehran,Iran",2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),25-May-21,2021,,,1365,1368,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",1945-8452,978-1-6654-1246-9,10.1109/ISBI48211.2021.9433923,University Hospitals; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433923,small animal PET;deep learning;gap correction;sparse detector configuration,Visualization;Statistical analysis;Animals;Fault detection;Computed tomography;Neural networks;Detectors,image reconstruction;image resolution;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography,streaking artifacts;incomplete projection views;sparse-view CT reconstruction;deep-learning-based methods;artifact-free PET images;sparse-crystal setting;artifact-free PET scans;severe dead regions;adjacent detector blocks;high-resolution preclinical PET scanner;image-space;quantitative analysis;partial-ring PET scanners;deep neural network;image-domain implementations;PET sinograms;image artifacts;sparse-ring detector configurations;full-ring PET scanners;faulty detectors;projection domain;inter-block gap regions;unacceptable image distortions;imaging community;impaired sinograms;animal pet imaging,,,,15,,25-May-21,,,IEEE,IEEE Conferences
Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"Istanbul Technical University,Mechatronics Engineering Department,Istanbul,Turkey; Istanbul Technical University,Geomatics Engineering Department,Istanbul,Turkey",2021 Innovations in Intelligent Systems and Applications Conference (ASYU),18-Nov-21,2021,,,1,4,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,978-1-6654-3405-8,10.1109/ASYU52992.2021.9598993,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9598993,Lung Cancer;COVID-19;deep learning;CT images;YOLO-v3,COVID-19;Deep learning;Technological innovation;Computed tomography;Lung cancer;Lung;Radiology,cancer;computerised tomography;diseases;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;patient diagnosis,deep learning models;COVID-19 lung diseases;Computed Tomography images;different deep learning mechanisms;lung disease screening;different deep learning-based models;VGG19;Corona Virus disease;large-scale CT dataset;lung cancer diagnoses;Lung-PET -CT-Dx;lung cancer CT images;International COVID-19 Open Radiology Dataset;COVID-19 CT images;DarkNet-53 overperformed other models,,,,17,,18-Nov-21,,,IEEE,IEEE Conferences
Fast Dynamic Brain PET Imaging Using a Generative Adversarial Network,A. Sanaat; E. Mirsadeghi; B. Razeghi; N. Ginovart; H. Zaidi,"Geneva University Hospital,Division of Nuclear Medicine & Molecular Imaging,Geneva,Switzerland; Amirkabir University of Technology,Electrical Engineering Department,Tehran,Iran; Geneva University,Department of Computer Sciences,Geneva,Switzerland; Geneva University,Department of Psychiatry and Department of Basic Neurosciences,Geneva,Switzerland; Geneva University Neurocenter, Geneva University,Geneva,Switzerland",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,3,"This work aims to present and evaluate a novel recurrent deep learning model for reduction of the acquisition time in dynamic brain PET imaging without forfeiting clinical information. The clinical dataset included 46 dynamic <sup>18</sup>F-DOPA brain PET/CT images used to evaluate a model for generation of complete dynamic PET images from 27% of the total acquisition time. The dataset was split into 35, 6, and 5 for training, validation, and test, respectively. Each dynamic PET scan lasts 90 minutes acquired in list-mode format used to reconstruct 26 dynamic frames). A video prediction deep learning algorithm consisted of two generative adversarial networks and one variational autoencoder was developed and optimized to depict the tracer variation trend from the initial 13 frames (0 to 25 min) and synthesize the last 13 frames (25 to 90 min), respectively. The generated image was analyzed quantitatively by calculating standard metrics, such as the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and time-activity curve (TAC). The PSNR and SSIM varied from 43.24 ± 0.4 to 38.82 ± 0.74 and from 0.98±0.03 to 0.81±0.09 for synthesized frames (14 to 26), respectively. The TAC trend showed that our model is able to predict images with similar tracer distribution compared to reference images. We demonstrated that the proposed method can generate the last 65 min time frames from the initial 25 min frames in dynamic PET imaging, thus reducing the total scanning time.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507894,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507894,,Measurement;Deep learning;Training;PSNR;Predictive models;Brain modeling;Market research,brain;image reconstruction;learning (artificial intelligence);medical image processing;positron emission tomography,fast dynamic brain PET imaging;generative adversarial network;deep learning model;clinical information;clinical dataset;complete dynamic PET images;total acquisition time;dynamic PET scan;list-mode format;dynamic frames;tracer variation trend;structural similarity index;time-activity curve;synthesized frames;reference images;time frames;dynamic PET imaging;total scanning time;time 90.0 min;time 65.0 min;time 25.0 min;time 0.0 min to 25.0 min;time 25.0 min to 90.0 min,,,,7,,12-Aug-21,,,IEEE,IEEE Conferences
Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Gordon Center for Medical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Department of Biomedical Engineering, University of California at Davis, Davis, CA, USA; Gordon Center for Medical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA",IEEE Transactions on Medical Imaging,02-Mar-22,2022,41,3,680,689,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",1558-254X,,10.1109/TMI.2021.3120913,"National Institutes of Health(grant numbers:R21AG067422,R03EB030280,RF1AG052653,P41EB022544); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576711,Direct reconstruction;dynamic PET;deep neural network;unsupervised learning;deep image prior;positron emission tomography,Image reconstruction;Positron emission tomography;Kinetic theory;Imaging;Training;Mathematical models;Kernel,image denoising;image reconstruction;learning (artificial intelligence);medical image processing;positron emission tomography,linear parametric images;nonlocal deep image;measured PET sinograms;tracer kinetics;integrated framework;direct reconstruction frameworks;supervised deep learning methods;high-quality training labels;static PET imaging;scanning time;dynamic PET imaging;unsupervised deep learning framework;direct parametric reconstruction;Patlak model;relative equilibrium Logan model;training objective function;PET statistical model;patient;linear kinetic model;dynamic datasets;kernel method-based direct reconstruction methods,"Algorithms;Fluorodeoxyglucose F18;Humans;Image Processing, Computer-Assisted;Positron Emission Tomography Computed Tomography;Positron-Emission Tomography;Signal-To-Noise Ratio",,,51,IEEE,15-Oct-21,,,IEEE,IEEE Journals
U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Warsaw University of Technology,Faculty of Electrical Engineering,Warsaw,Poland; Warsaw University of Technology,Faculty of Electrical Engineering,Warsaw,Poland; Warsaw University of Technology,Faculty of Electrical Engineering,Warsaw,Poland; Medical University of Lodz,Medicine Department,Lodz,Poland; Faculty of Medicine Medical University of Lodz,Lodz,Poland; Medical University of Warsaw,Nuclear Medicine Department,Warsaw,Poland",2021 22nd International Conference on Computational Problems of Electrical Engineering (CPEE),01-Nov-21,2021,,,1,4,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,978-1-7281-8430-2,10.1109/CPEE54040.2021.9585272,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585272,U-Net;deep learning;cross-validation;brain tumours segmentation;computer vision,Training;Measurement;Image segmentation;Shape;Computational modeling;Computed tomography;Semantics,brain;computerised tomography;convolutional neural nets;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;tumours,U-Net training model;efficient brain tumour segmentation;vision systems;accurate diagnoses;deep learning based framework;semantic segmentation;segment glioma;tumour area;convolutional neural network models;training parameters;efficient brain tumour detection;multimodality CT images;multimodality PET images;computer-aided detection;computer-aided diagnoses,,,,11,IEEE,01-Nov-21,,,IEEE,IEEE Conferences
Investigation of Spatial-Temporal Kernel Method for Dynamic Imaging in Short and Long Range PET Scanners,Y. Li; Y. Zhao; Y. Lv; J. Zhao,"School of Biomedical Engineering, Shanghai Jiao Tong University,Shanghai,China; Shanghai Imaging Healthcare, co., Ltd.,Shanghai,China; Shanghai Imaging Healthcare, co., Ltd.,Shanghai,China; School of Biomedical Engineering, Shanghai Jiao Tong University,Shanghai,China",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,4,"Dynamic positron emission tomography (PET) imaging has a limited temporal resolution, and always suffers from poor signal to noise ratio (SNR) due to low count statistics from short time frames and the ill-posed nature of expectation-maximization (EM) based reconstruction algorithm. Increasing acquisition time for each frame improves image SNR, but may introduce motion artefacts to the region of heart, lung and head. So, a trade-off has to be made between temporal resolution and image quality. Previous studies have demonstrated that aided by prior information from long time frames and/or anatomical images, the kernel method can effectively improve the SNR of the dynamic PET images from very low-count data. In this study, we have investigated the performance of kernel method in long range and simulated short range scanners. We implemented a spatial-temporal kernel method (ST-KEM) and applied it to the data collected from the 194-cm PET/CT scanner (uEXPLORER). Two short range PET scanners were simulated by using partial of the total eight units of uEXPLORER. Additionally, we exploited the way to improve prior images of short range scanners through deep learning. The results showed that ST-KEM outperformed the kernelized Expectation-Maximization (KEM) and conventional Ordered Subsets Expectation Maximization (OSEM) for frames as short as 0.1 second in uEXPLORER, from which noiseless cardiac motion signal could be extracted. In simulated short range scanners, similar image quality could be achieved when the frame duration extends to 1 to 2-second.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507931,"National Key R&D Program of China(grant numbers:2016YFC0104608); National Nature Science Foundation of China(grant numbers:81371634); Shanghai Jiao Tong University Medical Engineering Cross Research(grant numbers:YG2017ZD10,YG2014ZD05); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507931,,Image quality;Image resolution;Heuristic algorithms;Dynamics;Lung;Reconstruction algorithms;Positron emission tomography,cardiology;computerised tomography;deep learning (artificial intelligence);expectation-maximisation algorithm;image reconstruction;medical image processing;positron emission tomography,spatial-temporal kernel method;dynamic imaging;uEXPLORER;noiseless cardiac motion signal;long range PET scanners;dynamic positron emission tomography;kernelized expectation maximization;deep learning;ordered subsets expectation maximization;time 0.1 s,,,,10,,12-Aug-21,,,IEEE,IEEE Conferences
Deep Neural Network for Automatic Characterization of Lesions on 68Ga-PSMA PET/CT Images,Y. Zhao; A. Gafita; G. Tetteh; F. Haupt; A. Afshar-Oromieh; B. Menze; M. Eiber; A. Rominger; K. Shi,"Department of Computer Science, Technische Universität München, Munich, Germany; Department of Nuclear Medicine, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Nuclear Medicine, University of Bern, Bern, Switzerland; Department of Nuclear Medicine, University of Bern, Bern, Switzerland; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Nuclear Medicine, Technische Universität München, Munich, Germany; Department of Nuclear Medicine, University of Bern, Bern, Switzerland; Department of Computer Science, Technische Universität München, Munich, Germany",2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),07-Oct-19,2019,,,951,954,"The emerging PSMA-targeted radionuclide therapy provides an effective method for the treatment of advanced metastatic prostate cancer. To optimize the therapeutic effect and maximize the theranostic benefit, there is a need to identify and quantify target lesions prior to treatment. However, this is extremely challenging considering that a high number of lesions of heterogeneous size and uptake may distribute in a variety of anatomical context with different backgrounds. This study proposes an end-to-end deep neural network to characterize the prostate cancer lesions on PSMA imaging automatically. A <sup>68</sup>Ga-PSMA-11 PET/CT image dataset including 71 patients with metastatic prostate cancer was collected from three medical centres for training and evaluating the proposed network. For proof-of-concept, we focus on the detection of bone and lymph node lesions in the pelvic area suggestive for metastases of prostate cancer. The preliminary test on pelvic area confirms the potential of deep learning methods. Increasing the amount of training data may further enhance the performance of the proposed deep learning method.",1558-4615,978-1-5386-1311-5,10.1109/EMBC.2019.8857955,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857955,,Lesions;Bones;Prostate cancer;Lymph nodes;Image segmentation;Convolution,biological organs;bone;cancer;computerised tomography;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography;radioisotope imaging;tumours,heterogeneous size;end-to-end deep neural network;prostate cancer lesions;PSMA imaging;bone;lymph node lesions;deep learning method;metastatic prostate cancer;therapeutic effect;theranostic benefit;target lesions;PSMA-targeted radionuclide therapy;68Ga-PSMA-11 PET-CT image dataset,"Automation, Laboratory;Edetic Acid;Humans;Male;Membrane Glycoproteins;Neural Networks, Computer;Organometallic Compounds;Positron Emission Tomography Computed Tomography;Prostatic Neoplasms",,,18,,07-Oct-19,,,IEEE,IEEE Conferences
Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Xidian University,Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence,Xi'an,China,710071; Xidian University,Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence,Xi'an,China,710071; University of Central Missouri,School of Computer Science and Mathematics,Warrensburg,MO,United States of America,64093; University of Texas Southwestern Medical Center,Advanced Imaging and Informatics for Radiation Therapy (AIRT) Laboratory,Dallas,TX,United States of America,75390; Putian Unviersity,Engineering Research Center of Big Data Application in Private Health Medicine, School of Information Engineering,Putian,China,351100; University of Texas Southwestern Medical Center,Advanced Imaging and Informatics for Radiation Therapy (AIRT) Laboratory,Dallas,TX,United States of America,75390",2021 International Joint Conference on Neural Networks (IJCNN),20-Sep-21,2021,,,1,7,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",2161-4407,978-1-6654-3900-8,10.1109/IJCNN52387.2021.9533703,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533703,head and neck cancer;multi-view convolutional neural network;local recurrence;outcome prediction,Three-dimensional displays;Head;Computed tomography;Redundancy;Predictive models;Feature extraction;Neck,cancer;convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;medical image processing;positron emission tomography,multidisciplinary management;optimal personalized treatment strategy;multiview convolutional neural network model;LRR prediction;mMmV;dimension reduction operator;multiview strategy;3D method;3D information;multimodal data;cross modality data;multimodality deep neural network;mMm V-CNN;multiview 2D CNN;locoregional recurrence;head & neck cancer;end-to-end multimodality convolutional neural network model;multimodality 2D CNN;3D volume;2D images;CT;PET;clinical features;positron emission tomography;computed tomography,,,,26,,20-Sep-21,,,IEEE,IEEE Conferences
Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"Department of Computer Science and Engineering, Walchand College of Engineering, Sangli, India; Department of Computer Science and Engineering, Walchand College of Engineering, Sangli, India",2019 Amity International Conference on Artificial Intelligence (AICAI),29-Apr-19,2019,,,218,223,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,978-1-5386-9346-9,10.1109/AICAI.2019.8701358,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701358,Artificial Neural Network (ANN);Convolutional Neural Network (CNN);Deep Neural Network (DNN);heterogeneous data,Artificial neural networks;Feature extraction;Deep learning;Electroencephalography;Brain modeling;Neurons,convolutional neural nets;data handling;entropy;learning (artificial intelligence);mean square error methods;medical information systems,ANN;CNN;DNN;multimodal heterogeneous data;multimodal data;learning speed;integrated learning technique;learning performance;image classification;speech recognition;biomedical analysis;biomedical data;convolutional neural network;deep neural network;back propagation;artificial neural network;heterogeneous medical data handling;integrated deep convolution neural network approach;root mean square error;cross entropy,,,,20,,29-Apr-19,,,IEEE,IEEE Conferences
Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"AU College of Engineering(A), AU,Department of ECE,Visakhapatnam,Andhra Pradesh,India; GITAM(deemed to be University),Department of ECE,Visakhapatnam,Andhra Pradesh,India; JNT University,Department of ECE,Kakinada,Andhra Pradesh,India",2019 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET),12-Mar-20,2019,,,169,173,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,978-1-5386-9279-0,10.1109/WiSPNET45539.2019.9032747,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9032747,Neural Network;Compression;Gradient Descent;Conjugate Gradient;Performance Metrics,Image coding;Biological neural networks;Training;Computed tomography;Medical diagnostic imaging;Neurons,biomedical MRI;computerised tomography;data compression;feedforward neural nets;gradient methods;image reconstruction;learning (artificial intelligence);mean square error methods;medical image processing;positron emission tomography,MRI scan;lower abdomen medical image;CT scan;gradient descent algorithm;conjugate gradient technique;feed forward artificial neural network;image processing;X-ray imagery;PET scan;medical images;medical field;data compression;computed tomography medical image compression,,,,13,,12-Mar-20,,,IEEE,IEEE Conferences
A deformable model-based system for 3D analysis and visualization of tumor in PET/CT images,J. Landre; S. Lebonvallet; S. Ruan; Li Xiaobing; Qiu Tianshuang; F. Brunotte,"CReSTIC, IUT de Troyes, 9 Rue de Québec, 10026 Cedex, France; CReSTIC, IUT de Troyes, 9 Rue de Québec, 10026 Cedex, France; CReSTIC, IUT de Troyes, 9 Rue de Québec, 10026 Cedex, France; School of Electronic and Information Engineering, Dalian University of Technology, 116023, China; School of Electronic and Information Engineering, Dalian University of Technology, 116023, China; centre G.-F. Leclerc, University of Bourgogne, 1 rue professeur Marion, 21000 Dijon, France",2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,14-Oct-08,2008,,,3130,3133,"This paper presents a tumor detecting system that allows interactive 3D tumor visualization and tumor volume measurements. An improved level set method is proposed to automatically segment the tumor images slice by slice. PET images are used to detect the tumor while CT images make a 3D representation of the patient's body possible. An initial slice with a seed within the tumor is firstly chosen by the operator. The system then performs automatically the tumor volume segmentation that allows the clinician to visualize the tumor, to measure it and to evaluate the best medical treatment adapted to the patient.",1558-4615,978-1-4244-1814-5,10.1109/IEMBS.2008.4649867,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4649867,,,,,"Algorithms;Artificial Intelligence;Computer Graphics;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Medical Oncology;Models, Statistical;Neoplasms;Pattern Recognition, Automated;Positron-Emission Tomography;Reproducibility of Results;Sensitivity and Specificity;Tomography, X-Ray Computed;User-Computer Interface",,,14,,14-Oct-08,,,IEEE,IEEE Conferences
Medical image understanding and Computational Anatomy,Y. Masutani,"Medical Imaging laboratory, Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-higashi, Asaminami-ku, 731-3194, Japan",2015 IEEE 8th International Workshop on Computational Intelligence and Applications (IWCIA),09-Apr-16,2015,,,1,1,"By the rapid development of medical imaging equipments such as X-ray CT, MRI, PET, etc., data quantity yielded in hospitals is still explosively increasing. For instance, it often reaches to more than 1000 slices of X-ray CT and MRI images in a single examination. This is mainly due to improvement in spatial and temporal resolution of images, and acquisition of multi-modal information from various imaging physics. In contrast to such rich information, image-reading workload for radiologists becomes extremely heavier. In some cases, radiologists can take only less than one second per slice image in average and oversights of abnormalities may possibly occur. Therefore, full or partial automation of such image-reading tasks is a natural demand. Generally, image-reading task includes visual search of abnormalities in images such as tumors, deformation or degeneration of tissues. The computational support technology for assisting radiologists, so-called “Computer-Assisted Diagnosis/Detection (CAD)”, based on image analysis and pattern recognition have a long history over 30 years. In the early phases of CAD technology development, simple schemes such as search of round-shaped structures were employed to obtain limited success due to lack of anatomical information. Recently, information of shape and structure of the inner organs as image analysis priors becomes indispensable for reliable results. That is, computational image understanding with anatomical knowledge is a certain standard of medical image analysis. Especially, thanks to machine learning approaches with high computational powers and large database, studies on statistical analysis and mathematical description of anatomical structures opened a new discipline called “Computational Anatomy”. In this lecture, several examples of state-of-the-art techniques and systems are introduced and discussed with the practical problems in clinical situations.",1883-3977,978-1-4799-9886-9,10.1109/IWCIA.2015.7449449,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449449,,Medical diagnostic imaging;Image analysis;Urban areas;X-ray imaging;Computed tomography;Magnetic resonance imaging,biomedical MRI;computerised tomography;hospitals;learning (artificial intelligence);medical image processing;patient diagnosis;pattern recognition;radiology;X-ray imaging,machine learning;pattern recognition;image analysis;CAD;computer-assisted diagnosis/detection;visual search;image-reading task;radiology;imaging physics;MRI images;X-ray CT;hospitals;data quantity;medical imaging equipments;computational anatomy;medical image understanding,,,,,,09-Apr-16,,,IEEE,IEEE Conferences
Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Pattern Recognition Lab; Pattern Recognition Lab; Pattern Recognition Lab; Department of Radiology, University of Pennsylvania",2012 9th IEEE International Symposium on Biomedical Imaging (ISBI),12-Jul-12,2012,,,704,707,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",1945-8452,978-1-4577-1858-8,10.1109/ISBI.2012.6235645,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235645,non-rigid registration;landmarks;regularizer based on PCA,Principal component analysis;Computed tomography;Positron emission tomography;Bones;Biomedical imaging;Mathematical model;Educational institutions,biomedical MRI;bone;computerised tomography;image registration;learning (artificial intelligence);medical image processing;orthopaedics;positron emission tomography;principal component analysis,landmarks;learning;anatomical meaningful transformations;nonrigid registration framework;principle component analysis;PCA;computational complexity;skull CT scan;MR-PET hybrid scanner;Dice coefficient;bone;PET reconstruction,,,,10,,12-Jul-12,,,IEEE,IEEE Conferences
Hybrid Medical Image Fusion based on Fast Filtering and Wavelet Analysis,S. A. El-Masry; S. Y. El-Mashad; N. E. El-Attar; W. A. Awad,"Faculty of Science, Port Said University,Mathematics and Computer Science Department,Port Said,Egypt; Faculty of Engineering, Benha University,Benha,Egypt; Faculty of Computers and Artificial Intelligence, Benha University,Benha,Egypt; Faculty of Science, Port Said University,Mathematics and Computer Science Department,Port Said,Egypt",2019 Ninth International Conference on Intelligent Computing and Information Systems (ICICIS),12-Mar-20,2019,,,167,173,"Within medical imaging, there are various modalities of medical images like CT, X-rays, MRI and other modalities that provide information about a human body in different ways. Each modality has distinctive characteristics that provide various sources of information. Therefore, there are some problems like image comparison such as CT/PET, CT /MRI, and MRI/ PET were usually meet by the clinical treatment and diagnosis. Hence the need to combine the different images' information and this process is known as `medical image fusion'. In this paper, two techniques for the `medical image fusion' are introduced. The first proposed fusion technique is the combination of the fast filtering with the discrete wavelet transform `DWT' methods for overcoming the low spatial resolution fused image provided by DWT and preserve the source images' salient features. Where we used the fast filtering method procedures for combining the corresponding `low-frequency coefficients' to maintain the `salient features' of the initial images, and the maximum rule with the high-frequency coefficients which lead getting better the resultant image contrast. The second proposed technique is the combination of fast filtering with stationary wavelet transform (SWT) methods, where `SWT' has the shift-invariant property which enables to overcome the shift-variance DWT's drawback. The performance of the fused output is tested and compared with five of the common fusion methods like the Gradient pyramid, Contrast pyramid, DWT, Fast Filtering, and SWT techniques, using performance parameters: E, SNR, SD, and PSNR.",,978-1-7281-3995-1,10.1109/ICICIS46948.2019.9014677,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9014677,‘image fusion’;‘fast filtering’;‘wavelet transform’,Discrete wavelet transforms;Image fusion;Filtering;Medical diagnostic imaging,biomedical MRI;computerised tomography;discrete wavelet transforms;image filtering;image fusion;image resolution;medical image processing;positron emission tomography;wavelet transforms,gradient pyramid;contrast pyramid;stationary wavelet transform;discrete wavelet transform;MRI-PET;CT-PET;CT-MRI;image contrast;fast filtering;source images;fusion technique;medical images;medical imaging;hybrid medical image fusion,,,,36,,12-Mar-20,,,IEEE,IEEE Conferences
A Two-Level Dynamic Adaptive Network for Medical Image Fusion,W. Huang; H. Zhang; X. Quan; J. Wang,"College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China",IEEE Transactions on Instrumentation and Measurement,10-May-22,2022,71,,1,17,"The research of deep learning-based methods for image fusion has become a current hotspot. Medical image fusion with the problem of few samples also lacks a unified end-to-end model for the input of different modal pairs. In this article, we propose a two-level dynamic adaptive network for medical image fusion, which addresses the above two problems and provides a unified fusion framework to take the advantage of different modal pairs. Specifically, we develop a dynamic meta-learning method on task level, which achieves a dynamical meta-knowledge transfer from the heterogeneous task of multifocus image fusion to medical image fusion by dynamic convolution decomposition (DCD). Then, we provide an efficient adaptive fusion method on multimodal feature level, which uses dynamic attention mechanism and dynamic channel fusion mechanism to fuse features of different aspects. For model evaluation, we have done the qualitative and quantitative tests on the transferred multifocus deep network and verified its superior fusion performance. On this basis, the experiments are carried out on the public datasets of the two most commonly used modal pairs (computerized tomography (CT)-magnetic resonance imaging (MRI) and positron emission tomography (PET)-MRI) and show that our hierarchical model is superior to the state-of-the-art methods in terms of visual effects and quantitative measurement. Our code is publicly available at <uri>https://github.com/zhanglabNKU/TDAN</uri>.",1557-9662,,10.1109/TIM.2022.3169546,National Natural Science Foundation of China(grant numbers:61973174); Key Project of the Natural Science Foundation of Tianjin City(grant numbers:21JCZDJC00140); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762233,Deep learning;medical image fusion;meta-learning;multifocus image fusion,Task analysis;Image fusion;Convolution;Medical diagnostic imaging;Feature extraction;Training;Fuses,,,,,,49,IEEE,22-Apr-22,,,IEEE,IEEE Journals
Deep Learning-Assisted Whole-Body Voxel-Based Internal Dosimetry,A. Akhavanallaf; I. Shiri; H. Arabi; H. Zaidi,"Geneva University Hospital,Division of Nuclear Medicine and Molecular Imaging,Department of Medical Imaging,Geneva 4,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine and Molecular Imaging,Department of Medical Imaging,Geneva 4,Switzerland,CH-1211; Geneva University Hospital,Division of Nuclear Medicine and Molecular Imaging,Department of Medical Imaging,Geneva 4,Switzerland,CH-1211; University of Southern Denmark,Department of Nuclear Medicine,Odense,Denmark,DK-500",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,3,"We propose a novel methodology to conduct whole-body organ-level dosimetry taking into account the heterogeneity of activity distribution as well as patient-specific anatomy using Monte Carlo (MC) simulations and machine learning algorithms. We extended the core idea of the voxel-scale MIRD approach that utilizes a single S-value kernel for internal dosimetry by generating specific S-value kernels corresponding to patient-specific anatomy. In this context, we employed deep learning algorithms to predict the deposited energy distribution, representing the S-value kernel. The training dataset consists of density maps obtained from CT images along with the ground-truth dose distribution obtained from MC simulations. Accordingly, whole-body dose maps are constructed through convolving specific S-values with the activity map. The Deep Neural Network (DNN) predicted dose map was compared with the reference (Monte Carlo-based) and two MIRD-based methods, including single-voxel S-value (SSV) and multiple voxel S-value (MSV) approaches. The Mean Relative Absolute Errors (MRAE) of the estimated absorbed dose between DNN, MSV, and SSV against reference MC simulations were 2.6%, 3%, and 49%, respectively. MRAEs of 23.5%, 5.1%, and 21.8% were obtained between the proposed method and MSV, SSV, and Olinda dosimetry package in organ-level dosimetry, respectively. The proposed internal dosimetry technique exhibited comparable performance to the direct Monte Carlo approach while overcoming the computational burden limitation of MC simulations.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507983,Swiss National Science Foundation(grant numbers:320030_176052); Swiss Cancer Research Foundation(grant numbers:KFS-3855-02-2016); Iran's Ministry of Science; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507983,Internal dosimetry;Deep learning;PET;Monte Carlo,Deep learning;Training;Monte Carlo methods;Machine learning algorithms;Computational modeling;Conferences;Computed tomography,biological organs;computerised tomography;dosimetry;image sampling;learning (artificial intelligence);medical image processing;Monte Carlo methods;neural nets;phantoms;radiation therapy,whole-body voxel-based internal dosimetry;whole-body organ-level dosimetry;activity distribution;patient-specific anatomy;machine learning algorithms;voxel-scale MIRD approach;single S-value kernel;specific S-value kernels;deep learning algorithms;deposited energy distribution;density maps;ground-truth dose distribution;whole-body dose maps;specific S-values;activity map;deep neural network;DNN;dose map;Monte Carlo-based methods;MIRD-based methods;SSV;MSV;estimated absorbed dose;reference MC simulations;Olinda dosimetry package;internal dosimetry technique;direct Monte Carlo approach,,,,14,,12-Aug-21,,,IEEE,IEEE Conferences
Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"CMR Institute of Technology,Department of ECE,Bengaluru,India; CMR Institute of Technology,Department of ECE,Bengaluru,India",2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),12-Apr-21,2021,,,853,858,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,978-1-7281-9537-7,10.1109/ICAIS50930.2021.9396001,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396001,Discrete Wavelet Transform;Entropy;Image fusion;Max Absolute Rule;Sparse representation;Standard deviation,Magnetic resonance imaging;Computed tomography;Transforms;Entropy;Discrete wavelet transforms;Positron emission tomography;Image fusion,biomedical MRI;brain;computerised tomography;discrete wavelet transforms;entropy;image fusion;image reconstruction;image representation;medical image processing;positron emission tomography,image fusion;medical images;magnetic resonance imaging;CT images;discrete wavelet transform;high pass patch;low pass patches;high pass fusion;DWT reconstruction;computed tomography;positron emission tomography;sparse representation;brain regions;fused patch;elapsed time;entropy,,,,17,,12-Apr-21,,,IEEE,IEEE Conferences
MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Concordia University,Concordia Institute for Information Systems Engineering,Montreal,QC,Canada; University of Toronto,Department of Medical Imaging,Canada; University of Toronto,Department of Electrical and Computer Engineering,Toronto,ON,Canada; Concordia University,Concordia Institute for Information Systems Engineering,Montreal,QC,Canada","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",09-Apr-20,2020,,,2013,2017,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",2379-190X,978-1-5090-6631-5,10.1109/ICASSP40776.2020.9053243,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053243,Survival Prediction;Multi-scale;Deep Learningbased Radiomics;Lung Tumor.,Computed tomography;Lung cancer;Predictive models;Feature extraction;Positron emission tomography;Tumors;Radiomics,cancer;computerised tomography;image segmentation;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours,tumor features;lung cancer patients;MDR-SURV model;pulmonary malignancies;multiscale deep learning-based radiomics model;positron emission tomography;computed tomography;PET-CT images,,,,29,,09-Apr-20,,,IEEE,IEEE Conferences
Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Software College, Northeastern University, Shenyang, China",IEEE Access,28-Aug-19,2019,7,,116402,116412,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",2169-3536,,10.1109/ACCESS.2019.2932842,National Natural Science Foundation of China(grant numbers:61872075); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786226,Image diagnosis report;visibility attribute;anomaly detection;PET/CT image;CNN;GRU;CRF,Feature extraction;Anomaly detection;Vocabulary;Medical diagnostic imaging;Task analysis;Encoding,biomedical imaging;computerised tomography;convolutional neural nets;diseases;feature extraction;image classification;medical image processing;natural language processing;patient diagnosis;positron emission tomography,medical image automatic diagnosis;semantic analysis;image diagnosis report;computed tomography;positron emission tomography;case-level anomaly detection;sentence-level anomaly detection;conditional random field;word embeddings;VA;BiGRU;VAE-Net;visibility attribute extraction network;image findings section;cascade networks;Chinese diagnostic report,,,,32,CCBY,05-Aug-19,,,IEEE,IEEE Journals
Application of Deep Learning in Imaging diagnosis of Brain diseases,J. Liang; Z. Wang; X. Ye,"School of mathematics and statistics, Xi'an Jiaotong University,Xi'an,China; Wenzhou Kean University,Department of computer science,Zhejiang,China; College of Automotive Engineering, Jilin University,Chang Chun,China","2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",17-Mar-22,2021,,,166,175,"The brain is one of the most important parts of the human body, and the diagnosis of its diseases is of great significance to the treatment of diseases. With the rapid development of deep learning in recent years, its automatically extracted image features have significant advantages compared to traditional artificially extracted features. Therefore, more and more recognition methods based on deep learning are widely used in medical image recognition tasks (such as CT, MRI, PET-CT.). This paper will introduce the application of traditional methods and deep learning methods in various brain diseases. These methods are compared, analyzed, and summarized, then we explored their development status and future development trends.",,978-1-6654-1790-7,10.1109/MLBDBI54094.2021.00040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730993,brain disease;medical image;computer-assisted diagnosis;deep learning,Deep learning;Image recognition;Machine learning algorithms;Imaging;Manuals;Feature extraction;Medical diagnosis,biomedical MRI;brain;diseases;feature extraction;image recognition;learning (artificial intelligence);medical image processing,brain diseases;imaging diagnosis;human body;automatically extracted image features;traditional artificially extracted features;recognition methods;medical image recognition tasks;deep learning methods,,,,51,,17-Mar-22,,,IEEE,IEEE Conferences
Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Brac University,Software Engineering and HCI Research Group,Department of Computer Science and Engineering,Bangladesh; Brac University,Software Engineering and HCI Research Group,Department of Computer Science and Engineering,Bangladesh; Brac University,Software Engineering and HCI Research Group,Department of Computer Science and Engineering,Bangladesh; Brac University,Software Engineering and HCI Research Group,Department of Computer Science and Engineering,Bangladesh; Brac University,Software Engineering and HCI Research Group,Department of Computer Science and Engineering,Bangladesh; Brac University,Software Engineering and HCI Research Group,Department of Computer Science and Engineering,Bangladesh",2020 International Conference on Machine Learning and Cybernetics (ICMLC),05-Jul-21,2020,,,131,136,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",2160-1348,978-1-6654-1943-7,10.1109/ICMLC51923.2020.9469530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9469530,Parkinson’s Disease (PD);Neurological condition;Dopaminergic neurons;PPMI;MRI;CNN;PET;Extract features;InceptionV3;VGG16;VGG19;Accuracy,Ultrasonic imaging;Handheld computers;Computed tomography;Computer architecture;Medical services;Predictive models;Feature extraction,biomedical MRI;brain;computerised tomography;diseases;feature extraction;image segmentation;learning (artificial intelligence);medical computing;medical disorders;medical image processing;neural nets;neurophysiology;patient diagnosis;positron emission tomography,Parkinson's disease detection;FMRI images leveraging transfer learning;neurological condition;human body;central apprehensive system;neuro-degenerative incubation;particular diagnosis process;Magnetic Resonance Imaging;Positron Emission Tomography scans;common imaging tests;data group - control;Parkinson's Progression Markers Initiative;selected data group;CNN models;different Convolutional Neural Network architectures;VGG16;VGG19;detecting PD,,,,19,,05-Jul-21,,,IEEE,IEEE Conferences
Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Department of Instrumentation and Control Engineering, College of Engineering,Pune; Department of Instrumentation and Control Engineering, College of Engineering,Pune; Department of Instrumentation and Control Engineering, College of Engineering,Pune","2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",07-Dec-21,2021,,,1,6,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",2766-2101,978-1-6654-2849-1,10.1109/CONECCT52877.2021.9622568,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622568,Lung cancer;Machine Learning algorithm;AdaBoost,Support vector machines;Machine learning algorithms;Sensitivity;Computed tomography;Lung cancer;Lung;Feature extraction,biomedical MRI;cancer;computerised tomography;feature extraction;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;patient diagnosis;tumours,lung cancer types prediction;Machine Learning approach;breast cancer;cancerous elements;diagnosing;imaging expertise;Ensemble Machine Learning algorithm;different lung cancer types;lung CT images,,,,20,,07-Dec-21,,,IEEE,IEEE Conferences
A Deep Residual Learning Network for Practical Voxel Dosimetry in Radionuclide Therapy,Z. Li; J. A. Fessler; J. K. Mikell; S. J. Wilderman; Y. K. Dewaraja,"University of Michigan,Department of Electrical Engineering and Computer Science,Ann Arbor,MI,48109-2122; University of Michigan,Department of Electrical Engineering and Computer Science,Ann Arbor,MI,48109-2122; Radiation Oncology, University of Michigan,Ann Arbor,MI,48109-5010; University of Michigan,Department of Nuclear Engineering and Radiologic Sciences,Ann Arbor,MI,48109-2104; University of Michigan,Department of Radiology,Ann Arbor,MI,48109-5667",2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),12-Aug-21,2020,,,1,4,"Current standard methods for voxel-level dosimetry in radionuclide therapy suffers from a tradeoff between accuracy and computational efficiency. Monte Carlo (MC) radiation transport algorithms are considered as the gold standard, but are associated with long computation time, while fast voxel dose kernel (VDK) based methods can be inaccurate in the presence of tissue density heterogeneities. This paper investigates a deep residual Convolutional Neural Networks (CNN) approach that learns the difference between the MC and the VDK dose-rate maps to address the speed-accuracy trade-off issue. As with MC and VDK-based dosimetry, the input to the CNN was the patient's SPECT activity map and CT-based density map. MC dosimetry was used only during the training process to generate ground truth training labels. Furthermore, to potentially account for the degradation of dose-rate maps due to poor SPECT spatial resolution, we trained the CNN using dose-rate maps directly corresponding to phantom activity/density maps that were generated from patient's PET scans. The test data consisted of phantom simulations and one patient who underwent 177Lu DOTATATE therapy for neuroendocrine tumors. In phantom cases, the lesion/organ mean dose-rates from ground truth (GT) agreed better with the CNN dose-rates compared to VDK with density scaling, with an average of 60% improvement for lesions and 55%, 63% improvement for left/right kidney, respectively. For all regions, the normalized root mean square error (NRMSE) relative to GT was substantially lower with CNN than with VDK and MC, i.e., an average of 23%, 22% improvement for lesion, respectively. Using a GPU, the CNN took only about 2.0 seconds to generate a patient's 512×512×130 absorbed dose-rate map while the same calculation took about 40 minutes using our fast in-house Dose Planning Method (DPM) MC algorithm that runs on a CPU. In conclusion, the proposed CNN approach demonstrated consistently higher accuracy than VDK-density scaling and comparable accuracy versus MC and is fast enough to be used clinically.",2577-0829,978-1-7281-7693-2,10.1109/NSS/MIC42677.2020.9507764,National Institute of Biomedical Imaging and Bioengineering (NIBIB)(grant numbers:R01 CA240706); National Cancer Institute (NCI); NIH; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507764,,Training;Degradation;Medical treatment;Phantoms;Graphics processing units;Dosimetry;Convolutional neural networks,computerised tomography;convolutional neural nets;dosimetry;image resolution;kidney;learning (artificial intelligence);medical image processing;Monte Carlo methods;phantoms;positron emission tomography;radiation therapy;single photon emission computed tomography;tumours,voxel dose kernel;deep residual learning network;practical voxel dosimetry;radionuclide therapy;Monte Carlo radiation transport algorithms;tissue density heterogeneities;deep residual convolutional neural networks;VDK dose-rate maps;VDK-based dosimetry;MC dosimetry;SPECT spatial resolution;CNN dose-rates;in-house dose planning method MC algorithm;177Lu DOTATATE therapy;SPECT activity map;CT-based density map;phantom activity-density maps;neuroendocrine tumors;GPU;kidney;time 40.0 min,,,,9,,12-Aug-21,,,IEEE,IEEE Conferences
CMIM: Cross-Modal Information Maximization For Medical Imaging,T. Sylvain; F. Dutil; T. Berthier; L. Di Jorio; M. Luck; D. Hjelm; Y. Bengio,"Université de Montréal,Mila; Imagia Cybernetics; Imagia Cybernetics; Imagia Cybernetics; Université de Montréal,Mila; Microsoft Research; Université de Montréal,Mila","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",13-May-21,2021,,,1190,1194,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",2379-190X,978-1-7281-7605-5,10.1109/ICASSP39728.2021.9414132,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414132,Deep learning;Medical Imaging;Multimodal data;Classification;Segmentation,Training;Image segmentation;Adaptation models;Ultrasonic imaging;Magnetic resonance imaging;Signal processing;Task analysis,biomedical MRI;computerised tomography;image classification;learning (artificial intelligence);medical image processing;medical information systems;radiology,CMIM;cross-modal information maximization;specific information systems;different medical imaging;CT scans;associated radiology reports;train-time those multiple views;test-time;innovative framework;multimodal input;mutual information maximization;train time;medical image classification;inference-time performance;weaker modalities,,,,28,,13-May-21,,,IEEE,IEEE Conferences
Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"Cairo University,Department of Biomedical Engineering and Systems Faculty of Engineering; Cairo University,Department of Biomedical Engineering and Systems Faculty of Engineering; Cairo University,Department of Biomedical Engineering and Systems Faculty of Engineering; Cairo University,Department of Biomedical Engineering and Systems Faculty of Engineering; Cairo University,Department of Biomedical Engineering and Systems Faculty of Engineering; Cairo University,Department of Biomedical Engineering and Systems Faculty of Engineering",2021 3rd Novel Intelligent and Leading Emerging Sciences Conference (NILES),18-Nov-21,2021,,,269,272,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,978-1-6654-2157-7,10.1109/NILES53778.2021.9600556,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600556,Brain Tumor;Image Segmentation;Medical Imaging;3D U-Net;Hyperparamter;Medical Decathlon;Loss Function;Optimization,Deep learning;Image segmentation;Three-dimensional displays;Shape;Magnetic resonance imaging;Positron emission tomography;White matter,biomedical MRI;brain;computerised tomography;convolutional neural nets;deep learning (artificial intelligence);image segmentation;medical image processing;positron emission tomography;tumours,3D brain tumors;brain MRI data;2018 Brain tumor Image Segmentation challenge;Log-Cosh loss function;manual brain tumour segmentation;automatic segmentation algorithms;gray matter;magnetic resonance image;3D U-Net;hyperparameter optimization;white matter;cerebrospinal fluid;positron emission tomography;computer tomography image;deep learning framework;RMSProp;Adam;Nadam;Dice loss;focal Tversky loss;Dice coefficient,,,,20,IEEE,18-Nov-21,,,IEEE,IEEE Conferences
Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Sri Ramakrishna Institute of Technology,Department of Information Technology,Coimbatore,India; PSG College of Technology,Department of Information Technology,Coimbatore,India",2021 Innovations in Power and Advanced Computing Technologies (i-PACT),08-Feb-22,2021,,,1,7,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,978-1-6654-2691-6,10.1109/i-PACT52855.2021.9696646,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696646,Machine Learning;health care;EHR;Prediction,Industries;Support vector machines;Technological innovation;Machine learning algorithms;Systematics;Hospitals;Machine learning,biomedical MRI;health care;hospitals;learning (artificial intelligence);medical administrative data processing;medical image processing;medical information systems,prediction model;health care sector;hidden complex patterns;machine learning algorithms;huge heterogeneous data;data hospitals;hospital administrative records;multidimensional data;proper smart data management;analysis mechanism;modern health care industry;machine learning techniques,,,,30,IEEE,08-Feb-22,,,IEEE,IEEE Conferences
Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"SRM University,Department of ECE,Chennai,India; SRM University,Department of ECE,Chennai,India",2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART),18-Jan-22,2021,,,733,738,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",2767-7362,978-1-6654-3970-1,10.1109/SMART52563.2021.9676229,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676229,Epilepsy;ES Prediction;Different Seizure (Partial;Generalized Clonic-Tonic);Machine Learning,Neurological diseases;Measurement;Brain;Epilepsy;Surgery;Feature extraction;Prediction algorithms,biomedical MRI;brain;diseases;electroencephalography;learning (artificial intelligence);medical disorders;medical image processing;medical signal processing;neurophysiology;patient treatment;seizure;surgery,epileptic seizure prediction;familiar form;neurological diseases;central nervous system;neuro logical disorder;brain activity;unusual behavior;consciousness loss;disorders;brain electrical activity;epilepsy seizure;brain damage;epileptic patients;seizure-free;anti-seizure devices;surgery;notice,,,,26,IEEE,18-Jan-22,,,IEEE,IEEE Conferences
Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Johns Hopkins University,USA; College of Engineering,Thiruvananthapuram,India; A J College of Science and Technology,Thiruvananthapuram,India","2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)",04-Jun-21,2021,,,1,5,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,978-1-6654-4126-1,10.1109/ICBSII51839.2021.9445172,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445172,Ocular Melanoma;eye tumour;Cancer;Deep learning algorithm;CNN;LeNet;hough circle;grayscale conversion,Deep learning;Training;Iris;Machine learning algorithms;Ultrasonic imaging;Melanoma;Cameras,biomedical optical imaging;convolutional neural nets;data acquisition;deep learning (artificial intelligence);diseases;eye;feature extraction;Hough transforms;image segmentation;medical image processing;tumours,eye tumour detection;iris melanocytic tumours;eye diseases;eye melanoma;eye ultrasound;eye images;deep learning;freckle;nevus;melanocytoma;Lisch nodule;melanoma;modified LeNet architecture;deep network model;segmented eyeball images;Hough circle transformation;image augmentation,,,,17,,04-Jun-21,,,IEEE,IEEE Conferences
Guest Editorial Generative Adversarial Networks in Biomedical Image Computing,H. Fu; T. Zhou; S. Li; A. F. Frangi,"Agency for Science, Technology and Research, Singapore; Nanjing University of Science and Technology, China; University of Western Ontario, Canada; University of Leeds, U.K.",IEEE Journal of Biomedical and Health Informatics,17-Jan-22,2022,26,1,4,6,"The papers in this special section focus on generative adversarial networks in biomedical image computing. The field of biomedical imaging has obtained great progress from Roentgen’s original discovery of the X-ray to the current imaging tools, including Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound (US). The benefits of using these non-invasive imaging technologies are to assess the current condition of an organ or tissue, which can be used to monitor a patient over time over time for accurate and timely diagnosis and treatment.With the development of imaging technologies, developing advanced artificial intelligence algorithms for automated image analysis has shown the potential to change many aspects of clinical applications within the next decade. Meanwhile, these advanced technologies have also brought new issues and challenges. Thus, there has been a growing demand for biomedical imaging computing to be a component of clinical trials and device improvement. Currently, Generative adversarial networks (GANs) have been attached growing interests in the computer vision community due to their capability of data generation or translation. GAN-based models are able to learn from a set of training data and generate new data with the same characteristics as the training ones, which have also proven to be the state of the art for generating sharp and realistic images. More importantly, GAN has been rapidly applied to many traditional and novel applications in the medical domain, such as image reconstruction, segmentation, diagnosis, synthesis, and so on. Despite GAN substantial progress in these areas, their application to medical image computing still faces challenges and unsolved problems remain.",2168-2208,,10.1109/JBHI.2021.3134004,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684161,,Special issues and sections;Biomedical imaging;Generative adversarial networks;Image segmentation;Image synthesis;Image reconstruction;Magnetic resonance imaging;Feature extraction;Adversarial machine learning,,,,,,7,IEEE,17-Jan-22,,,IEEE,IEEE Journals
Table of contents,,,2017 IEEE International Conference on Imaging Systems and Techniques (IST),18-Jan-18,2017,,,iii,xxviii,The following topics are dealt with: imaging sensors and detectors; image analysis; image quality assessment; image segmentation; pattern recognition; image enhancement; MRI; CT; SPECT; PET; ET: microscopy and deep learning.,,978-1-5386-1620-8,10.1109/IST.2017.8261437,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261437,,,biomedical MRI;image enhancement;image recognition;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography;single photon emission computed tomography,deep learning;PET;SPECT;CT;MRI;image enhancement;pattern recognition;image segmentation;image quality assessment;image analysis;imaging sensors,,,,,,18-Jan-18,,,IEEE,IEEE Conferences
