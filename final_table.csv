,Document Title,Authors,Abstract,Article Citation Count,Publication Year,DOI,Terms
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,attenuation correction
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,nuclear medicine
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,psnr
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,biographies
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,scatter correction
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,transfer learning
0,Development of a deep learning method for CT-free correction for an ultra-long axial field of view PET scanner,S. Xue; K. P. Bohn; R. Guo; H. Sari; M. Viscione; A. Rominger; B. Li; K. Shi,"Introduction: The possibility of low-dose positron emission tomography (PET) imaging using high sensitivity long axial field of view (FOV) PET/computed tomography (CT) scanners makes CT a critical radiation burden in clinical applications. Artificial intelligence has shown the potential to generate PET images from non-corrected PET images. Our aim in this work is to develop a CT-free correction for a long axial FOV PET scanner. Methods: Whole body PET images of 165 patients scanned with a digital regular FOV PET scanner (Biograph Vision 600 (Siemens Healthineers) in Shanghai and Bern) was included for the development and testing of the deep learning methods. Furthermore, the developed algorithm was tested on data of 7 patients scanned with a long axial FOV scanner (Biograph Vision Quadra, Siemens Healthineers). A 2D generative adversarial network (GAN) was developed featuring a residual dense block, which enables the model to fully exploit hierarchical features from all network layers. The normalized root mean squared error (NRMSE) and peak signal-to-noise ratio (PSNR), were calculated to evaluate the results generated by deep learning. Results: The preliminary results showed that, the developed deep learning method achieved an average NRMSE of 0.4±0.3% and PSNR of 51.4±6.4 for the test on Biograph Vision, and an average NRMSE of 0.5±0.4% and PSNR of 47.9±9.4 for the validation on Biograph Vision Quadra, after applied transfer learning. Conclusion: The developed deep learning method shows the potential for CT-free AI-correction for a long axial FOV PET scanner. Work in progress includes clinical assessment of PET images by independent nuclear medicine physicians. Training and fine-tuning with more datasets will be performed to further consolidate the development.",,2021,10.1109/EMBC46164.2021.9630590,ct-free
1,Novel deep learning-based CT synthesis algorithm for MRI-guided PET attenuation correction in brain PET/MR imaging,H. Arabi; G. Zeng; G. Zheng; H. Zaidi,"MRI-guided synthetic CT (sCT) generation is one of the main challenges hampering quantitative PET/MR imaging as well as MRI-only radiation planning. Deep learning-based approaches have recently gained momentum in a variety of medical imaging applications. In this work, a novel synthetic CT generation algorithm based on deep convolutional neural network is proposed for MRI-guided attenuation correction in PET/MRI. The proposed algorithm (AsCT) exploits adversarial semantic structure learning implemented as a CT segmentation approach to constrain the adversarial synthetic CT generation process. The proposed technique was trained using 50 pairs of CT and MR brain scans under a two-fold<sup>1</sup> cross validation scheme. The AsCT method was compared to an atlas-based method (Bone-Atl), previously developed for MRI-only radiation planning, as well as the commercial segmentation-based approach (2-class) implemented on the Philips TF PET/MRI system. The evaluation was performed using clinical brain studies of 40 patients who have undergone PET/CT and MRI scanning. The accuracy of the CT value estimation and cortical bone identification were assessed for the three different methods taking CT images as reference. Bias of tracer uptake (SUV) was measured on attenuation corrected PET images using the three techniques taking CT-based attenuation corrected PET as reference. Bone-Atl and AsCT exhibited similar cortical bone extraction (using an intensity threshold of 600 Hounsfield Unit (HU)) resulting in Dice coefficient (DSC) of 0.78±0.07 and 0.77±0.07, respectively. Bone-Atl method performed slightly better in terms of accuracy of CT value estimation where a mean absolute error of 123±40 (HU) was obtained for the whole head region while AsCT and 2-class methods led to 141±40 and 230±33 (HU), respectively. Quantitative analysis of brain PET images demonstrated competitive performance of AsCT and Bone-Atl methods where mean relative errors of 1.2±13.8% and 1.0±9.9% were achieved in bony structures, respectively, while the 2-class approach led to a mean SUV error of -14.7±8.9%. The proposed AsCT algorithm showed competitive performance with respect to the atlas-based method and outperformed the segmentation-based (2-class) method with clinically tolerable errors.",,2018,10.1109/NSSMIC.2018.8824733,bones
1,Novel deep learning-based CT synthesis algorithm for MRI-guided PET attenuation correction in brain PET/MR imaging,H. Arabi; G. Zeng; G. Zheng; H. Zaidi,"MRI-guided synthetic CT (sCT) generation is one of the main challenges hampering quantitative PET/MR imaging as well as MRI-only radiation planning. Deep learning-based approaches have recently gained momentum in a variety of medical imaging applications. In this work, a novel synthetic CT generation algorithm based on deep convolutional neural network is proposed for MRI-guided attenuation correction in PET/MRI. The proposed algorithm (AsCT) exploits adversarial semantic structure learning implemented as a CT segmentation approach to constrain the adversarial synthetic CT generation process. The proposed technique was trained using 50 pairs of CT and MR brain scans under a two-fold<sup>1</sup> cross validation scheme. The AsCT method was compared to an atlas-based method (Bone-Atl), previously developed for MRI-only radiation planning, as well as the commercial segmentation-based approach (2-class) implemented on the Philips TF PET/MRI system. The evaluation was performed using clinical brain studies of 40 patients who have undergone PET/CT and MRI scanning. The accuracy of the CT value estimation and cortical bone identification were assessed for the three different methods taking CT images as reference. Bias of tracer uptake (SUV) was measured on attenuation corrected PET images using the three techniques taking CT-based attenuation corrected PET as reference. Bone-Atl and AsCT exhibited similar cortical bone extraction (using an intensity threshold of 600 Hounsfield Unit (HU)) resulting in Dice coefficient (DSC) of 0.78±0.07 and 0.77±0.07, respectively. Bone-Atl method performed slightly better in terms of accuracy of CT value estimation where a mean absolute error of 123±40 (HU) was obtained for the whole head region while AsCT and 2-class methods led to 141±40 and 230±33 (HU), respectively. Quantitative analysis of brain PET images demonstrated competitive performance of AsCT and Bone-Atl methods where mean relative errors of 1.2±13.8% and 1.0±9.9% were achieved in bony structures, respectively, while the 2-class approach led to a mean SUV error of -14.7±8.9%. The proposed AsCT algorithm showed competitive performance with respect to the atlas-based method and outperformed the segmentation-based (2-class) method with clinically tolerable errors.",,2018,10.1109/NSSMIC.2018.8824733,attenuation
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,ct
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,lymph nodes
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,contrast ct
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,adaptive histogram equalization
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,lymph node
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,histograms
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,generative adversarial networks
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,conditional generative adversarial network
2,Contrast CT image generation model using CT image of PET/CT,W. Kim; S. -K. Woo; J. Park; H. Sheen; I. Lim; S. M. Lim; B. Hyun Byun,"Contrast CT imaging is a useful method to highlight structures such as blood vessels and tissue. The aim of this study was to generate contrast enhanced CT image for defining region of blood vessels and tumor from PET/CT image using deep learning approach. The deep learning approach in this study was based on conditional generative adversarial network (cGAN). The structure of cGAN composed of two convolutional neural network (CNN), generator and discriminator. Generator was trained to generate contrast CT (G-CCT) image from CT image which cannot be discriminated from the original contrast CT (CCT) image. Discriminator was trained to discriminate CCT image from G-CCT image by generator. To improve the generation performance of the model, we applied adaptive histogram equalization as image preprocessing. We compare the model trained with CT and the model with histogram equalized CT. Structural similarity index measurement (SSIM) was calculated to measure the similarity between G-CCT and CCT image. We defined and compared the region of lymphoma in left upper cervical lymph node level 2 using CT, CCT, and G-CCT. Detect the lymph node cancer was calculated by short to long axis ratio (S/L) method. In the case of the model trained with CT, SSIM was 0.9493±0.0183 in training phase and 0.9055±0.0484 in testing phase. In the case of the model trained with histogram equalized CT, SSIM was 0.954±0.0149 in training phase and 0.9081±0.047 in testing phase. G-CCT images showed more enhanced contrast in blood vessels and tumor region than CT image. Lymph node cancer S/L of CT, CCT, and G-CCT were 0.412, 0.395, and 0.397, respectively. The tumor S/L of generated contrast CT image was evaluated similar to these of real contrast CT image. This contrast CT image generation based on deep learning is helped for a cost-effective and less-hazardous process of acquiring contrast CT image to patients as well as more anatomical information with only CT scan.",,2018,10.1109/NSSMIC.2018.8824278,adaptive equalizers
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,self-organizing feature maps
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,self-organizing map
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,radiomics
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,cancer
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,redundancy
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,medical treatment
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,tools
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,predictive models
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,feature extraction
3,A Self Organizing Map for Exploratory Analysis of PET Radiomic Features,E. Alsyed; R. Smith; S. Paisey; C. Marshall; E. Spezi,"Texture analysis for quantification of intratumor uptake heterogeneity in PET/CT images has received increasing attention. This allows the extraction of a large number of `radiomic' features to be correlated with end point information such as tumor type, therapy response, prognosis. The conventional complex workflow for calculation of texture features introduces numerous confounding variables. This non exhaustively includes, imaging time post administration of radiopharmaceutical and the method and extent of functional volume segmentation. A lack of understanding on the dependency of texture features with these variables serves as a detriment to the urgent need to standardize texture measurements to pool results from different imaging centers. The utilization of machine learning techniques for feature (and their combinations) selection serves as a promising method to alleviate redundancy in radiomics. To this avail, we introduce for the first time the application of a Kohonen self-organizing feature map to identify the emergent properties present when performing texture analysis. The application of the self-organizing map to radiomic analysis serves as a powerful general-purpose exploratory instrument to reveal the statistical indicators of texture distributions. For this purpose, texture features from PET-CT images of 8 pre-clinical mice with mammary carcinoma xenografts were analyzed with varying post injection imaging time and tumor segmentation contour size. This varying distribution of texture parameters were interpreted by the self-organizing map to reveal two distinct clusters of texture features which are dependent on contour size, providing additional evidence that contour size and hence segmentation method is a confounding variable when performing texture analysis. Furthermore, the self-organizing map can be utilized as a method to incorporate this revealed dependency in a prediction model in the presence of end point information, which will be an area of future work.",,2020,10.1109/NSS/MIC42677.2020.9507846,texture analysis
4,PET-Train: Automatic Ground Truth Generation from PET Acquisitions for Urinary Bladder Segmentation in CT Images using Deep Learning,C. Gsaxner; B. Pfarrkirchner; L. Lindner; A. Pepe; P. M. Roth; J. Egger; J. Wallner,"In this contribution, we propose an automatic ground truth generation approach that utilizes Positron Emission Tomography (PET) acquisitions to train neural networks for automatic urinary bladder segmentation in Computed Tomography (CT) images. We evaluated different deep learning architectures to segment the urinary bladder. However, deep neural networks require a large amount of training data, which is currently the main bottleneck in the medical field, because ground truth labels have to be created by medical experts on a time-consuming slice-by-slice basis. To overcome this problem, we generate the training data set from the PET data of combined PET/CT acquisitions. This can be achieved by applying simple thresholding to the PET data, where the radiotracer accumulates very distinct in the urinary bladder. However, the ultimate goal is to entirely skip PET imaging and its additional radiation exposure in the future, and only use CT images for segmentation.",3.0,2018,10.1109/BMEiCON.2018.8609954,urinary bladder
4,PET-Train: Automatic Ground Truth Generation from PET Acquisitions for Urinary Bladder Segmentation in CT Images using Deep Learning,C. Gsaxner; B. Pfarrkirchner; L. Lindner; A. Pepe; P. M. Roth; J. Egger; J. Wallner,"In this contribution, we propose an automatic ground truth generation approach that utilizes Positron Emission Tomography (PET) acquisitions to train neural networks for automatic urinary bladder segmentation in Computed Tomography (CT) images. We evaluated different deep learning architectures to segment the urinary bladder. However, deep neural networks require a large amount of training data, which is currently the main bottleneck in the medical field, because ground truth labels have to be created by medical experts on a time-consuming slice-by-slice basis. To overcome this problem, we generate the training data set from the PET data of combined PET/CT acquisitions. This can be achieved by applying simple thresholding to the PET data, where the radiotracer accumulates very distinct in the urinary bladder. However, the ultimate goal is to entirely skip PET imaging and its additional radiation exposure in the future, and only use CT images for segmentation.",3.0,2018,10.1109/BMEiCON.2018.8609954,segmentation
4,PET-Train: Automatic Ground Truth Generation from PET Acquisitions for Urinary Bladder Segmentation in CT Images using Deep Learning,C. Gsaxner; B. Pfarrkirchner; L. Lindner; A. Pepe; P. M. Roth; J. Egger; J. Wallner,"In this contribution, we propose an automatic ground truth generation approach that utilizes Positron Emission Tomography (PET) acquisitions to train neural networks for automatic urinary bladder segmentation in Computed Tomography (CT) images. We evaluated different deep learning architectures to segment the urinary bladder. However, deep neural networks require a large amount of training data, which is currently the main bottleneck in the medical field, because ground truth labels have to be created by medical experts on a time-consuming slice-by-slice basis. To overcome this problem, we generate the training data set from the PET data of combined PET/CT acquisitions. This can be achieved by applying simple thresholding to the PET data, where the radiotracer accumulates very distinct in the urinary bladder. However, the ultimate goal is to entirely skip PET imaging and its additional radiation exposure in the future, and only use CT images for segmentation.",3.0,2018,10.1109/BMEiCON.2018.8609954,bladder
5,Co-Learning Feature Fusion Maps From PET-CT Images of Lung Cancer,A. Kumar; M. Fulham; D. Feng; J. Kim,"The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modalityspecific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modalityspecific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p <; 0.05) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.",40.0,2020,10.1109/TMI.2019.2923601,fusion learning
5,Co-Learning Feature Fusion Maps From PET-CT Images of Lung Cancer,A. Kumar; M. Fulham; D. Feng; J. Kim,"The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modalityspecific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modalityspecific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p <; 0.05) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.",40.0,2020,10.1109/TMI.2019.2923601,tumors
5,Co-Learning Feature Fusion Maps From PET-CT Images of Lung Cancer,A. Kumar; M. Fulham; D. Feng; J. Kim,"The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modalityspecific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modalityspecific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p <; 0.05) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.",40.0,2020,10.1109/TMI.2019.2923601,cancer
5,Co-Learning Feature Fusion Maps From PET-CT Images of Lung Cancer,A. Kumar; M. Fulham; D. Feng; J. Kim,"The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modalityspecific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modalityspecific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p <; 0.05) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.",40.0,2020,10.1109/TMI.2019.2923601,lung
6,Deep Analysis of Dementia Disorder Using Artificial Intelligence to Improve Healthcare Services,D. Kavitha; A. Murugan; M. Sathiyanarayanan,"Dementia is a worldwide concern and early discovery of dementia is substantial for the administration of mental illness and healthy living. The data acquired from Magnetic resonance imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) are used in identifying dementia. Albeit numerous non-AI and AI techniques have been implemented to understand the reasons behind dementia there is very little success to predict at an early stage. Considering the self-determination medicine, a new healthcare model that could be delivered by 5G, there is a pressing need to diagnose dementia patients in early stages using deep learning techniques and that can constantly improve assessment and diagnostic tools for distinguishing people with normal brain aging from those who will develop mild cognitive impairment. We will conduct state-of-the-art research to identify various deep learning algorithms and approaches that can be used for the diagnosis of dementia and aid in predicting at an early stage. We will develop and evaluate an intelligent algorithm that will predict and distinguish people with normal brain aging from those with mild cognitive impairment on a constant basis, which will be a good fit to the future healthcare model that will be delivered by 5G.",,2021,10.1109/COMSNETS51098.2021.9352897,dementia
6,Deep Analysis of Dementia Disorder Using Artificial Intelligence to Improve Healthcare Services,D. Kavitha; A. Murugan; M. Sathiyanarayanan,"Dementia is a worldwide concern and early discovery of dementia is substantial for the administration of mental illness and healthy living. The data acquired from Magnetic resonance imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) are used in identifying dementia. Albeit numerous non-AI and AI techniques have been implemented to understand the reasons behind dementia there is very little success to predict at an early stage. Considering the self-determination medicine, a new healthcare model that could be delivered by 5G, there is a pressing need to diagnose dementia patients in early stages using deep learning techniques and that can constantly improve assessment and diagnostic tools for distinguishing people with normal brain aging from those who will develop mild cognitive impairment. We will conduct state-of-the-art research to identify various deep learning algorithms and approaches that can be used for the diagnosis of dementia and aid in predicting at an early stage. We will develop and evaluate an intelligent algorithm that will predict and distinguish people with normal brain aging from those with mild cognitive impairment on a constant basis, which will be a good fit to the future healthcare model that will be delivered by 5G.",,2021,10.1109/COMSNETS51098.2021.9352897,medical services
6,Deep Analysis of Dementia Disorder Using Artificial Intelligence to Improve Healthcare Services,D. Kavitha; A. Murugan; M. Sathiyanarayanan,"Dementia is a worldwide concern and early discovery of dementia is substantial for the administration of mental illness and healthy living. The data acquired from Magnetic resonance imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) are used in identifying dementia. Albeit numerous non-AI and AI techniques have been implemented to understand the reasons behind dementia there is very little success to predict at an early stage. Considering the self-determination medicine, a new healthcare model that could be delivered by 5G, there is a pressing need to diagnose dementia patients in early stages using deep learning techniques and that can constantly improve assessment and diagnostic tools for distinguishing people with normal brain aging from those who will develop mild cognitive impairment. We will conduct state-of-the-art research to identify various deep learning algorithms and approaches that can be used for the diagnosis of dementia and aid in predicting at an early stage. We will develop and evaluate an intelligent algorithm that will predict and distinguish people with normal brain aging from those with mild cognitive impairment on a constant basis, which will be a good fit to the future healthcare model that will be delivered by 5G.",,2021,10.1109/COMSNETS51098.2021.9352897,prediction algorithms
6,Deep Analysis of Dementia Disorder Using Artificial Intelligence to Improve Healthcare Services,D. Kavitha; A. Murugan; M. Sathiyanarayanan,"Dementia is a worldwide concern and early discovery of dementia is substantial for the administration of mental illness and healthy living. The data acquired from Magnetic resonance imaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT) are used in identifying dementia. Albeit numerous non-AI and AI techniques have been implemented to understand the reasons behind dementia there is very little success to predict at an early stage. Considering the self-determination medicine, a new healthcare model that could be delivered by 5G, there is a pressing need to diagnose dementia patients in early stages using deep learning techniques and that can constantly improve assessment and diagnostic tools for distinguishing people with normal brain aging from those who will develop mild cognitive impairment. We will conduct state-of-the-art research to identify various deep learning algorithms and approaches that can be used for the diagnosis of dementia and aid in predicting at an early stage. We will develop and evaluate an intelligent algorithm that will predict and distinguish people with normal brain aging from those with mild cognitive impairment on a constant basis, which will be a good fit to the future healthcare model that will be delivered by 5G.",,2021,10.1109/COMSNETS51098.2021.9352897,alzheimer’s disease (ad)
7,An image segmentation method of lung lymphatic tumors based on PET-CT images,Z. Dong; T. Xu; Y. Kang; J. Lian; B. Shi,"To address the problem from lymphatic tumor segmentation from PET-CT images, this paper presents an image segmentation method based on pulse-coupled neural network (PCNN). PET-CT image detection can provide more detailed tumor information for later diagnosis and treatment, while our adjustable fire-controlled MSPCNN model (AFC-MSPCNN) can segment lung tumors effectively. Related experiment results show that our proposed method can improve the image segmentation accuracy than other competitive methods.",,2021,10.1109/ICIBA52610.2021.9688245,corrosion
7,An image segmentation method of lung lymphatic tumors based on PET-CT images,Z. Dong; T. Xu; Y. Kang; J. Lian; B. Shi,"To address the problem from lymphatic tumor segmentation from PET-CT images, this paper presents an image segmentation method based on pulse-coupled neural network (PCNN). PET-CT image detection can provide more detailed tumor information for later diagnosis and treatment, while our adjustable fire-controlled MSPCNN model (AFC-MSPCNN) can segment lung tumors effectively. Related experiment results show that our proposed method can improve the image segmentation accuracy than other competitive methods.",,2021,10.1109/ICIBA52610.2021.9688245,segmentation
7,An image segmentation method of lung lymphatic tumors based on PET-CT images,Z. Dong; T. Xu; Y. Kang; J. Lian; B. Shi,"To address the problem from lymphatic tumor segmentation from PET-CT images, this paper presents an image segmentation method based on pulse-coupled neural network (PCNN). PET-CT image detection can provide more detailed tumor information for later diagnosis and treatment, while our adjustable fire-controlled MSPCNN model (AFC-MSPCNN) can segment lung tumors effectively. Related experiment results show that our proposed method can improve the image segmentation accuracy than other competitive methods.",,2021,10.1109/ICIBA52610.2021.9688245,afc-mspcnn
7,An image segmentation method of lung lymphatic tumors based on PET-CT images,Z. Dong; T. Xu; Y. Kang; J. Lian; B. Shi,"To address the problem from lymphatic tumor segmentation from PET-CT images, this paper presents an image segmentation method based on pulse-coupled neural network (PCNN). PET-CT image detection can provide more detailed tumor information for later diagnosis and treatment, while our adjustable fire-controlled MSPCNN model (AFC-MSPCNN) can segment lung tumors effectively. Related experiment results show that our proposed method can improve the image segmentation accuracy than other competitive methods.",,2021,10.1109/ICIBA52610.2021.9688245,lung
7,An image segmentation method of lung lymphatic tumors based on PET-CT images,Z. Dong; T. Xu; Y. Kang; J. Lian; B. Shi,"To address the problem from lymphatic tumor segmentation from PET-CT images, this paper presents an image segmentation method based on pulse-coupled neural network (PCNN). PET-CT image detection can provide more detailed tumor information for later diagnosis and treatment, while our adjustable fire-controlled MSPCNN model (AFC-MSPCNN) can segment lung tumors effectively. Related experiment results show that our proposed method can improve the image segmentation accuracy than other competitive methods.",,2021,10.1109/ICIBA52610.2021.9688245,mathematical models
8,A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,Meei-Ling Jan; Keh-Shih Chuang; Guo-Wei Chen; Yu-Ching Ni; S. Chen; Chih-Hsien Chang; Jay Wu; Te-Wei Lee; Ying-Kai Fu,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.",46.0,2005,10.1109/TMI.2005.848617,registration
8,A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,Meei-Ling Jan; Keh-Shih Chuang; Guo-Wei Chen; Yu-Ching Ni; S. Chen; Chih-Hsien Chang; Jay Wu; Te-Wei Lee; Ying-Kai Fu,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.",46.0,2005,10.1109/TMI.2005.848617,pharmaceuticals
8,A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,Meei-Ling Jan; Keh-Shih Chuang; Guo-Wei Chen; Yu-Ching Ni; S. Chen; Chih-Hsien Chang; Jay Wu; Te-Wei Lee; Ying-Kai Fu,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.",46.0,2005,10.1109/TMI.2005.848617,diseases
8,A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,Meei-Ling Jan; Keh-Shih Chuang; Guo-Wei Chen; Yu-Ching Ni; S. Chen; Chih-Hsien Chang; Jay Wu; Te-Wei Lee; Ying-Kai Fu,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.",46.0,2005,10.1109/TMI.2005.848617,testing
8,A three-dimensional registration method for automated fusion of micro PET-CT-SPECT whole-body images,Meei-Ling Jan; Keh-Shih Chuang; Guo-Wei Chen; Yu-Ching Ni; S. Chen; Chih-Hsien Chang; Jay Wu; Te-Wei Lee; Ying-Kai Fu,"Micro positron emission tomography (PET) and micro single-photon emission computed tomography (SPECT), used for imaging small animals, have become essential tools in developing new pharmaceuticals and can be used, among other things, to test new therapeutic approaches in animal models of human disease, as well as to image gene expression. These imaging techniques can be used noninvasively in both detection and quantification. However, functional images provide little information on the structure of tissues and organs, which makes the localization of lesions difficult. Image fusion techniques can be exploited to map the functional images to structural images, such as X-ray computed tomography (CT), to support target identification and to facilitate the interpretation of PET or SPECT studies. Furthermore, the mapping of two functional images of SPECT and PET on a structural CT image can be beneficial for those in vivo studies that require two biological processes to be monitored simultaneously. This paper proposes an automated method for registering PET, CT, and SPECT images for small animals. A calibration phantom and a holder were used to determine the relationship among three-dimensional fields of view of various modalities. The holder was arranged in fixed positions on the couches of the scanners, and the spatial transformation matrix between the modalities was held unchanged. As long as objects were scanned together with the holder, the predetermined matrix could register the acquired tomograms from different modalities, independently of the imaged objects. In this work, the PET scan was performed by Concorde's microPET R4 scanner, and the SPECT and CT data were obtained using the Gamma Medica's X-SPECT/CT system. Fusion studies on phantoms and animals have been successfully performed using this method. For microPET-CT fusion, the maximum registration errors were 0.21 mm /spl plusmn/ 0.14 mm, 0.26 mm /spl plusmn/ 0.14 mm, and 0.45 mm /spl plusmn/ 0.34 mm in the X (right-left), Y (upper lower), and Z (rostral-caudal) directions, respectively; for the microPET-SPECT fusion, they were 0.24 mm /spl plusmn/ 0.14 mm, 0.28 mm /spl plusmn/ 0.15 mm, and 0.54 mm /spl plusmn/ 0.35 mm in the X, Y, and Z directions, respectively. The results indicate that this simple method can be used in routine fusion studies.",46.0,2005,10.1109/TMI.2005.848617,animal structures
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,conditional random fields
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,medical services
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,lymphoma detection
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,mathematical model
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,three-dimensional displays
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,supervoxel
9,3D lymphoma detection in PET-CT images with supervoxel and CRFs,J. Zha; P. Decazes; J. Lapuyade; A. Elmoataz; S. Ruan,"In this paper we present a lymphoma detection method on image PET-CT by combining supervoxel and conditional random fields(CRFs). Positron-emission tomography(PET) is often used to analysis diseases like cancer. And it is usually combined with computed tomography scan (CT), which provides accurate anatomical location of lesions. Most lymphoma detection in PET are based on machine learning technique which requires a large learning database. However, it is difficult to acquire such a large standard database in medical field. In our previous work, a new approach which combines an anatomical atlas obtained in CT with CRFs (Conditional Random Fields) in PET is proposed and is proved to have good results, however it is very time consuming due to the fully connection of each voxel in 3D. To cope with this problem, we proposed a method that combines supervoxel and CRFs to accelerate the progress. Our method consists of 3 steps. First, we apply the supervoxel on the PET image to group the voxels into supervoxels. Then, an anatomic atlas is applied on CT to remove the organs having hyper-fixation in PET. Finally, CRFs will detect lymphoma regions in PET. The obtained results show good performance in terms of speed and lymphoma detection.",1.0,2018,10.1109/IPTA.2018.8608129,sensitivity
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,attenuation correction
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,noise reduction
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,medical services
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,scatter correction
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,attenuation
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,denoising
10,"Simultaneous Attenuation Correction, Scatter Correction, and Denoising in PET Imaging with Deep Learning",J. Hu; W. Whiteley; X. Zhang; C. Zhou; V. Panin,"Low radiation dose is desirable in PET/CT imaging. The delivered dose originates from both CT scans and injected PET radioisotopes. CT data is used for attenuation and scatter corrections in PET image formation. A standard PET dose is usually needed to generate PET images of clinical quality so that physicians can make diagnosis with confidence. In this work, we eliminated the CT scans and reduced the PET dose while maintaining image quality by performing simultaneous attenuation correction, scatter correction, and denoising using a deep learning approach. We trained a multi-layer convolutional neural network (CNN) with non-attenuation corrected, non-scatter corrected, and low dose PET images as input, and fully corrected standard dose PET images as labels. After the CNN is trained, it is used to generate fully corrected standard dose PET images from low dose PET data alone. This capability will make CT scan unnecessary and save PET dose significantly. We validated our methodology with patient data. The results showed that attenuation correction, scatter correction, and denoising can be performed simultaneously using the deep learning method.",,2020,10.1109/NSS/MIC42677.2020.9508020,radioactive materials
12,Automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of planning CT and FDG-PET/CT images,H. Arimura; Z. Jin; Y. Shioyama; K. Nakamura; T. Magome; M. Sasaki,"We have developed an automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of treatment planning computed tomography (CT) and 18F-fluorodeoxyglucose (FDG)-positron emission tomography (PET)/CT images. First, the PET images were registered with the treatment planning CT images through the diagnostic CT images of PET/CT. Second, six voxel-based features including voxel values and magnitudes of image gradient vectors were derived from each voxel in the planning CT and PET /CT image data sets. Finally, lung tumors were extracted by using a support vector machine (SVM), which learned 6 voxel-based features inside and outside each true tumor region determined by radiation oncologists. The results showed that the average DSCs for 3 and 6 features for three cases were 0.744 and 0.899, and thus the SVM may need 6 features to learn the distinguishable characteristics. The proposed method may be useful for assisting treatment planners in delineation of the tumor region.",2.0,2013,10.1109/EMBC.2013.6610168,planning
12,Automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of planning CT and FDG-PET/CT images,H. Arimura; Z. Jin; Y. Shioyama; K. Nakamura; T. Magome; M. Sasaki,"We have developed an automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of treatment planning computed tomography (CT) and 18F-fluorodeoxyglucose (FDG)-positron emission tomography (PET)/CT images. First, the PET images were registered with the treatment planning CT images through the diagnostic CT images of PET/CT. Second, six voxel-based features including voxel values and magnitudes of image gradient vectors were derived from each voxel in the planning CT and PET /CT image data sets. Finally, lung tumors were extracted by using a support vector machine (SVM), which learned 6 voxel-based features inside and outside each true tumor region determined by radiation oncologists. The results showed that the average DSCs for 3 and 6 features for three cases were 0.744 and 0.899, and thus the SVM may need 6 features to learn the distinguishable characteristics. The proposed method may be useful for assisting treatment planners in delineation of the tumor region.",2.0,2013,10.1109/EMBC.2013.6610168,support vector machines
12,Automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of planning CT and FDG-PET/CT images,H. Arimura; Z. Jin; Y. Shioyama; K. Nakamura; T. Magome; M. Sasaki,"We have developed an automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of treatment planning computed tomography (CT) and 18F-fluorodeoxyglucose (FDG)-positron emission tomography (PET)/CT images. First, the PET images were registered with the treatment planning CT images through the diagnostic CT images of PET/CT. Second, six voxel-based features including voxel values and magnitudes of image gradient vectors were derived from each voxel in the planning CT and PET /CT image data sets. Finally, lung tumors were extracted by using a support vector machine (SVM), which learned 6 voxel-based features inside and outside each true tumor region determined by radiation oncologists. The results showed that the average DSCs for 3 and 6 features for three cases were 0.744 and 0.899, and thus the SVM may need 6 features to learn the distinguishable characteristics. The proposed method may be useful for assisting treatment planners in delineation of the tumor region.",2.0,2013,10.1109/EMBC.2013.6610168,tumors
12,Automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of planning CT and FDG-PET/CT images,H. Arimura; Z. Jin; Y. Shioyama; K. Nakamura; T. Magome; M. Sasaki,"We have developed an automated method for extraction of lung tumors using a machine learning classifier with knowledge of radiation oncologists on data sets of treatment planning computed tomography (CT) and 18F-fluorodeoxyglucose (FDG)-positron emission tomography (PET)/CT images. First, the PET images were registered with the treatment planning CT images through the diagnostic CT images of PET/CT. Second, six voxel-based features including voxel values and magnitudes of image gradient vectors were derived from each voxel in the planning CT and PET /CT image data sets. Finally, lung tumors were extracted by using a support vector machine (SVM), which learned 6 voxel-based features inside and outside each true tumor region determined by radiation oncologists. The results showed that the average DSCs for 3 and 6 features for three cases were 0.744 and 0.899, and thus the SVM may need 6 features to learn the distinguishable characteristics. The proposed method may be useful for assisting treatment planners in delineation of the tumor region.",2.0,2013,10.1109/EMBC.2013.6610168,lungs
13,Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung Tumor Segmentation,X. Fu; L. Bi; A. Kumar; M. Fulham; J. Kim,"Multimodal positron emission tomography-computed tomography (PET-CT) is used routinely in the assessment of cancer. PET-CT combines the high sensitivity for tumor detection of PET and anatomical information from CT. Tumor segmentation is a critical element of PET-CT but at present, the performance of existing automated methods for this challenging task is low. Segmentation tends to be done manually by different imaging experts, which is labor-intensive and prone to errors and inconsistency. Previous automated segmentation methods largely focused on fusing information that is extracted separately from the PET and CT modalities, with the underlying assumption that each modality contains complementary information. However, these methods do not fully exploit the high PET tumor sensitivity that can guide the segmentation. We introduce a deep learning-based framework in multimodal PET-CT segmentation with a multimodal spatial attention module (MSAM). The MSAM automatically learns to emphasize regions (spatial areas) related to tumors and suppress normal regions with physiologic high-uptake from the PET input. The resulting spatial attention maps are subsequently employed to target a convolutional neural network (CNN) backbone for segmentation of areas with higher tumor likelihood from the CT image. Our experimental results on two clinical PET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma (STS) validate the effectiveness of our framework in these different cancer types. We show that our MSAM, with a conventional U-Net backbone, surpasses the state-of-the-art lung tumor segmentation approach by a margin of 7.6% in Dice similarity coefficient (DSC).",6.0,2021,10.1109/JBHI.2021.3059453,tumors
13,Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung Tumor Segmentation,X. Fu; L. Bi; A. Kumar; M. Fulham; J. Kim,"Multimodal positron emission tomography-computed tomography (PET-CT) is used routinely in the assessment of cancer. PET-CT combines the high sensitivity for tumor detection of PET and anatomical information from CT. Tumor segmentation is a critical element of PET-CT but at present, the performance of existing automated methods for this challenging task is low. Segmentation tends to be done manually by different imaging experts, which is labor-intensive and prone to errors and inconsistency. Previous automated segmentation methods largely focused on fusing information that is extracted separately from the PET and CT modalities, with the underlying assumption that each modality contains complementary information. However, these methods do not fully exploit the high PET tumor sensitivity that can guide the segmentation. We introduce a deep learning-based framework in multimodal PET-CT segmentation with a multimodal spatial attention module (MSAM). The MSAM automatically learns to emphasize regions (spatial areas) related to tumors and suppress normal regions with physiologic high-uptake from the PET input. The resulting spatial attention maps are subsequently employed to target a convolutional neural network (CNN) backbone for segmentation of areas with higher tumor likelihood from the CT image. Our experimental results on two clinical PET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma (STS) validate the effectiveness of our framework in these different cancer types. We show that our MSAM, with a conventional U-Net backbone, surpasses the state-of-the-art lung tumor segmentation approach by a margin of 7.6% in Dice similarity coefficient (DSC).",6.0,2021,10.1109/JBHI.2021.3059453,feature extraction
13,Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung Tumor Segmentation,X. Fu; L. Bi; A. Kumar; M. Fulham; J. Kim,"Multimodal positron emission tomography-computed tomography (PET-CT) is used routinely in the assessment of cancer. PET-CT combines the high sensitivity for tumor detection of PET and anatomical information from CT. Tumor segmentation is a critical element of PET-CT but at present, the performance of existing automated methods for this challenging task is low. Segmentation tends to be done manually by different imaging experts, which is labor-intensive and prone to errors and inconsistency. Previous automated segmentation methods largely focused on fusing information that is extracted separately from the PET and CT modalities, with the underlying assumption that each modality contains complementary information. However, these methods do not fully exploit the high PET tumor sensitivity that can guide the segmentation. We introduce a deep learning-based framework in multimodal PET-CT segmentation with a multimodal spatial attention module (MSAM). The MSAM automatically learns to emphasize regions (spatial areas) related to tumors and suppress normal regions with physiologic high-uptake from the PET input. The resulting spatial attention maps are subsequently employed to target a convolutional neural network (CNN) backbone for segmentation of areas with higher tumor likelihood from the CT image. Our experimental results on two clinical PET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma (STS) validate the effectiveness of our framework in these different cancer types. We show that our MSAM, with a conventional U-Net backbone, surpasses the state-of-the-art lung tumor segmentation approach by a margin of 7.6% in Dice similarity coefficient (DSC).",6.0,2021,10.1109/JBHI.2021.3059453,sensitivity
14,PET/CT Radiomic Sequencer for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients,I. Shiri; H. Maleki; G. Hajianfar; H. Abdollahi; S. Ashrafinia; M. G. Oghli; M. Hatt; M. Oveisi; A. Rahmim,"The aim of this study was to develop radiomic models using PET/CT radiomic features with different machine learning approaches for finding best predictive epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) mutation status. Patient's images including PET and CT [diagnostic (CTD) and low dose CT (CTA)] were pre-processed using wavelet (WAV), Laplacian of Gaussian (LOG) and 64 bin discretization (BIN) (alone or in combinations) and several features from images were extracted. The prediction performance of model was checked using the area under the receiver operator characteristic (ROC) curve (AUC). Results showed a wide range of radiomic model AUC performances up to 0.75 in prediction of EGFR and KRAS mutation status. Combination of K-Best and variance threshold feature selector with logistic regression (LREG) classifier in diagnostic CT scan led to the best performance in EGFR (CTD-BIN+B-KB+LREG, AUC: 0.75±0.10) and KRAS (CTD-BIN-LOG-WAV+B-VT+LREG, AUC: 0.75±0.07) respectively. Additionally, incorporating PET, kept AUC values at ~0.74. When considering conventional features only, highest predictive performance was achieved by PET SUVpeak (AUC: 0.69) for EGFR and by PET MTV (AUC: 0.55) for KRAS. In comparison with conventional PET parameters such as standard uptake value, radiomic models were found as more predictive. Our findings demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients.",2.0,2018,10.1109/NSSMIC.2018.8824469,feature extraction
14,PET/CT Radiomic Sequencer for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients,I. Shiri; H. Maleki; G. Hajianfar; H. Abdollahi; S. Ashrafinia; M. G. Oghli; M. Hatt; M. Oveisi; A. Rahmim,"The aim of this study was to develop radiomic models using PET/CT radiomic features with different machine learning approaches for finding best predictive epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) mutation status. Patient's images including PET and CT [diagnostic (CTD) and low dose CT (CTA)] were pre-processed using wavelet (WAV), Laplacian of Gaussian (LOG) and 64 bin discretization (BIN) (alone or in combinations) and several features from images were extracted. The prediction performance of model was checked using the area under the receiver operator characteristic (ROC) curve (AUC). Results showed a wide range of radiomic model AUC performances up to 0.75 in prediction of EGFR and KRAS mutation status. Combination of K-Best and variance threshold feature selector with logistic regression (LREG) classifier in diagnostic CT scan led to the best performance in EGFR (CTD-BIN+B-KB+LREG, AUC: 0.75±0.10) and KRAS (CTD-BIN-LOG-WAV+B-VT+LREG, AUC: 0.75±0.07) respectively. Additionally, incorporating PET, kept AUC values at ~0.74. When considering conventional features only, highest predictive performance was achieved by PET SUVpeak (AUC: 0.69) for EGFR and by PET MTV (AUC: 0.55) for KRAS. In comparison with conventional PET parameters such as standard uptake value, radiomic models were found as more predictive. Our findings demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients.",2.0,2018,10.1109/NSSMIC.2018.8824469,cancer
14,PET/CT Radiomic Sequencer for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients,I. Shiri; H. Maleki; G. Hajianfar; H. Abdollahi; S. Ashrafinia; M. G. Oghli; M. Hatt; M. Oveisi; A. Rahmim,"The aim of this study was to develop radiomic models using PET/CT radiomic features with different machine learning approaches for finding best predictive epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) mutation status. Patient's images including PET and CT [diagnostic (CTD) and low dose CT (CTA)] were pre-processed using wavelet (WAV), Laplacian of Gaussian (LOG) and 64 bin discretization (BIN) (alone or in combinations) and several features from images were extracted. The prediction performance of model was checked using the area under the receiver operator characteristic (ROC) curve (AUC). Results showed a wide range of radiomic model AUC performances up to 0.75 in prediction of EGFR and KRAS mutation status. Combination of K-Best and variance threshold feature selector with logistic regression (LREG) classifier in diagnostic CT scan led to the best performance in EGFR (CTD-BIN+B-KB+LREG, AUC: 0.75±0.10) and KRAS (CTD-BIN-LOG-WAV+B-VT+LREG, AUC: 0.75±0.07) respectively. Additionally, incorporating PET, kept AUC values at ~0.74. When considering conventional features only, highest predictive performance was achieved by PET SUVpeak (AUC: 0.69) for EGFR and by PET MTV (AUC: 0.55) for KRAS. In comparison with conventional PET parameters such as standard uptake value, radiomic models were found as more predictive. Our findings demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients.",2.0,2018,10.1109/NSSMIC.2018.8824469,predictive models
14,PET/CT Radiomic Sequencer for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients,I. Shiri; H. Maleki; G. Hajianfar; H. Abdollahi; S. Ashrafinia; M. G. Oghli; M. Hatt; M. Oveisi; A. Rahmim,"The aim of this study was to develop radiomic models using PET/CT radiomic features with different machine learning approaches for finding best predictive epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) mutation status. Patient's images including PET and CT [diagnostic (CTD) and low dose CT (CTA)] were pre-processed using wavelet (WAV), Laplacian of Gaussian (LOG) and 64 bin discretization (BIN) (alone or in combinations) and several features from images were extracted. The prediction performance of model was checked using the area under the receiver operator characteristic (ROC) curve (AUC). Results showed a wide range of radiomic model AUC performances up to 0.75 in prediction of EGFR and KRAS mutation status. Combination of K-Best and variance threshold feature selector with logistic regression (LREG) classifier in diagnostic CT scan led to the best performance in EGFR (CTD-BIN+B-KB+LREG, AUC: 0.75±0.10) and KRAS (CTD-BIN-LOG-WAV+B-VT+LREG, AUC: 0.75±0.07) respectively. Additionally, incorporating PET, kept AUC values at ~0.74. When considering conventional features only, highest predictive performance was achieved by PET SUVpeak (AUC: 0.69) for EGFR and by PET MTV (AUC: 0.55) for KRAS. In comparison with conventional PET parameters such as standard uptake value, radiomic models were found as more predictive. Our findings demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients.",2.0,2018,10.1109/NSSMIC.2018.8824469,lung
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,anatomical structure
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,abdomen
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,anatomical constraints
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,nonrigid registration
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,free-form deformations (ffd)
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,anatomy
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,deformable models
15,Explicit Incorporation of Prior Anatomical Information Into a Nonrigid Registration of Thoracic and Abdominal CT and 18-FDG Whole-Body Emission PET Images,O. Camara; G. Delso; O. Colliot; A. Moreno-Ingelmo; I. Bloch,"The aim of this paper is to develop a registration methodology in order to combine anatomical and functional information provided by thoracic/abdominal computed tomography (CT) and whole-body positron emission tomography (PET) images. The proposed procedure is based on the incorporation of prior anatomical information in an intensity-based nonrigid registration algorithm. This incorporation is achieved in an explicit way, initializing the intensity-based registration stage with the solution obtained by a nonrigid registration of corresponding anatomical structures. A segmentation algorithm based on a hierarchically ordered set of anatomy-specific rules is used to obtain anatomical structures in CT and emission PET scans. Nonrigid deformations are modeled in both registration stages by means of free-form deformations, the optimization of the control points being achieved by means of an original vector field-based approach instead of the classical gradient-based techniques, considerably reducing the computational time of the structure registration stage. We have applied the proposed methodology to 38 sets of images (33 provided by standalone machines and five by hybrid systems) and an assessment protocol has been developed to furnish a qualitative evaluation of the algorithm performance",24.0,2007,10.1109/TMI.2006.889712,oncology
16,3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",27.0,2018,10.1109/ISBI.2018.8363561,lung tumor segmentation
16,3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",27.0,2018,10.1109/ISBI.2018.8363561,co-segmentation
16,3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",27.0,2018,10.1109/ISBI.2018.8363561,fully convolutional networks
16,3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",27.0,2018,10.1109/ISBI.2018.8363561,three-dimensional displays
16,3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",27.0,2018,10.1109/ISBI.2018.8363561,tumors
16,3D fully convolutional networks for co-segmentation of tumors on PET-CT images,Z. Zhong; Y. Kim; L. Zhou; K. Plichta; B. Allen; J. Buatti; X. Wu,"Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.",27.0,2018,10.1109/ISBI.2018.8363561,lung
17,Can Deep Learning Detect Esophageal Lesions In PET-CT Scans?,I. Ackerley; R. Smith; J. Scuffham; M. Halling-Brown; E. Lewis; E. Spezi; V. Prakash; K. Wells,"PET-CT scans using <sup>18</sup>F-FDG with a co-registered CT scan are increasingly used to detect cancer. This paper compares deep learning-based lesion detection tools trained on PET, CT and combined modality data. 486 pre-contoured scans were used from a retrospective cohort study into esophageal cancer. Scans were partitioned into training, validation and test sets with an 80:10:10 ratio. 1000 image segments were generated from each scan, with tumor present segments located on the contoured lesion and tumor absent segments distributed randomly within the patient but excluding the tumor. PET and CT image segments were used to train a separate dedicated 5-layer convolutional neural networks (CNN). Testing on segments from unseen scans resulted in an accuracy of greater than 95% for the PET data, and greater than 90% for CT data.",,2019,10.1109/NSS/MIC42101.2019.9059833,solid modeling
17,Can Deep Learning Detect Esophageal Lesions In PET-CT Scans?,I. Ackerley; R. Smith; J. Scuffham; M. Halling-Brown; E. Lewis; E. Spezi; V. Prakash; K. Wells,"PET-CT scans using <sup>18</sup>F-FDG with a co-registered CT scan are increasingly used to detect cancer. This paper compares deep learning-based lesion detection tools trained on PET, CT and combined modality data. 486 pre-contoured scans were used from a retrospective cohort study into esophageal cancer. Scans were partitioned into training, validation and test sets with an 80:10:10 ratio. 1000 image segments were generated from each scan, with tumor present segments located on the contoured lesion and tumor absent segments distributed randomly within the patient but excluding the tumor. PET and CT image segments were used to train a separate dedicated 5-layer convolutional neural networks (CNN). Testing on segments from unseen scans resulted in an accuracy of greater than 95% for the PET data, and greater than 90% for CT data.",,2019,10.1109/NSS/MIC42101.2019.9059833,data models
17,Can Deep Learning Detect Esophageal Lesions In PET-CT Scans?,I. Ackerley; R. Smith; J. Scuffham; M. Halling-Brown; E. Lewis; E. Spezi; V. Prakash; K. Wells,"PET-CT scans using <sup>18</sup>F-FDG with a co-registered CT scan are increasingly used to detect cancer. This paper compares deep learning-based lesion detection tools trained on PET, CT and combined modality data. 486 pre-contoured scans were used from a retrospective cohort study into esophageal cancer. Scans were partitioned into training, validation and test sets with an 80:10:10 ratio. 1000 image segments were generated from each scan, with tumor present segments located on the contoured lesion and tumor absent segments distributed randomly within the patient but excluding the tumor. PET and CT image segments were used to train a separate dedicated 5-layer convolutional neural networks (CNN). Testing on segments from unseen scans resulted in an accuracy of greater than 95% for the PET data, and greater than 90% for CT data.",,2019,10.1109/NSS/MIC42101.2019.9059833,three-dimensional displays
17,Can Deep Learning Detect Esophageal Lesions In PET-CT Scans?,I. Ackerley; R. Smith; J. Scuffham; M. Halling-Brown; E. Lewis; E. Spezi; V. Prakash; K. Wells,"PET-CT scans using <sup>18</sup>F-FDG with a co-registered CT scan are increasingly used to detect cancer. This paper compares deep learning-based lesion detection tools trained on PET, CT and combined modality data. 486 pre-contoured scans were used from a retrospective cohort study into esophageal cancer. Scans were partitioned into training, validation and test sets with an 80:10:10 ratio. 1000 image segments were generated from each scan, with tumor present segments located on the contoured lesion and tumor absent segments distributed randomly within the patient but excluding the tumor. PET and CT image segments were used to train a separate dedicated 5-layer convolutional neural networks (CNN). Testing on segments from unseen scans resulted in an accuracy of greater than 95% for the PET data, and greater than 90% for CT data.",,2019,10.1109/NSS/MIC42101.2019.9059833,tumors
17,Can Deep Learning Detect Esophageal Lesions In PET-CT Scans?,I. Ackerley; R. Smith; J. Scuffham; M. Halling-Brown; E. Lewis; E. Spezi; V. Prakash; K. Wells,"PET-CT scans using <sup>18</sup>F-FDG with a co-registered CT scan are increasingly used to detect cancer. This paper compares deep learning-based lesion detection tools trained on PET, CT and combined modality data. 486 pre-contoured scans were used from a retrospective cohort study into esophageal cancer. Scans were partitioned into training, validation and test sets with an 80:10:10 ratio. 1000 image segments were generated from each scan, with tumor present segments located on the contoured lesion and tumor absent segments distributed randomly within the patient but excluding the tumor. PET and CT image segments were used to train a separate dedicated 5-layer convolutional neural networks (CNN). Testing on segments from unseen scans resulted in an accuracy of greater than 95% for the PET data, and greater than 90% for CT data.",,2019,10.1109/NSS/MIC42101.2019.9059833,heating systems
18,SVM based lung cancer diagnosis using multiple image features in PET/CT,N. Guo; R. Yen; G. E. Fakhri; Q. Li,"In this project, we assessed the clinical value of tumor heterogeneity measured with <sup>18</sup>F-FLT as a biomarker for lung cancer diagnosis and staging, then compared its performance to traditional image features using final pathology as gold standard. We also proposed to apply support vector machine (SVM) to train a vector of image features including heterogeneity extracted from PET image and CT texture features to improve the diagnosis and staging for lung cancer. Thirty-two subjects with lung nodules (19 M, 13 F, age 70 ± 9 y) who underwent <sup>18</sup>F-FLT PET/CT scans were included in our study. We applied the global Moran I(d) analysis to characterize the intra-tumor heterogeneity on PET images 1h post-injection. Other than texture analysis that widely used in heterogeneity prediction, I(d) statistic is a measure of spatial autocorrelation characterized by the correlation among 3D neighboring voxels. Other image features including SUV and CT texture were extracted from PET/CT images. Then we trained and applied a SVM based statistical machine learning tool to fuse the features and test the SVM performance in classifying patient groups: benign/early malignant and early/advanced malignant. Heterogeneity derived from <sup>18</sup>F-FLT images significantly differentiated benign (0.24 ± 0.09, N = 9) from early stage malignancy (0.40 ± 0.09, N = 10; P = 0.002), as well as early stage from advanced stage malignancy (0.50 ± 0.07, N = 13, P = 0.005). Other image features, SUVmean and CT texture, didn't demonstrated similar capability. Intra-tumor heterogeneity showed superior performance than other traditional image features when single feature was applied to staging. Furthermore, the SVM classification showed that best performance of staging was achieved when all image features are combined in the SVM training. In conclusion, we obtained a novel measurement of intra-tumor heterogeneity which has promising performance for diagnosis and staging of lung cancer. We demonstrated the feasibility of performing SVM based cancer staging using multiple image features in PET/CT. SVM analysis and classification with combination of effective features has great potential to augment diagnostic accuracy and improve tumor staging in oncological practice.",5.0,2015,10.1109/NSSMIC.2015.7582234,support vector machines
18,SVM based lung cancer diagnosis using multiple image features in PET/CT,N. Guo; R. Yen; G. E. Fakhri; Q. Li,"In this project, we assessed the clinical value of tumor heterogeneity measured with <sup>18</sup>F-FLT as a biomarker for lung cancer diagnosis and staging, then compared its performance to traditional image features using final pathology as gold standard. We also proposed to apply support vector machine (SVM) to train a vector of image features including heterogeneity extracted from PET image and CT texture features to improve the diagnosis and staging for lung cancer. Thirty-two subjects with lung nodules (19 M, 13 F, age 70 ± 9 y) who underwent <sup>18</sup>F-FLT PET/CT scans were included in our study. We applied the global Moran I(d) analysis to characterize the intra-tumor heterogeneity on PET images 1h post-injection. Other than texture analysis that widely used in heterogeneity prediction, I(d) statistic is a measure of spatial autocorrelation characterized by the correlation among 3D neighboring voxels. Other image features including SUV and CT texture were extracted from PET/CT images. Then we trained and applied a SVM based statistical machine learning tool to fuse the features and test the SVM performance in classifying patient groups: benign/early malignant and early/advanced malignant. Heterogeneity derived from <sup>18</sup>F-FLT images significantly differentiated benign (0.24 ± 0.09, N = 9) from early stage malignancy (0.40 ± 0.09, N = 10; P = 0.002), as well as early stage from advanced stage malignancy (0.50 ± 0.07, N = 13, P = 0.005). Other image features, SUVmean and CT texture, didn't demonstrated similar capability. Intra-tumor heterogeneity showed superior performance than other traditional image features when single feature was applied to staging. Furthermore, the SVM classification showed that best performance of staging was achieved when all image features are combined in the SVM training. In conclusion, we obtained a novel measurement of intra-tumor heterogeneity which has promising performance for diagnosis and staging of lung cancer. We demonstrated the feasibility of performing SVM based cancer staging using multiple image features in PET/CT. SVM analysis and classification with combination of effective features has great potential to augment diagnostic accuracy and improve tumor staging in oncological practice.",5.0,2015,10.1109/NSSMIC.2015.7582234,tumors
18,SVM based lung cancer diagnosis using multiple image features in PET/CT,N. Guo; R. Yen; G. E. Fakhri; Q. Li,"In this project, we assessed the clinical value of tumor heterogeneity measured with <sup>18</sup>F-FLT as a biomarker for lung cancer diagnosis and staging, then compared its performance to traditional image features using final pathology as gold standard. We also proposed to apply support vector machine (SVM) to train a vector of image features including heterogeneity extracted from PET image and CT texture features to improve the diagnosis and staging for lung cancer. Thirty-two subjects with lung nodules (19 M, 13 F, age 70 ± 9 y) who underwent <sup>18</sup>F-FLT PET/CT scans were included in our study. We applied the global Moran I(d) analysis to characterize the intra-tumor heterogeneity on PET images 1h post-injection. Other than texture analysis that widely used in heterogeneity prediction, I(d) statistic is a measure of spatial autocorrelation characterized by the correlation among 3D neighboring voxels. Other image features including SUV and CT texture were extracted from PET/CT images. Then we trained and applied a SVM based statistical machine learning tool to fuse the features and test the SVM performance in classifying patient groups: benign/early malignant and early/advanced malignant. Heterogeneity derived from <sup>18</sup>F-FLT images significantly differentiated benign (0.24 ± 0.09, N = 9) from early stage malignancy (0.40 ± 0.09, N = 10; P = 0.002), as well as early stage from advanced stage malignancy (0.50 ± 0.07, N = 13, P = 0.005). Other image features, SUVmean and CT texture, didn't demonstrated similar capability. Intra-tumor heterogeneity showed superior performance than other traditional image features when single feature was applied to staging. Furthermore, the SVM classification showed that best performance of staging was achieved when all image features are combined in the SVM training. In conclusion, we obtained a novel measurement of intra-tumor heterogeneity which has promising performance for diagnosis and staging of lung cancer. We demonstrated the feasibility of performing SVM based cancer staging using multiple image features in PET/CT. SVM analysis and classification with combination of effective features has great potential to augment diagnostic accuracy and improve tumor staging in oncological practice.",5.0,2015,10.1109/NSSMIC.2015.7582234,cancer
18,SVM based lung cancer diagnosis using multiple image features in PET/CT,N. Guo; R. Yen; G. E. Fakhri; Q. Li,"In this project, we assessed the clinical value of tumor heterogeneity measured with <sup>18</sup>F-FLT as a biomarker for lung cancer diagnosis and staging, then compared its performance to traditional image features using final pathology as gold standard. We also proposed to apply support vector machine (SVM) to train a vector of image features including heterogeneity extracted from PET image and CT texture features to improve the diagnosis and staging for lung cancer. Thirty-two subjects with lung nodules (19 M, 13 F, age 70 ± 9 y) who underwent <sup>18</sup>F-FLT PET/CT scans were included in our study. We applied the global Moran I(d) analysis to characterize the intra-tumor heterogeneity on PET images 1h post-injection. Other than texture analysis that widely used in heterogeneity prediction, I(d) statistic is a measure of spatial autocorrelation characterized by the correlation among 3D neighboring voxels. Other image features including SUV and CT texture were extracted from PET/CT images. Then we trained and applied a SVM based statistical machine learning tool to fuse the features and test the SVM performance in classifying patient groups: benign/early malignant and early/advanced malignant. Heterogeneity derived from <sup>18</sup>F-FLT images significantly differentiated benign (0.24 ± 0.09, N = 9) from early stage malignancy (0.40 ± 0.09, N = 10; P = 0.002), as well as early stage from advanced stage malignancy (0.50 ± 0.07, N = 13, P = 0.005). Other image features, SUVmean and CT texture, didn't demonstrated similar capability. Intra-tumor heterogeneity showed superior performance than other traditional image features when single feature was applied to staging. Furthermore, the SVM classification showed that best performance of staging was achieved when all image features are combined in the SVM training. In conclusion, we obtained a novel measurement of intra-tumor heterogeneity which has promising performance for diagnosis and staging of lung cancer. We demonstrated the feasibility of performing SVM based cancer staging using multiple image features in PET/CT. SVM analysis and classification with combination of effective features has great potential to augment diagnostic accuracy and improve tumor staging in oncological practice.",5.0,2015,10.1109/NSSMIC.2015.7582234,lungs
19,Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",,2020,10.1109/NSS/MIC42677.2020.9507903,attenuation correction
19,Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",,2020,10.1109/NSS/MIC42677.2020.9507903,mr-based attenuation map
19,Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",,2020,10.1109/NSS/MIC42677.2020.9507903,head
19,Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",,2020,10.1109/NSS/MIC42677.2020.9507903,conditional generative adversarial network
19,Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",,2020,10.1109/NSS/MIC42677.2020.9507903,attenuation
19,Automatic Generation of MR-based Attenuation Map using Conditional Generative Adversarial Network for Attenuation Correction in PET/MR,E. Anaya; C. Levin,"Attenuation correction is an important correction for quantitative PET image reconstruction. Current PET/MR attenuation correction methods involve segmenting MR images acquired with zero-time echo (ZTE) or Dixon sequences and assigning known attenuation coefficients to different tissues. This work builds upon our previous work where we explore a novel deep learning method of attenuation map (μ-map) generation using a conditional generative adversarial network (cGAN) that allows for continuous attenuation coefficients. We develop the use of a cGAN network to directly convert MR images to CT images (pseudo CT) through registered training data. A straightforward bilinear conversion can be applied to the pseudo CT images to obtain attenuation maps at 511keV for PET attenuation correction of the head and neck region, including brain. The overall average MAE of the pseudo CT compared to the real CT test images was found to be 88.2 ± 32.7 HU. Future work includes applying the correction on PET data and comparing the reconstructed PET image with CT-based attenuation correction at 511keV as the gold standard.",,2020,10.1109/NSS/MIC42677.2020.9507903,conferences
20,Deep Learning based Respiratory Pattern Classification and Applications in PET/CT Motion Correction,Y. Guo; N. Dvornek; Y. Lu; Y. -J. Tsai; J. Hamill; M. Casey; C. Liu,"Respiratory motion has to be corrected in PET/CT imaging for precise tumor detection and quantification. The optimal motion correction methods for regular breathers and irregular breathers could be different. In this study, we developed deep learning based methods to automatically classify patient breathing patterns and investigated the impact of breathing pattern variability on gating performance. We implemented a hybrid neural network consisting of convolutional (Conv) layers, recurrent layers (LSTM, long short-term memory) and a linear classifier to differentiate breathing patterns. 1295 respiratory traces collected using RPM (Real-time Position Management) system were used for training and testing, as well as additional traces acquired using the Anzai system. We optimized the deep neural network with respect to data preprocessing, augmentation, weighted loss function and generalization capability. The results showed that the proposed deep learning model has reached a high prediction accuracy, with a sensitivity of 92.0% and a specificity of 91.8%. Using phase gating approach, for regular breathers, end-expiration phase gating can effectively reduce the respiratory motion. In contrast, for irregular breathers, larger amount of intra-gate motion was present in the gated PET/CT images and more sophisticated motion correction methods are required.",3.0,2019,10.1109/NSS/MIC42101.2019.9059783,tumors
21,Deep multi-modality collaborative learning for distant metastases predication in PET-CT soft-tissue sarcoma studies,Y. Peng; L. Bi; Y. Guo; D. Feng; M. Fulham; J. Kim,"Soft-tissue Sarcomas (STS) are a heterogeneous group of malignant neoplasms with a relatively high mortality rate from distant metastases. Early prediction or quantitative evaluation of distant metastases risk for patients with STS is an important step which can provide better-personalized treatments and thereby improve survival rates. Positron emission tomography-computed tomography (PET-CT) image is regarded as the imaging modality of choice for the evaluation, staging and assessment of STS. Radiomics, which refers to the extraction and analysis of the quantitative of high-dimensional mineable data from medical images, is foreseen as an important prognostic tool for cancer risk assessment. However, conventional radiomics methods that depend heavily on hand-crafted features (e.g. shape and texture) and prior knowledge (e.g. tuning of many parameters) therefore cannot fully represent the semantic information of the image. In addition, convolutional neural networks (CNN) based radiomics methods present capabilities to improve, but currently, they are mainly designed for single modality e.g., CT or a particular body region e.g., lung structure. In this work, we propose a deep multi-modality collaborative learning to iteratively derive optimal ensembled deep and conventional features from PET-CT images. In addition, we introduce an end-to-end volumetric deep learning architecture to learn complementary PET-CT features optimised for image radiomics. Our experimental results using public PET-CT dataset of STS patients demonstrate that our method has better performance when compared with the state-of-the-art methods.",5.0,2019,10.1109/EMBC.2019.8857666,None
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,anatomical structure
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,mutual information
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,convolutional neural network
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,three-dimensional displays
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,bladder
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,cancer
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,unsupervised registration
22,Unsupervised 3D PET-CT Image Registration Method Using a Metabolic Constraint Function and a Multi-Domain Similarity Measure,H. Yu; H. Jiang; X. Zhou; T. Hara; Y. -D. Yao; H. Fujita,"High-resolution CT images can clearly display anatomical structures but does not display functional information, while blurred PET images can display molecular and functional information of lesions but cannot clearly display morphological structures. Therefore, accurate PET-CT image registration, which is used for anatomical structure and functional information fusion, is a prerequisite for early stage cancer diagnosis. However, some hypermetabolic anatomical structures, such as brain and bladder, have low registration accuracy. To solve this problem, a 3D unsupervised network based on a metabolic constraint function and a multi-domain similarity measure (3D MC-MDS Net) is proposed for 3D PET-CT image registration. Specifically, a metabolic constraint model is established based on the standard uptake value (SUV) distribution of hypermetabolic regions such as brain, bladder, liver and heart, which reduces the excessive distortion on displacement vector field (DVF) caused by hypermetabolic anatomical structures in PET images. A DVF estimator is built based on 3D unsupervised convolutional neural networks and a spatial transformer is used for warping 3D PET images to 3D CT images. The generated registration results (PET image patches) and the original 3D CT image patches are used for calculating the spatial domain similarity (SD similarity) and frequency domain similarity (FD similarity). Finally, the loss function of the entire registration network is constructed by a weighted sum of SD similarity, FD similarity and a smoothness of DVF. A dataset consisted of 170 whole-body PET-CT images is used for registration accuracy evaluation. The proposed unsupervised registration network, 3D MC-MDS Net, can accurately learn the 3D registration model by using the training dataset with the metabolic constraint model, which significantly improves the registration accuracy.",6.0,2020,10.1109/ACCESS.2020.2984804,metabolic constraint
23,Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",8.0,2019,10.1109/ISBI.2019.8759529,mutual information
23,Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",8.0,2019,10.1109/ISBI.2019.8759529,bones
23,Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",8.0,2019,10.1109/ISBI.2019.8759529,adversarial learning
23,Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",8.0,2019,10.1109/ISBI.2019.8759529,unpaired data
23,Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",8.0,2019,10.1109/ISBI.2019.8759529,cross modality
23,Unpaired Mr to CT Synthesis with Explicit Structural Constrained Adversarial Learning,Y. Ge; D. Wei; Z. Xue; Q. Wang; X. Zhou; Y. Zhan; S. Liao,"In medical imaging such as PET-MR attenuation correction and MRI-guided radiation therapy, synthesizing CT images from MR plays an important role in obtaining tissue density properties. Recently deep-learning-based image synthesis techniques have attracted much attention because of their superior ability for image mapping. However, most of the current deep-learning-based synthesis methods require large scales of paired data, which greatly limits their usage. Efforts have been made to relax such a restriction, and the cycle-consistent adversarial networks (Cycle-GAN) is an example to synthesize medical images with unpaired data. In Cycle-GAN, the cycle consistency loss is employed as an indirect structural similarity metric between the input and the synthesized images and often leads to mismatch of anatomical structures in the synthesized results. To overcome this shortcoming, we propose to (1) use the mutual information loss to directly enforce the structural similarity between the input MR and the synthesized CT image and (2) to incorporate the shape consistency information to improve the synthesis result. Experimental results demonstrate that the proposed method can achieve better performance both qualitatively and quantitatively for whole-body MR to CT synthesis with unpaired training images compared to Cycle-GAN.",8.0,2019,10.1109/ISBI.2019.8759529,attenuation
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,biological tissues
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,abdomen
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,bones
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,mr
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,feature extraction
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,pelvis
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,intelligent transformation
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,synthetic ct
24,Transforming UTE-mDixon MR Abdomen-Pelvis Images Into CT by Jointly Leveraging Prior Knowledge and Partial Supervision,P. Qian; J. Zheng; Q. Zheng; Y. Liu; T. Wang; R. Al Helo; A. Baydoun; N. Avril; R. J. Ellis; H. Friel; M. S. Traughber; A. Devaraj; B. Traughber; R. F. Muzic,"Computed tomography (CT) provides information for diagnosis, PET attenuation correction (AC), and radiation treatment planning (RTP). Disadvantages of CT include poor soft tissue contrast and exposure to ionizing radiation. While MRI can overcome these disadvantages, it lacks the photon absorption information needed for PET AC and RTP. Thus, an intelligent transformation from MR to CT, i.e., the MR-based synthetic CT generation, is of great interest as it would support PET/MR AC and MR-only RTP. Using an MR pulse sequence that combines ultra-short echo time (UTE) and modified Dixon (mDixon), we propose a novel method for synthetic CT generation jointly leveraging prior knowledge as well as partial supervision (SCT-PK-PS for short) on large-field-of-view images that span abdomen and pelvis. Two key machine learning techniques, i.e., the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) and the Laplacian support vector machine (LapSVM), are used in SCT-PK-PS. The significance of our effort is threefold: 1) Using the prior knowledge-referenced KL-TFCM clustering, SCT-PK-PS is able to group the feature data of MR images into five initial clusters of fat, soft tissue, air, bone, and bone marrow. Via these initial partitions, clusters needing to be refined are observed and for each of them a few additionally labeled examples are given as the partial supervision for the subsequent semi-supervised classification using LapSVM; 2) Partial supervision is usually insufficient for conventional algorithms to learn the insightful classifier. Instead, exploiting not only the given supervision but also the manifold structure embedded primarily in numerous unlabeled data, LapSVM is capable of training multiple desired tissue-recognizers; 3) Benefiting from the joint use of KL-TFCM and LapSVM, and assisted by the edge detector filter based feature extraction, the proposed SCT-PK-PS method features good recognition accuracy of tissue types, which ultimately facilitates the good transformation from MR images to CT images of the abdomen-pelvis. Applying the method on twenty subjects’ feature data of UTE-mDixon MR images, the average score of the mean absolute prediction deviation (MAPD) of all subjects is 140.72 ± 30.60 HU which is statistically significantly better than the 241.36 ± 21.79 HU obtained using the all-water method, the 262.77 ± 42.22 HU obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method, and the 197.05 ± 76.53 HU obtained via the conventional SVM method. These results demonstrate the effectiveness of our method for the intelligent transformation from MR to CT on the body section of abdomen-pelvis.",5.0,2021,10.1109/TCBB.2020.2979841,fats
25,Robust MR-free Grey Matter Extraction in Amyloid PET/CT Studies with Deep Learning,L. Presotto; C. Bezzi; G. Vanoli; C. Muscio; F. Tagliavini; D. Perani; V. Bettinardi,"Quantification of amyloid PET studies is most accurate if regions of interest (ROIs) are not affected by the presence of cerebrospinal fluid. Patients with high amyloid load often have great atrophy, therefore, the use of atlas-based ROIs, instead of patient specific anatomy, can underestimate amyloid load, leading to a bias. Traditionally, this can be overcome only using MR anatomical sequences, which are burdensome and might not be ideal to be performed for each patient in the clinical routine. In this work, we propose to overcome this issue by using a method based on deep learning. As CT scans provide anatomical information, even at the very low doses used for PET attenuation correction, we propose the use of such a scan, together with the PET one, for a U-NET based segmentation. The approach achieves a median DICE score of 77% on a validation cohort of N=20 patients, even when using only N=14 patients in the training dataset. A dedicated data augmentation strategy is used, and the individual contribution of each modality is analyzed. We find that the joint effect of PET and CT is beneficial (median DICE: PET only 73.0%, CT only 74%). A near perfect correlation with MR-based quantification was also found.",,2020,10.1109/NSS/MIC42677.2020.9507836,correlation
25,Robust MR-free Grey Matter Extraction in Amyloid PET/CT Studies with Deep Learning,L. Presotto; C. Bezzi; G. Vanoli; C. Muscio; F. Tagliavini; D. Perani; V. Bettinardi,"Quantification of amyloid PET studies is most accurate if regions of interest (ROIs) are not affected by the presence of cerebrospinal fluid. Patients with high amyloid load often have great atrophy, therefore, the use of atlas-based ROIs, instead of patient specific anatomy, can underestimate amyloid load, leading to a bias. Traditionally, this can be overcome only using MR anatomical sequences, which are burdensome and might not be ideal to be performed for each patient in the clinical routine. In this work, we propose to overcome this issue by using a method based on deep learning. As CT scans provide anatomical information, even at the very low doses used for PET attenuation correction, we propose the use of such a scan, together with the PET one, for a U-NET based segmentation. The approach achieves a median DICE score of 77% on a validation cohort of N=20 patients, even when using only N=14 patients in the training dataset. A dedicated data augmentation strategy is used, and the individual contribution of each modality is analyzed. We find that the joint effect of PET and CT is beneficial (median DICE: PET only 73.0%, CT only 74%). A near perfect correlation with MR-based quantification was also found.",,2020,10.1109/NSS/MIC42677.2020.9507836,conferences
25,Robust MR-free Grey Matter Extraction in Amyloid PET/CT Studies with Deep Learning,L. Presotto; C. Bezzi; G. Vanoli; C. Muscio; F. Tagliavini; D. Perani; V. Bettinardi,"Quantification of amyloid PET studies is most accurate if regions of interest (ROIs) are not affected by the presence of cerebrospinal fluid. Patients with high amyloid load often have great atrophy, therefore, the use of atlas-based ROIs, instead of patient specific anatomy, can underestimate amyloid load, leading to a bias. Traditionally, this can be overcome only using MR anatomical sequences, which are burdensome and might not be ideal to be performed for each patient in the clinical routine. In this work, we propose to overcome this issue by using a method based on deep learning. As CT scans provide anatomical information, even at the very low doses used for PET attenuation correction, we propose the use of such a scan, together with the PET one, for a U-NET based segmentation. The approach achieves a median DICE score of 77% on a validation cohort of N=20 patients, even when using only N=14 patients in the training dataset. A dedicated data augmentation strategy is used, and the individual contribution of each modality is analyzed. We find that the joint effect of PET and CT is beneficial (median DICE: PET only 73.0%, CT only 74%). A near perfect correlation with MR-based quantification was also found.",,2020,10.1109/NSS/MIC42677.2020.9507836,grey matter
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,biomedical applications of radiation
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,planning
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,u-net
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,gallium nitride
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,attenuation
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,pelvis
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,generative adversarial network
26,Synthetic CT Generation of the Pelvis in Patients With Cervical Cancer: A Single Input Approach Using Generative Adversarial Network,A. Baydoun; K. Xu; J. U. Heo; H. Yang; F. Zhou; L. A. Bethell; E. T. Fredman; R. J. Ellis; T. K. Podder; M. S. Traughber; R. M. Paspulati; P. Qian; B. J. Traughber; R. F. Muzic,"Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of <sup>18</sup>F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks.",1.0,2021,10.1109/ACCESS.2021.3049781,cervical cancer
27,Artificial Intelligence in Radiation Therapy,Y. Fu; H. Zhang; E. D. Morris; C. K. Glide-Hurst; S. Pai; A. Traverso; L. Wee; I. Hadzic; P. -I. L&#x00F8;nne; C. Shen; T. Liu; X. Yang,"Artificial intelligence (AI) has great potential to transform the clinical workflow of radiotherapy. Since the introduction of deep neural networks (DNNs), many AI-based methods have been proposed to address challenges in different aspects of radiotherapy. Commercial vendors have started to release AI-based tools that can be readily integrated to the established clinical workflow. To show the recent progress in AI-aided radiotherapy, we have reviewed AI-based studies in five major aspects of radiotherapy, including image reconstruction, image registration, image segmentation, image synthesis, and automatic treatment planning. In each section, we summarized and categorized the recently published methods, followed by a discussion of the challenges, concerns, and future development. Given the rapid development of AI-aided radiotherapy, the efficiency and effectiveness of radiotherapy in the future could be substantially improved through intelligent automation of various aspects of radiotherapy.",1.0,2022,10.1109/TRPMS.2021.3107454,planning
27,Artificial Intelligence in Radiation Therapy,Y. Fu; H. Zhang; E. D. Morris; C. K. Glide-Hurst; S. Pai; A. Traverso; L. Wee; I. Hadzic; P. -I. L&#x00F8;nne; C. Shen; T. Liu; X. Yang,"Artificial intelligence (AI) has great potential to transform the clinical workflow of radiotherapy. Since the introduction of deep neural networks (DNNs), many AI-based methods have been proposed to address challenges in different aspects of radiotherapy. Commercial vendors have started to release AI-based tools that can be readily integrated to the established clinical workflow. To show the recent progress in AI-aided radiotherapy, we have reviewed AI-based studies in five major aspects of radiotherapy, including image reconstruction, image registration, image segmentation, image synthesis, and automatic treatment planning. In each section, we summarized and categorized the recently published methods, followed by a discussion of the challenges, concerns, and future development. Given the rapid development of AI-aided radiotherapy, the efficiency and effectiveness of radiotherapy in the future could be substantially improved through intelligent automation of various aspects of radiotherapy.",1.0,2022,10.1109/TRPMS.2021.3107454,tumors
27,Artificial Intelligence in Radiation Therapy,Y. Fu; H. Zhang; E. D. Morris; C. K. Glide-Hurst; S. Pai; A. Traverso; L. Wee; I. Hadzic; P. -I. L&#x00F8;nne; C. Shen; T. Liu; X. Yang,"Artificial intelligence (AI) has great potential to transform the clinical workflow of radiotherapy. Since the introduction of deep neural networks (DNNs), many AI-based methods have been proposed to address challenges in different aspects of radiotherapy. Commercial vendors have started to release AI-based tools that can be readily integrated to the established clinical workflow. To show the recent progress in AI-aided radiotherapy, we have reviewed AI-based studies in five major aspects of radiotherapy, including image reconstruction, image registration, image segmentation, image synthesis, and automatic treatment planning. In each section, we summarized and categorized the recently published methods, followed by a discussion of the challenges, concerns, and future development. Given the rapid development of AI-aided radiotherapy, the efficiency and effectiveness of radiotherapy in the future could be substantially improved through intelligent automation of various aspects of radiotherapy.",1.0,2022,10.1109/TRPMS.2021.3107454,radiotherapy
27,Artificial Intelligence in Radiation Therapy,Y. Fu; H. Zhang; E. D. Morris; C. K. Glide-Hurst; S. Pai; A. Traverso; L. Wee; I. Hadzic; P. -I. L&#x00F8;nne; C. Shen; T. Liu; X. Yang,"Artificial intelligence (AI) has great potential to transform the clinical workflow of radiotherapy. Since the introduction of deep neural networks (DNNs), many AI-based methods have been proposed to address challenges in different aspects of radiotherapy. Commercial vendors have started to release AI-based tools that can be readily integrated to the established clinical workflow. To show the recent progress in AI-aided radiotherapy, we have reviewed AI-based studies in five major aspects of radiotherapy, including image reconstruction, image registration, image segmentation, image synthesis, and automatic treatment planning. In each section, we summarized and categorized the recently published methods, followed by a discussion of the challenges, concerns, and future development. Given the rapid development of AI-aided radiotherapy, the efficiency and effectiveness of radiotherapy in the future could be substantially improved through intelligent automation of various aspects of radiotherapy.",1.0,2022,10.1109/TRPMS.2021.3107454,treatment planning
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,smoothing methods
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,maximum a posteriori
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,observer study
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,lesions
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,spatial resolution
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,reconstruction algorithms
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,penalized-likelihood
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,emission tomography
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,detection
28,Performance of MAP Reconstruction for Hot Lesion Detection in Whole-Body PET/CT: An Evaluation With Human and Numerical Observers,J. Nuyts; C. Michel; L. Brepoels; L. D. Ceuninck; C. Deroose; K. Goffin; F. M. Mottaghy; S. Stroobants; J. V. Riet; R. Verscuren,"For positron emission tomography (PET) imaging, different reconstruction methods can be applied, including maximum likelihood (ML ) and maximum <i>a</i> <i>posteriori</i> (MAP) reconstruction. Postsmoothed ML images have approximately position and object independent spatial resolution, which is advantageous for (semi-) quantitative analysis. However, the complex object dependent smoothing obtained with MAP might yield improved noise characteristics, beneficial for lesion detection. In this contribution, MAP and postsmoothed ML are compared for hot spot detection by human observers and by the channelized Hotelling observer (CHO). The study design was based on the ldquomultiple alternative forced choicerdquo approach. For the MAP reconstruction, the relative difference prior was used. For postsmoothed ML, a Gaussian smoothing kernel was used. Both the human observers and the CHO performed slightly better on MAP images than on postsmoothed ML images. The average CHO performance was similar to the best human performance. The CHO was then applied to evaluate the performance of priors with reduced penalty for large differences. For these priors, a poorer detection performance was obtained.",20.0,2009,10.1109/TMI.2008.927349,maximum likelihood detection
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,deconvolution
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,noise reduction
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,two dimensional displays
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,three-dimensional displays
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,transfer learning
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,convolution
29,Parameter-Transferred Wasserstein Generative Adversarial Network (PT-WGAN) for Low-Dose PET Image Denoising,Y. Gong; H. Shan; Y. Teng; N. Tu; M. Li; G. Liang; G. Wang; S. Wang,"Due to the widespread of positron emission tomography (PET) in clinical practice, the potential risk of PET-associated radiation dose to patients needs to be minimized. However, with the reduction in the radiation dose, the resultant images may suffer from noise and artifacts that compromise diagnostic performance. In this article, we propose a parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising. The contributions of this article are twofold: 1) a PT-WGAN framework is designed to denoise low-dose PET images without compromising structural details and 2) a task-specific initialization based on transfer learning is developed to train PT-WGAN using trainable parameters transferred from a pretrained model, which significantly improves the training efficiency of PT-WGAN. The experimental results on clinical data show that the proposed network can suppress image noise more effectively while preserving better image fidelity than recently published state-of-the-art methods. We make our code available at <uri>https://github.com/90n9-yu/PT-WGAN</uri>.",9.0,2021,10.1109/TRPMS.2020.3025071,task-specific initialization
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,lymphoma segmentation
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,random forest
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,multimodality
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,lesions
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,feature extraction
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,mathematical morphology
30,Automated 3D lymphoma lesion segmentation from PET/CT characteristics,É. Grossiord; H. Talbot; N. Passat; M. Meignan; L. Najman,"Positron Emission Tomography (PET) using <sup>18</sup>F-FDG is recognized as the modality of choice for lymphoma, due to its high sensitivity and specificity. Its wider use for the detection of lesions, quantification of their metabolic activity and evaluation of response to treatment demands the development of accurate and reproducible quantitative image interpretation tools. An accurate tumour delineation remains a challenge in PET, due to the limitations the modality suffers from, despite being essential for quantifying reliable changes in tumour tissues. Due to the spatial and spectral properties of PET images, most methods rely on intensity-based strategies. Recent methods also propose to integrate anatomical priors to improve the segmentation process. However, the current routinely-used approach remains a local relative thresholding and requires important user interaction, leading to a process that is not only user-dependent but very laborious in the case of lymphomas. In this paper, we propose to rely on hierarchical image models embedding multimodality PET/CT descriptors for a fully automated PET lesion detection / segmentation, performed via a machine learning process. More precisely, we propose to perform random forest classification within the mixed spatial-spectral space of component-trees modeling PET/CT mages. This new approach, combining the strengths of machine learning and morphological hierarchy models leads to intelligent thresholding based on high-level PET/CT knowledge. We evaluate our approach on a database of multi-centric PET/CT images of patients treated for lymphoma, delineated by an expert. Our method provides good efficiency, with the detection of 92% of all lesions, and accurate segmentation results with mean sensitivity and specificity of 0.73 and 0.99 respectively, without any user interaction.",16.0,2017,10.1109/ISBI.2017.7950495,component-tree
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,bandwidth
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,application software
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,surgery
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,multi-volume rendering
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,rendering (computer graphics)
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,fuzzy $c$-means cluster analysis
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,interactive three-dimensional (3-d) segmentation
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,table lookup
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,real-time volume rendering
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,computer aided diagnosis
31,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,J. Kim; W. Cai; S. Eberl; D. Feng,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",28.0,2007,10.1109/TITB.2006.875669,visualization
32,Deep Learning-based Automated Delineation of Head and Neck Malignant Lesions from PET Images,H. Arabi; I. Shiri; E. Jenabi; M. Becker; H. Zaidi,"Accurate delineation of the gross tumor volume (GTV) is critical for treatment planning in radiation oncology. This task is very challenging owing to the irregular and diverse shapes of malignant lesions. Manual delineation of the GTVs on PET images is not only time-consuming but also suffers from inter- and intra-observer variability. In this work, we developed deep learning-based approaches for automated GTV delineation on PET images of head and neck cancer patients. To this end, V-Net, a fully convolutional neural network for volumetric medical image segmentation, and HighResNet, a 20-layer residual convolutional neural network, were adopted. <sup>18</sup>F-FDG-PET/CT images of 510 patients presenting with head and neck cancer on which manually defined (reference) GTVs were utilized for training, evaluation and testing of these algorithms. The input of these networks (in both training or evaluation phases) were 12×12×12 cm sub-volumes of PET images containing the whole volume of the tumors and the neighboring background radiotracer uptake. These networks were trained to generate a binary mask representing the GTV on the input PET subvolume. Standard segmentation metrics, including Dice similarity and precision were used for performance assessment of these algorithms. HighResNet achieved automated GTV delineation with a Dice index of 0.87±0.04 compared to 0.86±0.06 achieved by V-Net. Despite the close performance of these two approaches, HighResNet exhibited less variability among different subjects as reflected in the smaller standard deviation and significantly higher precision index (0.87±0.07 versus 0.80±0.10). Deep learning techniques, in particular HighResNet algorithm, exhibited promising performance for automated GTV delineation on head and neck PET images. Incorporation of anatomical/structural information, particularly MRI, may result in higher segmentation accuracy or less variability among the different subjects.",,2020,10.1109/NSS/MIC42677.2020.9507977,lesions
32,Deep Learning-based Automated Delineation of Head and Neck Malignant Lesions from PET Images,H. Arabi; I. Shiri; E. Jenabi; M. Becker; H. Zaidi,"Accurate delineation of the gross tumor volume (GTV) is critical for treatment planning in radiation oncology. This task is very challenging owing to the irregular and diverse shapes of malignant lesions. Manual delineation of the GTVs on PET images is not only time-consuming but also suffers from inter- and intra-observer variability. In this work, we developed deep learning-based approaches for automated GTV delineation on PET images of head and neck cancer patients. To this end, V-Net, a fully convolutional neural network for volumetric medical image segmentation, and HighResNet, a 20-layer residual convolutional neural network, were adopted. <sup>18</sup>F-FDG-PET/CT images of 510 patients presenting with head and neck cancer on which manually defined (reference) GTVs were utilized for training, evaluation and testing of these algorithms. The input of these networks (in both training or evaluation phases) were 12×12×12 cm sub-volumes of PET images containing the whole volume of the tumors and the neighboring background radiotracer uptake. These networks were trained to generate a binary mask representing the GTV on the input PET subvolume. Standard segmentation metrics, including Dice similarity and precision were used for performance assessment of these algorithms. HighResNet achieved automated GTV delineation with a Dice index of 0.87±0.04 compared to 0.86±0.06 achieved by V-Net. Despite the close performance of these two approaches, HighResNet exhibited less variability among different subjects as reflected in the smaller standard deviation and significantly higher precision index (0.87±0.07 versus 0.80±0.10). Deep learning techniques, in particular HighResNet algorithm, exhibited promising performance for automated GTV delineation on head and neck PET images. Incorporation of anatomical/structural information, particularly MRI, may result in higher segmentation accuracy or less variability among the different subjects.",,2020,10.1109/NSS/MIC42677.2020.9507977,head
32,Deep Learning-based Automated Delineation of Head and Neck Malignant Lesions from PET Images,H. Arabi; I. Shiri; E. Jenabi; M. Becker; H. Zaidi,"Accurate delineation of the gross tumor volume (GTV) is critical for treatment planning in radiation oncology. This task is very challenging owing to the irregular and diverse shapes of malignant lesions. Manual delineation of the GTVs on PET images is not only time-consuming but also suffers from inter- and intra-observer variability. In this work, we developed deep learning-based approaches for automated GTV delineation on PET images of head and neck cancer patients. To this end, V-Net, a fully convolutional neural network for volumetric medical image segmentation, and HighResNet, a 20-layer residual convolutional neural network, were adopted. <sup>18</sup>F-FDG-PET/CT images of 510 patients presenting with head and neck cancer on which manually defined (reference) GTVs were utilized for training, evaluation and testing of these algorithms. The input of these networks (in both training or evaluation phases) were 12×12×12 cm sub-volumes of PET images containing the whole volume of the tumors and the neighboring background radiotracer uptake. These networks were trained to generate a binary mask representing the GTV on the input PET subvolume. Standard segmentation metrics, including Dice similarity and precision were used for performance assessment of these algorithms. HighResNet achieved automated GTV delineation with a Dice index of 0.87±0.04 compared to 0.86±0.06 achieved by V-Net. Despite the close performance of these two approaches, HighResNet exhibited less variability among different subjects as reflected in the smaller standard deviation and significantly higher precision index (0.87±0.07 versus 0.80±0.10). Deep learning techniques, in particular HighResNet algorithm, exhibited promising performance for automated GTV delineation on head and neck PET images. Incorporation of anatomical/structural information, particularly MRI, may result in higher segmentation accuracy or less variability among the different subjects.",,2020,10.1109/NSS/MIC42677.2020.9507977,head and neck cancer
32,Deep Learning-based Automated Delineation of Head and Neck Malignant Lesions from PET Images,H. Arabi; I. Shiri; E. Jenabi; M. Becker; H. Zaidi,"Accurate delineation of the gross tumor volume (GTV) is critical for treatment planning in radiation oncology. This task is very challenging owing to the irregular and diverse shapes of malignant lesions. Manual delineation of the GTVs on PET images is not only time-consuming but also suffers from inter- and intra-observer variability. In this work, we developed deep learning-based approaches for automated GTV delineation on PET images of head and neck cancer patients. To this end, V-Net, a fully convolutional neural network for volumetric medical image segmentation, and HighResNet, a 20-layer residual convolutional neural network, were adopted. <sup>18</sup>F-FDG-PET/CT images of 510 patients presenting with head and neck cancer on which manually defined (reference) GTVs were utilized for training, evaluation and testing of these algorithms. The input of these networks (in both training or evaluation phases) were 12×12×12 cm sub-volumes of PET images containing the whole volume of the tumors and the neighboring background radiotracer uptake. These networks were trained to generate a binary mask representing the GTV on the input PET subvolume. Standard segmentation metrics, including Dice similarity and precision were used for performance assessment of these algorithms. HighResNet achieved automated GTV delineation with a Dice index of 0.87±0.04 compared to 0.86±0.06 achieved by V-Net. Despite the close performance of these two approaches, HighResNet exhibited less variability among different subjects as reflected in the smaller standard deviation and significantly higher precision index (0.87±0.07 versus 0.80±0.10). Deep learning techniques, in particular HighResNet algorithm, exhibited promising performance for automated GTV delineation on head and neck PET images. Incorporation of anatomical/structural information, particularly MRI, may result in higher segmentation accuracy or less variability among the different subjects.",,2020,10.1109/NSS/MIC42677.2020.9507977,segmentation
32,Deep Learning-based Automated Delineation of Head and Neck Malignant Lesions from PET Images,H. Arabi; I. Shiri; E. Jenabi; M. Becker; H. Zaidi,"Accurate delineation of the gross tumor volume (GTV) is critical for treatment planning in radiation oncology. This task is very challenging owing to the irregular and diverse shapes of malignant lesions. Manual delineation of the GTVs on PET images is not only time-consuming but also suffers from inter- and intra-observer variability. In this work, we developed deep learning-based approaches for automated GTV delineation on PET images of head and neck cancer patients. To this end, V-Net, a fully convolutional neural network for volumetric medical image segmentation, and HighResNet, a 20-layer residual convolutional neural network, were adopted. <sup>18</sup>F-FDG-PET/CT images of 510 patients presenting with head and neck cancer on which manually defined (reference) GTVs were utilized for training, evaluation and testing of these algorithms. The input of these networks (in both training or evaluation phases) were 12×12×12 cm sub-volumes of PET images containing the whole volume of the tumors and the neighboring background radiotracer uptake. These networks were trained to generate a binary mask representing the GTV on the input PET subvolume. Standard segmentation metrics, including Dice similarity and precision were used for performance assessment of these algorithms. HighResNet achieved automated GTV delineation with a Dice index of 0.87±0.04 compared to 0.86±0.06 achieved by V-Net. Despite the close performance of these two approaches, HighResNet exhibited less variability among different subjects as reflected in the smaller standard deviation and significantly higher precision index (0.87±0.07 versus 0.80±0.10). Deep learning techniques, in particular HighResNet algorithm, exhibited promising performance for automated GTV delineation on head and neck PET images. Incorporation of anatomical/structural information, particularly MRI, may result in higher segmentation accuracy or less variability among the different subjects.",,2020,10.1109/NSS/MIC42677.2020.9507977,neck
33,Brain PET Attenuation Correction without CT: An Investigation,M. Dewan; Y. Zhan; G. Hermosillo; B. Jian; X. S. Zhou,"In the last decade, Brain PET Imaging has taken big strides in becoming an effective diagnostic tool for dementia and epilepsy disorders, particularly Alzheimer's. CT is often used to provide information for PET attenuation correction. However, for dementia patients, which often require multiple follow-ups, the elimination of CT is desirable to reduce the radiation dose. In this paper, we present a robust algorithm for PET attenuation correction without CT. The algorithm involves building a database of non-attenuation corrected (NAC) PET and CT pairs (model scans). Given a new patient's NAC PET, a learning-based algorithm is used to detect key landmarks, which are then used to select the most similar model scans. Deformable registration is then employed to warp the model CTs to the subject space, followed by a fusion step to obtain the virtual CT for attenuation correction. Besides comparing the normalized AC values with ground truth, we also use a diagnostic tool to evaluate the solution. In addition, a diagnostic evaluation is conducted by a trained nuclear medicine physician, all with promising results.",,2013,10.1109/PRNI.2013.37,attenuation correction
33,Brain PET Attenuation Correction without CT: An Investigation,M. Dewan; Y. Zhan; G. Hermosillo; B. Jian; X. S. Zhou,"In the last decade, Brain PET Imaging has taken big strides in becoming an effective diagnostic tool for dementia and epilepsy disorders, particularly Alzheimer's. CT is often used to provide information for PET attenuation correction. However, for dementia patients, which often require multiple follow-ups, the elimination of CT is desirable to reduce the radiation dose. In this paper, we present a robust algorithm for PET attenuation correction without CT. The algorithm involves building a database of non-attenuation corrected (NAC) PET and CT pairs (model scans). Given a new patient's NAC PET, a learning-based algorithm is used to detect key landmarks, which are then used to select the most similar model scans. Deformable registration is then employed to warp the model CTs to the subject space, followed by a fusion step to obtain the virtual CT for attenuation correction. Besides comparing the normalized AC values with ground truth, we also use a diagnostic tool to evaluate the solution. In addition, a diagnostic evaluation is conducted by a trained nuclear medicine physician, all with promising results.",,2013,10.1109/PRNI.2013.37,dementia
33,Brain PET Attenuation Correction without CT: An Investigation,M. Dewan; Y. Zhan; G. Hermosillo; B. Jian; X. S. Zhou,"In the last decade, Brain PET Imaging has taken big strides in becoming an effective diagnostic tool for dementia and epilepsy disorders, particularly Alzheimer's. CT is often used to provide information for PET attenuation correction. However, for dementia patients, which often require multiple follow-ups, the elimination of CT is desirable to reduce the radiation dose. In this paper, we present a robust algorithm for PET attenuation correction without CT. The algorithm involves building a database of non-attenuation corrected (NAC) PET and CT pairs (model scans). Given a new patient's NAC PET, a learning-based algorithm is used to detect key landmarks, which are then used to select the most similar model scans. Deformable registration is then employed to warp the model CTs to the subject space, followed by a fusion step to obtain the virtual CT for attenuation correction. Besides comparing the normalized AC values with ground truth, we also use a diagnostic tool to evaluate the solution. In addition, a diagnostic evaluation is conducted by a trained nuclear medicine physician, all with promising results.",,2013,10.1109/PRNI.2013.37,brain modeling
33,Brain PET Attenuation Correction without CT: An Investigation,M. Dewan; Y. Zhan; G. Hermosillo; B. Jian; X. S. Zhou,"In the last decade, Brain PET Imaging has taken big strides in becoming an effective diagnostic tool for dementia and epilepsy disorders, particularly Alzheimer's. CT is often used to provide information for PET attenuation correction. However, for dementia patients, which often require multiple follow-ups, the elimination of CT is desirable to reduce the radiation dose. In this paper, we present a robust algorithm for PET attenuation correction without CT. The algorithm involves building a database of non-attenuation corrected (NAC) PET and CT pairs (model scans). Given a new patient's NAC PET, a learning-based algorithm is used to detect key landmarks, which are then used to select the most similar model scans. Deformable registration is then employed to warp the model CTs to the subject space, followed by a fusion step to obtain the virtual CT for attenuation correction. Besides comparing the normalized AC values with ground truth, we also use a diagnostic tool to evaluate the solution. In addition, a diagnostic evaluation is conducted by a trained nuclear medicine physician, all with promising results.",,2013,10.1109/PRNI.2013.37,attenuation
33,Brain PET Attenuation Correction without CT: An Investigation,M. Dewan; Y. Zhan; G. Hermosillo; B. Jian; X. S. Zhou,"In the last decade, Brain PET Imaging has taken big strides in becoming an effective diagnostic tool for dementia and epilepsy disorders, particularly Alzheimer's. CT is often used to provide information for PET attenuation correction. However, for dementia patients, which often require multiple follow-ups, the elimination of CT is desirable to reduce the radiation dose. In this paper, we present a robust algorithm for PET attenuation correction without CT. The algorithm involves building a database of non-attenuation corrected (NAC) PET and CT pairs (model scans). Given a new patient's NAC PET, a learning-based algorithm is used to detect key landmarks, which are then used to select the most similar model scans. Deformable registration is then employed to warp the model CTs to the subject space, followed by a fusion step to obtain the virtual CT for attenuation correction. Besides comparing the normalized AC values with ground truth, we also use a diagnostic tool to evaluate the solution. In addition, a diagnostic evaluation is conducted by a trained nuclear medicine physician, all with promising results.",,2013,10.1109/PRNI.2013.37,deformable models
34,The Role of Imaging in the Detection and Management of COVID-19: A Review,D. Dong; Z. Tang; S. Wang; H. Hui; L. Gong; Y. Lu; Z. Xue; H. Liao; F. Chen; F. Yang; R. Jin; K. Wang; Z. Liu; J. Wei; W. Mu; H. Zhang; J. Jiang; J. Tian; H. Li,"Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.",99.0,2021,10.1109/RBME.2020.2990959,covid-19
34,The Role of Imaging in the Detection and Management of COVID-19: A Review,D. Dong; Z. Tang; S. Wang; H. Hui; L. Gong; Y. Lu; Z. Xue; H. Liao; F. Chen; F. Yang; R. Jin; K. Wang; Z. Liu; J. Wei; W. Mu; H. Zhang; J. Jiang; J. Tian; H. Li,"Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.",99.0,2021,10.1109/RBME.2020.2990959,chest ct
34,The Role of Imaging in the Detection and Management of COVID-19: A Review,D. Dong; Z. Tang; S. Wang; H. Hui; L. Gong; Y. Lu; Z. Xue; H. Liao; F. Chen; F. Yang; R. Jin; K. Wang; Z. Liu; J. Wei; W. Mu; H. Zhang; J. Jiang; J. Tian; H. Li,"Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.",99.0,2021,10.1109/RBME.2020.2990959,hospitals
34,The Role of Imaging in the Detection and Management of COVID-19: A Review,D. Dong; Z. Tang; S. Wang; H. Hui; L. Gong; Y. Lu; Z. Xue; H. Liao; F. Chen; F. Yang; R. Jin; K. Wang; Z. Liu; J. Wei; W. Mu; H. Zhang; J. Jiang; J. Tian; H. Li,"Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.",99.0,2021,10.1109/RBME.2020.2990959,diseases
34,The Role of Imaging in the Detection and Management of COVID-19: A Review,D. Dong; Z. Tang; S. Wang; H. Hui; L. Gong; Y. Lu; Z. Xue; H. Liao; F. Chen; F. Yang; R. Jin; K. Wang; Z. Liu; J. Wei; W. Mu; H. Zhang; J. Jiang; J. Tian; H. Li,"Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.",99.0,2021,10.1109/RBME.2020.2990959,lung
35,Automating Lung Cancer Identification in PET/CT Imaging,E. D'Arnese; E. Del Sozzo; A. Chiti; T. Berger-Wolf; M. D. Santambrogio,"Early and accurate diagnosis of lung cancer is one of the most investigated open challenges in the last decades. The diagnosis for this cancer type is usually lethal if not detected in early stages. For these reasons it is clear the need of creating an automated diagnostic tool that requires less time for the identification and does not require a cross-validation of the results by different radiologist, being in this way cheaper and less error prone. The aim of this work is to implement a completely automated pipeline that starting from the current imaging technologies, such as Computed Tomography (CT) and Positron Emission Tomography (PET), will identify lung cancer to be employed for the staging; moreover, it will be a suitable starting point for a machine learning based classification procedure. In particular, this project proposes both a methodology and the related software tool that taking as input Digital Imaging and COmmunications in Medicine (DICOM®) files of chest PET and CT and by exploiting the characteristics of both of them is capable of automatically identify the lungs and the eventually presence of tumor lesions. A validation of the image processing pipeline has been done by computing the execution time and the reached accuracy. The obtained accuracy varies between 89-97% on the analyzed dataset with a significant reduction of the analysis time.",,2018,10.1109/RTSI.2018.8548388,lung cancer
35,Automating Lung Cancer Identification in PET/CT Imaging,E. D'Arnese; E. Del Sozzo; A. Chiti; T. Berger-Wolf; M. D. Santambrogio,"Early and accurate diagnosis of lung cancer is one of the most investigated open challenges in the last decades. The diagnosis for this cancer type is usually lethal if not detected in early stages. For these reasons it is clear the need of creating an automated diagnostic tool that requires less time for the identification and does not require a cross-validation of the results by different radiologist, being in this way cheaper and less error prone. The aim of this work is to implement a completely automated pipeline that starting from the current imaging technologies, such as Computed Tomography (CT) and Positron Emission Tomography (PET), will identify lung cancer to be employed for the staging; moreover, it will be a suitable starting point for a machine learning based classification procedure. In particular, this project proposes both a methodology and the related software tool that taking as input Digital Imaging and COmmunications in Medicine (DICOM®) files of chest PET and CT and by exploiting the characteristics of both of them is capable of automatically identify the lungs and the eventually presence of tumor lesions. A validation of the image processing pipeline has been done by computing the execution time and the reached accuracy. The obtained accuracy varies between 89-97% on the analyzed dataset with a significant reduction of the analysis time.",,2018,10.1109/RTSI.2018.8548388,tumors
35,Automating Lung Cancer Identification in PET/CT Imaging,E. D'Arnese; E. Del Sozzo; A. Chiti; T. Berger-Wolf; M. D. Santambrogio,"Early and accurate diagnosis of lung cancer is one of the most investigated open challenges in the last decades. The diagnosis for this cancer type is usually lethal if not detected in early stages. For these reasons it is clear the need of creating an automated diagnostic tool that requires less time for the identification and does not require a cross-validation of the results by different radiologist, being in this way cheaper and less error prone. The aim of this work is to implement a completely automated pipeline that starting from the current imaging technologies, such as Computed Tomography (CT) and Positron Emission Tomography (PET), will identify lung cancer to be employed for the staging; moreover, it will be a suitable starting point for a machine learning based classification procedure. In particular, this project proposes both a methodology and the related software tool that taking as input Digital Imaging and COmmunications in Medicine (DICOM®) files of chest PET and CT and by exploiting the characteristics of both of them is capable of automatically identify the lungs and the eventually presence of tumor lesions. A validation of the image processing pipeline has been done by computing the execution time and the reached accuracy. The obtained accuracy varies between 89-97% on the analyzed dataset with a significant reduction of the analysis time.",,2018,10.1109/RTSI.2018.8548388,automatic segmentation
35,Automating Lung Cancer Identification in PET/CT Imaging,E. D'Arnese; E. Del Sozzo; A. Chiti; T. Berger-Wolf; M. D. Santambrogio,"Early and accurate diagnosis of lung cancer is one of the most investigated open challenges in the last decades. The diagnosis for this cancer type is usually lethal if not detected in early stages. For these reasons it is clear the need of creating an automated diagnostic tool that requires less time for the identification and does not require a cross-validation of the results by different radiologist, being in this way cheaper and less error prone. The aim of this work is to implement a completely automated pipeline that starting from the current imaging technologies, such as Computed Tomography (CT) and Positron Emission Tomography (PET), will identify lung cancer to be employed for the staging; moreover, it will be a suitable starting point for a machine learning based classification procedure. In particular, this project proposes both a methodology and the related software tool that taking as input Digital Imaging and COmmunications in Medicine (DICOM®) files of chest PET and CT and by exploiting the characteristics of both of them is capable of automatically identify the lungs and the eventually presence of tumor lesions. A validation of the image processing pipeline has been done by computing the execution time and the reached accuracy. The obtained accuracy varies between 89-97% on the analyzed dataset with a significant reduction of the analysis time.",,2018,10.1109/RTSI.2018.8548388,cancer
35,Automating Lung Cancer Identification in PET/CT Imaging,E. D'Arnese; E. Del Sozzo; A. Chiti; T. Berger-Wolf; M. D. Santambrogio,"Early and accurate diagnosis of lung cancer is one of the most investigated open challenges in the last decades. The diagnosis for this cancer type is usually lethal if not detected in early stages. For these reasons it is clear the need of creating an automated diagnostic tool that requires less time for the identification and does not require a cross-validation of the results by different radiologist, being in this way cheaper and less error prone. The aim of this work is to implement a completely automated pipeline that starting from the current imaging technologies, such as Computed Tomography (CT) and Positron Emission Tomography (PET), will identify lung cancer to be employed for the staging; moreover, it will be a suitable starting point for a machine learning based classification procedure. In particular, this project proposes both a methodology and the related software tool that taking as input Digital Imaging and COmmunications in Medicine (DICOM®) files of chest PET and CT and by exploiting the characteristics of both of them is capable of automatically identify the lungs and the eventually presence of tumor lesions. A validation of the image processing pipeline has been done by computing the execution time and the reached accuracy. The obtained accuracy varies between 89-97% on the analyzed dataset with a significant reduction of the analysis time.",,2018,10.1109/RTSI.2018.8548388,lung
36,Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",11.0,2006,10.1109/IEMBS.2006.259236,quaternions
36,Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",11.0,2006,10.1109/IEMBS.2006.259236,neoplasms
36,Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",11.0,2006,10.1109/IEMBS.2006.259236,interpolation
36,Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",11.0,2006,10.1109/IEMBS.2006.259236,radiology
36,Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",11.0,2006,10.1109/IEMBS.2006.259236,biomedical engineering
36,Automated 3D Elastic Registration for Improving Tumor Localization in Whole-body PET-CT from Combined Scanner,V. Walimbe; O. Dandekar; F. Mahmoud; R. Shekhar,"Combined PET/CT scanners provide the ability to produce matching metabolic (from PET) and anatomic (from CT) information in a single examination. However, misalignments continue to exist in tumor localization in PET and CT images acquired using these scanners, due to their inability to compensate for nonrigid misalignment resulting from patient breathing and involuntary movement. We demonstrate that our automatic image subdivision-based elastic registration algorithm can correct this misalignment. In a quantitative validation involving 13 expert-identified tumor nodules in six PET-CT image pairs, the algorithm demonstrated statistically significant improvement over the scanner-defined localization. The accuracy of algorithm-determined localization was evaluated to be comparable to average manually defined localization. The results indicate the potential of using our registration algorithm for applications like radiotherapy treatment planning and treatment-monitoring involving combined PET/CT scanners",11.0,2006,10.1109/IEMBS.2006.259236,usa councils
37,Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1.0,2021,10.1109/TMI.2021.3082578,motion correction
37,Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1.0,2021,10.1109/TMI.2021.3082578,dynamics
37,Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1.0,2021,10.1109/TMI.2021.3082578,motion detection
37,Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1.0,2021,10.1109/TMI.2021.3082578,myocardium
37,Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1.0,2021,10.1109/TMI.2021.3082578,blood
37,Automatic Inter-Frame Patient Motion Correction for Dynamic Cardiac PET Using Deep Learning,L. Shi; Y. Lu; N. Dvornek; C. A. Weyman; E. J. Miller; A. J. Sinusas; C. Liu,"Patient motion during dynamic PET imaging can induce errors in myocardial blood flow (MBF) estimation. Motion correction for dynamic cardiac PET is challenging because the rapid tracer kinetics of 82Rb leads to substantial tracer distribution change across different dynamic frames over time, which can cause difficulties for image registration-based motion correction, particularly for early dynamic frames. In this paper, we developed an automatic deep learning-based motion correction (DeepMC) method for dynamic cardiac PET. In this study we focused on the detection and correction of inter-frame rigid translational motion caused by voluntary body movement and pattern change of respiratory motion. A bidirectional-3D LSTM network was developed to fully utilize both local and nonlocal temporal information in the 4D dynamic image data for motion detection. The network was trained and evaluated over motion-free patient scans with simulated motion so that the motion ground-truths are available, where one million samples based on 65 patient scans were used in training, and 600 samples based on 20 patient scans were used in evaluation. The proposed method was also evaluated using additional 10 patient datasets with real motion. We demonstrated that the proposed DeepMC obtained superior performance compared to conventional registration-based methods and other convolutional neural networks (CNN), in terms of motion estimation and MBF quantification accuracy. Once trained, DeepMC is much faster than the registration-based methods and can be easily integrated into the clinical workflow. In the future work, additional investigation is needed to evaluate this approach in a clinical context with realistic patient motion.",1.0,2021,10.1109/TMI.2021.3082578,myocardial perfusion
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,licenses
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,peri-tumor
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,data models
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,quantization (signal)
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,head&neck cancer
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,prognosis
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,radiomics
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,feature extraction
38,Complementary Value of Intra- and Peri-Tumoral PET/CT Radiomics for Outcome Prediction in Head and Neck Cancer,W. Lv; H. Feng; D. Du; J. Ma; L. Lu,"To investigate the prognostic value of peri-tumoral radiomics features of pre-treatment PET/CT images in patients with head and neck cancer. 166 patients from 4 centers (111 for training and 55 for external independent testing) were retrospectively analyzed. 11 regions were used for feature extraction, (1) Intra-tumoral region (Intra) was first dilated radially along the edge by 3, 6, 9, 12 and 15 mm to obtain (2) 5 solid combined regions (noted as Comb_3, 6, 9, 12, and 15, respectively), and (3) 5 hollow annular regions with equal ring width of 3 mm were then generated as peri-tumoral regions (noted as Peri_3, 6, 9, 12 and 15, respectively). 92 individual/integrated models were constructed by using features from Clinical alone, CT or PET alone, Clinical+PET, Clinical+CT, PET+CT, Clinical+PET+CT, Intra+Peri, Clinical+Intra+Peri and Clinical+PET+CT (Intra+Peri). In individual models, only 4 models showed p<; 0.05 (PET_Peri_3 and PET_Comb_6 for distant metastasis (DM) prediction, Clinical and PET_Peri_6 for death prediction). In integrated models, Clinical+CT (Intra+Peri_6), PET (Intra+Peri_3) and Clinical+PET_Peri_6 achieved the best performance for the prediction of local recurrence (LR), DM and death with AUC of 0.75, 0.80 and 0.87, C-index of 0.71, 0.80 and 0.83, p-value of 0.003, 0.008 and 0.001, respectively. Peri-tumoral regions that located closer to the intra-tumoral region (Peri_3 and Peri_6) showed better performance compared to those located further. The integration of intra-tumoral and peri-tumoral radiomics features achieved better performance than either of them alone, PET and CT radiomics features also provided complementary information to clinical features.",1.0,2021,10.1109/ACCESS.2021.3085601,predictive models
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,attenuation correction
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,mr-based attenuation map
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,conditional gan
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,generators
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,bones
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,head
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,gallium nitride
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,generative adversarial networks
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,attenuation
39,Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR,L. Tao; X. Li; J. Fisher; C. S. Levin,"Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.",1.0,2018,10.1109/NSSMIC.2018.8824444,generative adversarial network (gan)
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,biological tissues
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,synthetic ct generation
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,abdomen
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,bones
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,transfer fuzzy clustering (tfc)
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,dixon-based mr
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,attenuation
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,pelvis
40,mDixon-Based Synthetic CT Generation for PET Attenuation Correction on Abdomen and Pelvis Jointly Using Transfer Fuzzy Clustering and Active Learning-Based Classification,P. Qian; Y. Chen; J. -W. Kuo; Y. -D. Zhang; Y. Jiang; K. Zhao; R. Al Helo; H. Friel; A. Baydoun; F. Zhou; J. U. Heo; N. Avril; K. Herrmann; R. Ellis; B. Traughber; R. S. Jones; S. Wang; K. -H. Su; R. F. Muzic,"We propose a new method for generating synthetic CT images from modified Dixon (mDixon) MR data. The synthetic CT is used for attenuation correction (AC) when reconstructing PET data on abdomen and pelvis. While MR does not intrinsically contain any information about photon attenuation, AC is needed in PET/MR systems in order to be quantitatively accurate and to meet qualification standards required for use in many multi-center trials. Existing MR-based synthetic CT generation methods either use advanced MR sequences that have long acquisition time and limited clinical availability or use matching of the MR images from a newly scanned subject to images in a library of MR-CT pairs which has difficulty in accounting for the diversity of human anatomy especially in patients that have pathologies. To address these deficiencies, we present a five-phase interlinked method that uses mDixon MR acquisition and advanced machine learning methods for synthetic CT generation. Both transfer fuzzy clustering and active learning-based classification (TFC-ALC) are used. The significance of our efforts is fourfold: 1) TFC-ALC is capable of better synthetic CT generation than methods currently in use on the challenging abdomen using only common Dixon-based scanning. 2) TFC partitions MR voxels initially into the four groups regarding fat, bone, air, and soft tissue via transfer learning; ALC can learn insightful classifiers, using as few but informative labeled examples as possible to precisely distinguish bone, air, and soft tissue. Combining them, the TFC-ALC method successfully overcomes the inherent imperfection and potential uncertainty regarding the co-registration between CT and MR images. 3) Compared with existing methods, TFC-ALC features not only preferable synthetic CT generation but also improved parameter robustness, which facilitates its clinical practicability. Applying the proposed approach on mDixon-MR data from ten subjects, the average score of the mean absolute prediction deviation (MAPD) was 89.78±8.76 which is significantly better than the 133.17±9.67 obtained using the all-water (AW) method (p=4.11E-9) and the 104.97±10.03 obtained using the four-cluster-partitioning (FCP, i.e., external-air, internal-air, fat, and soft tissue) method (p=0.002). 4) Experiments in the PET SUV errors of these approaches show that TFC-ALC achieves the highest SUV accuracy and can generally reduce the SUV errors to 5% or less. These experimental results distinctively demonstrate the effectiveness of our proposed TFCALC method for the synthetic CT generation on abdomen and pelvis using only the commonly-available Dixon pulse sequence.",35.0,2020,10.1109/TMI.2019.2935916,attenuation correction (ac)
41,Deep learning-guided attenuation and scatter correction without using anatomical images in brain PET/MRI,K. Bortolin; H. Arabi; H. Zaidi,"Attenuation correction (AC) is essential component for quantitative PET imaging. However, in PET/MR imaging and dedicated brain PET devices, the attenuation map either suffers from a number of limitations or is not readily available in the absence of CT or transmission scan. To tackle this issue, a deep convolutional neural networks is proposed to perform joint attenuation and scatter correction in the image domain on the non-attenuation corrected PET images (PET-nonAC). The deep con-volutional neural network used in this work benefits from dilated convolutions and residual connections to establish an end-to-end PET attenuation correction (PET-DirAC). For the training phase, data of 30 patients who underwent brain <sup>18</sup>F-FDG PET/CT scans were used to generate reference PET-CTAC and PET-nonAC images. A five-fold cross-validation scheme was used for training/evaluation of the proposed algorithm. The quantitative accuracy of the proposed method was evaluated against the commercial segmentation-based method (2-class AC map referred to as MRAC). For quantitative analysis, tracer uptake estimated from PET-DirAC and PET-MRAC was compared to PET-CTAC. The relative SUV bias was calculated for bone, soft-tissue, air cavities and the entire head, separately. The proposed approach resulted in a mean relative absolute error (MRAE) of 4.1±7.5% and 5.8±10.4% for the entire head and bone regions, respectively. Conversely, MRAC led to a MRAE of 8.1±10.2% and 17.2±6.1% for these two regions, respectively. A mean SUV difference of 0.3±0.6 was achieved when using the direct method (DirAC) while the MRAC approach led to a mean SUV difference of -0.5±0.7. The quantitative analysis demonstrated the superior performance of the proposed deep learning-based AC approach over MRI segmentation-based method. The proposed approach seems promising to improve the quantitative accuracy of PET/MRI without the need for concurrent anatomical imaging.",6.0,2019,10.1109/NSS/MIC42101.2019.9059943,bones
41,Deep learning-guided attenuation and scatter correction without using anatomical images in brain PET/MRI,K. Bortolin; H. Arabi; H. Zaidi,"Attenuation correction (AC) is essential component for quantitative PET imaging. However, in PET/MR imaging and dedicated brain PET devices, the attenuation map either suffers from a number of limitations or is not readily available in the absence of CT or transmission scan. To tackle this issue, a deep convolutional neural networks is proposed to perform joint attenuation and scatter correction in the image domain on the non-attenuation corrected PET images (PET-nonAC). The deep con-volutional neural network used in this work benefits from dilated convolutions and residual connections to establish an end-to-end PET attenuation correction (PET-DirAC). For the training phase, data of 30 patients who underwent brain <sup>18</sup>F-FDG PET/CT scans were used to generate reference PET-CTAC and PET-nonAC images. A five-fold cross-validation scheme was used for training/evaluation of the proposed algorithm. The quantitative accuracy of the proposed method was evaluated against the commercial segmentation-based method (2-class AC map referred to as MRAC). For quantitative analysis, tracer uptake estimated from PET-DirAC and PET-MRAC was compared to PET-CTAC. The relative SUV bias was calculated for bone, soft-tissue, air cavities and the entire head, separately. The proposed approach resulted in a mean relative absolute error (MRAE) of 4.1±7.5% and 5.8±10.4% for the entire head and bone regions, respectively. Conversely, MRAC led to a MRAE of 8.1±10.2% and 17.2±6.1% for these two regions, respectively. A mean SUV difference of 0.3±0.6 was achieved when using the direct method (DirAC) while the MRAC approach led to a mean SUV difference of -0.5±0.7. The quantitative analysis demonstrated the superior performance of the proposed deep learning-based AC approach over MRI segmentation-based method. The proposed approach seems promising to improve the quantitative accuracy of PET/MRI without the need for concurrent anatomical imaging.",6.0,2019,10.1109/NSS/MIC42101.2019.9059943,attenuation
42,CT-guided PET Image Denoising using Deep Neural Network without Prior Training Data,J. Cui; K. Gong; N. Guo; X. Meng; K. Kim; H. Liu; Q. Li,Deep neural network have been a powerful tool for computer vision tasks and generally used in medical image [1]-[4].,2.0,2018,10.1109/NSSMIC.2018.8824397,noise reduction
42,CT-guided PET Image Denoising using Deep Neural Network without Prior Training Data,J. Cui; K. Gong; N. Guo; X. Meng; K. Kim; H. Liu; Q. Li,Deep neural network have been a powerful tool for computer vision tasks and generally used in medical image [1]-[4].,2.0,2018,10.1109/NSSMIC.2018.8824397,lesions
42,CT-guided PET Image Denoising using Deep Neural Network without Prior Training Data,J. Cui; K. Gong; N. Guo; X. Meng; K. Kim; H. Liu; Q. Li,Deep neural network have been a powerful tool for computer vision tasks and generally used in medical image [1]-[4].,2.0,2018,10.1109/NSSMIC.2018.8824397,optical filters
42,CT-guided PET Image Denoising using Deep Neural Network without Prior Training Data,J. Cui; K. Gong; N. Guo; X. Meng; K. Kim; H. Liu; Q. Li,Deep neural network have been a powerful tool for computer vision tasks and generally used in medical image [1]-[4].,2.0,2018,10.1109/NSSMIC.2018.8824397,noise measurement
43,A Review of Deep-Learning-Based Approaches for Attenuation Correction in Positron Emission Tomography,J. S. Lee,"Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. PET AC based on computed tomography (CT) frequently results in artifacts in attenuation-corrected PET images, and these artifacts mainly originate from CT artifacts and PET-CT mismatches. The AC in PET combined with a magnetic resonance imaging (MRI) scanner (PET/MRI) is more complex than PET/CT, given that MR images do not provide direct information on high-energy photon attenuation. Deep-learning (DL)-based methods for the improvement of PET AC have received significant research attention as alternatives to conventional AC methods. Many DL studies were focused on the transformation of MR images into synthetic pseudo-CT or attenuation maps. Alternative approaches that are not dependent on the anatomical images (CT or MRI) can overcome the limitations related to current CT- and MRI-based ACs and allow for more accurate PET quantification in stand-alone PET scanners for the realization of low radiation doses. In this article, a review is presented on the limitations of the PET AC in current dual-modality PET/CT and PET/MRI scanners, in addition to the current status and progress of DL-based approaches, for the realization of improved performance of PET AC.",22.0,2021,10.1109/TRPMS.2020.3009269,deep neural network
43,A Review of Deep-Learning-Based Approaches for Attenuation Correction in Positron Emission Tomography,J. S. Lee,"Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. PET AC based on computed tomography (CT) frequently results in artifacts in attenuation-corrected PET images, and these artifacts mainly originate from CT artifacts and PET-CT mismatches. The AC in PET combined with a magnetic resonance imaging (MRI) scanner (PET/MRI) is more complex than PET/CT, given that MR images do not provide direct information on high-energy photon attenuation. Deep-learning (DL)-based methods for the improvement of PET AC have received significant research attention as alternatives to conventional AC methods. Many DL studies were focused on the transformation of MR images into synthetic pseudo-CT or attenuation maps. Alternative approaches that are not dependent on the anatomical images (CT or MRI) can overcome the limitations related to current CT- and MRI-based ACs and allow for more accurate PET quantification in stand-alone PET scanners for the realization of low radiation doses. In this article, a review is presented on the limitations of the PET AC in current dual-modality PET/CT and PET/MRI scanners, in addition to the current status and progress of DL-based approaches, for the realization of improved performance of PET AC.",22.0,2021,10.1109/TRPMS.2020.3009269,plasmas
43,A Review of Deep-Learning-Based Approaches for Attenuation Correction in Positron Emission Tomography,J. S. Lee,"Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. PET AC based on computed tomography (CT) frequently results in artifacts in attenuation-corrected PET images, and these artifacts mainly originate from CT artifacts and PET-CT mismatches. The AC in PET combined with a magnetic resonance imaging (MRI) scanner (PET/MRI) is more complex than PET/CT, given that MR images do not provide direct information on high-energy photon attenuation. Deep-learning (DL)-based methods for the improvement of PET AC have received significant research attention as alternatives to conventional AC methods. Many DL studies were focused on the transformation of MR images into synthetic pseudo-CT or attenuation maps. Alternative approaches that are not dependent on the anatomical images (CT or MRI) can overcome the limitations related to current CT- and MRI-based ACs and allow for more accurate PET quantification in stand-alone PET scanners for the realization of low radiation doses. In this article, a review is presented on the limitations of the PET AC in current dual-modality PET/CT and PET/MRI scanners, in addition to the current status and progress of DL-based approaches, for the realization of improved performance of PET AC.",22.0,2021,10.1109/TRPMS.2020.3009269,attenuation
43,A Review of Deep-Learning-Based Approaches for Attenuation Correction in Positron Emission Tomography,J. S. Lee,"Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. PET AC based on computed tomography (CT) frequently results in artifacts in attenuation-corrected PET images, and these artifacts mainly originate from CT artifacts and PET-CT mismatches. The AC in PET combined with a magnetic resonance imaging (MRI) scanner (PET/MRI) is more complex than PET/CT, given that MR images do not provide direct information on high-energy photon attenuation. Deep-learning (DL)-based methods for the improvement of PET AC have received significant research attention as alternatives to conventional AC methods. Many DL studies were focused on the transformation of MR images into synthetic pseudo-CT or attenuation maps. Alternative approaches that are not dependent on the anatomical images (CT or MRI) can overcome the limitations related to current CT- and MRI-based ACs and allow for more accurate PET quantification in stand-alone PET scanners for the realization of low radiation doses. In this article, a review is presented on the limitations of the PET AC in current dual-modality PET/CT and PET/MRI scanners, in addition to the current status and progress of DL-based approaches, for the realization of improved performance of PET AC.",22.0,2021,10.1109/TRPMS.2020.3009269,photonics
43,A Review of Deep-Learning-Based Approaches for Attenuation Correction in Positron Emission Tomography,J. S. Lee,"Attenuation correction (AC) is essential for the generation of artifact-free and quantitatively accurate positron emission tomography (PET) images. PET AC based on computed tomography (CT) frequently results in artifacts in attenuation-corrected PET images, and these artifacts mainly originate from CT artifacts and PET-CT mismatches. The AC in PET combined with a magnetic resonance imaging (MRI) scanner (PET/MRI) is more complex than PET/CT, given that MR images do not provide direct information on high-energy photon attenuation. Deep-learning (DL)-based methods for the improvement of PET AC have received significant research attention as alternatives to conventional AC methods. Many DL studies were focused on the transformation of MR images into synthetic pseudo-CT or attenuation maps. Alternative approaches that are not dependent on the anatomical images (CT or MRI) can overcome the limitations related to current CT- and MRI-based ACs and allow for more accurate PET quantification in stand-alone PET scanners for the realization of low radiation doses. In this article, a review is presented on the limitations of the PET AC in current dual-modality PET/CT and PET/MRI scanners, in addition to the current status and progress of DL-based approaches, for the realization of improved performance of PET AC.",22.0,2021,10.1109/TRPMS.2020.3009269,attenuation correction (ac)
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,tumors
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,context
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,feature extraction
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,thorax
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,max-margin
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,global contexts
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,detection
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,abnormality
44,Global context inference for adaptive abnormality detection in PET-CT images,Y. Song; W. Cai; D. D. Feng,"PET-CT is now accepted as the best imaging technique for non-invasive staging of lung cancers, and a computer-based abnormality detection is potentially useful to assist the reading physicians in diagnosis. In this paper, we present a new fully-automatic approach to detect abnormalities in the thorax based on global context inference. A max-margin learning-based method is designed to infer the global contexts, which together with local features are then classified to produce the detection results adaptively. The proposed method is evaluated on clinical PET-CT images from NSCLC studies, and high detection precision and recall are demonstrated.",2.0,2012,10.1109/ISBI.2012.6235589,lungs
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,sparse detector configuration
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,gap correction
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,detectors
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,animals
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,statistical analysis
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,visualization
45,A Deep Neural Network To Recover Missing Data In Small Animal Pet Imaging: Comparison Between Sinogram- And Image-Domain Implementations,M. Amirrashedi; S. Sarkar; H. Ghadiri; P. Ghafarian; H. Zaidi; M. R. Ay,"Missing areas in PET sinograms and severe image artifacts as a consequence thereof, still gain prominence not only in sparse-ring detector configurations but also in full-ring PET scanners in case of faulty detectors. Empty bins in the projection domain, caused by inter-block gap regions or any failure in the detector blocks may lead to unacceptable image distortions and inaccuracies in quantitative analysis. Deep neural networks have recently attracted enormous attention within the imaging community and are being deployed for various applications, including handling impaired sinograms and removing the streaking artifacts generated by incomplete projection views. Despite the promising results in sparse-view CT reconstruction, the utility of deep-learning-based methods in synthesizing artifact-free PET images in the sparse-crystal setting is poorly explored. Herein, we investigated the feasibility of a modified U-Net to generate artifact-free PET scans in the presence of severe dead regions between adjacent detector blocks on a dedicated high-resolution preclinical PET scanner. The performance of the model was assessed in both projection and image-space. The visual inspection and quantitative analysis seem to indicate that the proposed method is well suited for application on partial-ring PET scanners.",,2021,10.1109/ISBI48211.2021.9433923,fault detection
46,A novel convolutional neural network for predicting full dose from low dose PET scans,A. Sanaat; H. Arabi; H. Zaidi,"The use of radiolabeled tracers in PET imaging raises concerns owing to potential risks from radiation exposure. Therefore, to reduce this potential risk in diagnostic PET imaging, efforts have been made to decrease the amount of radiotracer administered to the patient. However, decreasing the injected activity reduces the signal-to-noise Ratio (SNR) and deteriorates image quality, thus adversely impacting clinical diagnosis. Previously proposed techniques are complicated and slow, yet they yield satisfactory results at significantly low dose. In this work, we propose a deep learning algorithm to reconstruct full-dose (FD) from low-dose (LD) PET images using a fully convolutional encoder-decoder deep neural network model. The goal is to train a model to learn to reconstruct from images with only 5% of the counts to produce images corresponding to 100% of the dose. Brain PET/CT images of 140 patients acquired on the Siemens Biograph mCT with a standard injected activity of <sup>18</sup>F-FDG (205 ± 10 MBq). Images were acquired for about 20 min. The sinograms of each scan were used to produce a low-dose sinogram by randomly selecting only 1/20<sup>th</sup> of the counts. To avoid over fitting, data augmentation was used. A modified 3D U-Net, was developed to predict standard-dose sinogram (PSS) from their corresponding LD sinogram. Detailed quantitative and qualitative comparison demonstrated the proposed method can generate artefact-free diagnostic quality images that preserve internal structures without noise amplification. The structural similarity index (SSIM) and peak signal to noise ratio (PSNR) were used as quantitative metrics for assessment. For instance, the PSNR and SSIM in selected slices were 37.30±0.71 and 0.97±0.02, respectively. The proposed algorithm operates in the projection space and is capable of producing diagnostic quality images with only 5% of the standard injected activity.",1.0,2019,10.1109/NSS/MIC42101.2019.9059962,three-dimensional displays
46,A novel convolutional neural network for predicting full dose from low dose PET scans,A. Sanaat; H. Arabi; H. Zaidi,"The use of radiolabeled tracers in PET imaging raises concerns owing to potential risks from radiation exposure. Therefore, to reduce this potential risk in diagnostic PET imaging, efforts have been made to decrease the amount of radiotracer administered to the patient. However, decreasing the injected activity reduces the signal-to-noise Ratio (SNR) and deteriorates image quality, thus adversely impacting clinical diagnosis. Previously proposed techniques are complicated and slow, yet they yield satisfactory results at significantly low dose. In this work, we propose a deep learning algorithm to reconstruct full-dose (FD) from low-dose (LD) PET images using a fully convolutional encoder-decoder deep neural network model. The goal is to train a model to learn to reconstruct from images with only 5% of the counts to produce images corresponding to 100% of the dose. Brain PET/CT images of 140 patients acquired on the Siemens Biograph mCT with a standard injected activity of <sup>18</sup>F-FDG (205 ± 10 MBq). Images were acquired for about 20 min. The sinograms of each scan were used to produce a low-dose sinogram by randomly selecting only 1/20<sup>th</sup> of the counts. To avoid over fitting, data augmentation was used. A modified 3D U-Net, was developed to predict standard-dose sinogram (PSS) from their corresponding LD sinogram. Detailed quantitative and qualitative comparison demonstrated the proposed method can generate artefact-free diagnostic quality images that preserve internal structures without noise amplification. The structural similarity index (SSIM) and peak signal to noise ratio (PSNR) were used as quantitative metrics for assessment. For instance, the PSNR and SSIM in selected slices were 37.30±0.71 and 0.97±0.02, respectively. The proposed algorithm operates in the projection space and is capable of producing diagnostic quality images with only 5% of the standard injected activity.",1.0,2019,10.1109/NSS/MIC42101.2019.9059962,convolution
47,Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2.0,2018,10.1109/ICASSP.2018.8462188,ct
47,Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2.0,2018,10.1109/ICASSP.2018.8462188,support vector machines
47,Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2.0,2018,10.1109/ICASSP.2018.8462188,support vector machine
47,Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2.0,2018,10.1109/ICASSP.2018.8462188,tumors
47,Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2.0,2018,10.1109/ICASSP.2018.8462188,radiomics
47,Automated Detection of High FDG Uptake Regions in CT Images,A. Liebgott; F. Liebgott; B. Yang; S. Gatidis; K. Nikolaou,"Combined PET-CT scan is an important diagnostic tool in modern medicine, e.g. for staging or treatment planning in the field of oncology. Especially in small structures, like a tumour, textural variations visible in a PET image are not visually recognizable within a CT scan from the same region. Thus, both modalities are necessary for diagnosis. Since both techniques expose the patient to radiation, it would be desirable to get the same information about metabolic activity contained in the PET image from a CT scan only. To investigate the relationship between both imaging modalities, we propose a machine learning approach to automatically identify regions in a CT scan corresponding to areas with high FDG uptakes in a PET image.",2.0,2018,10.1109/ICASSP.2018.8462188,feature extraction
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,mr-based attenuation map
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,generators
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,bones
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,decoding
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,attenuation
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,generative adversarial network (gan)
48,Pseudo CT Image Synthesis and Bone Segmentation From MR Images Using Adversarial Networks With Residual Blocks for MR-Based Attenuation Correction of Brain PET Data,L. Tao; J. Fisher; E. Anaya; X. Li; C. S. Levin,"For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.",4.0,2021,10.1109/TRPMS.2020.2989073,task analysis
49,Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,2021,10.1109/ASYU52992.2021.9598993,lung cancer
49,Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,2021,10.1109/ASYU52992.2021.9598993,covid-19
49,Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,2021,10.1109/ASYU52992.2021.9598993,radiology
49,Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,2021,10.1109/ASYU52992.2021.9598993,technological innovation
49,Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,2021,10.1109/ASYU52992.2021.9598993,yolo-v3
49,Deep learning models for classifying cancer and COVID-19 lung diseases,D. Hişam; E. Hişam,"The use of Computed Tomography (CT) images for detecting lung diseases is both hard and time-consuming for humans. In the past few years, Artificial Intelligence (AI), especially, deep learning models have provided impressive results vs the classical methods in a lot of different fields. Nowadays, a lot of researchers are trying to develop different deep learning mechanisms to increase and improve the performance of different systems in lung disease screening with CT images. In this work, different deep learning-based models such as DarkNet-53 (the backbone of YOLO-v3), ResNet50, and VGG19 were applied to classify CT images of patients having Corona Virus disease (COVID-19) or lung cancer. Each model's performance is presented, analyzed, and compared. The dataset used in the study came from two different sources, the large-scale CT dataset for lung cancer diagnoses (Lung-PET -CT-Dx) for lung cancer CT images while International COVID-19 Open Radiology Dataset (RICORD) for COVID-19 CT images. As a result, DarkNet-53 overperformed other models by achieving 100% accuracy. While the accuracies for ResNet and VGG19 were 80% and 77% respectively.",,2021,10.1109/ASYU52992.2021.9598993,lung
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,heterogeneity
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,metabolic gradient
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,lung cancer
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,prognosis
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,texture
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,radiomics
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,tumors
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,nonsmall cell lung cancer (nsclc)
50,Combining Structural and Textural Assessments of Volumetric FDG-PET Uptake in NSCLC,E. Wolsztynski; J. O’Sullivan; N. M. Hughes; T. Mou; P. Murphy; F. O’Sullivan; K. O’Regan,"Numerous studies have reported the prognostic utility of texture analyses and the effectiveness of radiomics in PET and PET/CT assessment of nonsmall cell lung cancer (NSCLC). Here we explore the potential, relative to this methodology, of an alternative model-based approach to tumour characterization, which was successfully applied to sarcoma in previous works. The spatial distribution of 3-D FDG-PET uptake is evaluated in the spatial referential determined by the best-fitting ellipsoidal pattern, which provides a univariate uptake profile function of the radial position of intratumoral voxels. A group of structural features is extracted from this fit that include two heterogeneity variables and statistical summaries of local metabolic gradients. We demonstrate that these variables capture aspects of tumour metabolism that are separate to those described by conventional texture features. Prognostic model selection is performed in terms of a number of classifiers, including stepwise selection of logistic models, LASSO, random forests and neural networks with respect to two-year survival status. Our results for a cohort of 93 NSCLC patients show that structural variables have significant prognostic potential, and that they may be used in conjunction with texture features in a traditional radiomics sense, toward improved baseline multivariate models of patient overall survival. The statistical significance of these models also demonstrates the relevance of these machine learning classifiers for prognostic variable selection.",1.0,2019,10.1109/TRPMS.2019.2912433,spatial modeling
51,Three-dimensional shape completion using deep convolutional neural networks: Application to truncation compensation and metal artifact reduction in PET/MRI attenuation correction,H. Arabi; H. Zaidi,"Accurate attenuation correction (AC) of PET data is a prerequisite for quantitative PET/MR imaging. However, MR images are susceptible to metal artifacts leading to void MR signal around metallic implants. Moreover, for large patients, exceeding the scanner's field-of-view (FOV), MR images of the body will be truncated, mostly in the arms, thus hampering the accurate delineation of the body contour. Both metal-induced artifacts and body truncation affect PET AC by causing segmentation errors in MRI-based attenuation map generation. In this work, a deep learning convolutional neural network-based algorithm is proposed for the completion of MR images affected by body truncation or metal-induced artifacts. The core of the network utilizes dilated convolutions and residual connections to render an end-to-end 3D shape completion. The training of the network was performed using co-registered PET, CT and MR whole-body images of 15 patients. The evaluation of the proposed method was carried out on 10 patients with severe metal-induced artifacts and truncated MR images. Body contours from the corresponding non-attenuation corrected PET (non-AC PET) or CT images were segmented to estimate the amount of truncation in MR images. The estimated truncated volumes were later used as reference to assess the efficiency of the proposed method to recover the truncated or metal artifact affected areas. Moreover, the impact of the truncation compensation and metal-induced artifact reduction was investigated in the context of segmentation-based PET/MRI attenuation correction. The activity recovery in the affected areas was estimated before and after application of the shape completion method. The body truncation affected 11.1±2.3% of the body volume and consequently the MRI segmentation-based attenuation maps of 10 patients. After shape completion using the proposed method, the amount of truncated volume dropped to 0.7±0.2%. The SUV bias in the truncated area improved from -44.5±10% to -10.5±3% considering PET-CT AC as reference. Likewise, 8.5±1.9% of the head volume was affected by the metal-induced artifact leading to SUV bias of -59.5±11%. These were reduced to 0.3±0.1% and -23.5±9%, respectively, after shape completion. It was concluded that the proposed algorithm exhibited promising results towards the completion of MRI affected by truncation and metal-induced artifacts in whole-body PET/MRI.",1.0,2019,10.1109/NSS/MIC42101.2019.9059660,metals
51,Three-dimensional shape completion using deep convolutional neural networks: Application to truncation compensation and metal artifact reduction in PET/MRI attenuation correction,H. Arabi; H. Zaidi,"Accurate attenuation correction (AC) of PET data is a prerequisite for quantitative PET/MR imaging. However, MR images are susceptible to metal artifacts leading to void MR signal around metallic implants. Moreover, for large patients, exceeding the scanner's field-of-view (FOV), MR images of the body will be truncated, mostly in the arms, thus hampering the accurate delineation of the body contour. Both metal-induced artifacts and body truncation affect PET AC by causing segmentation errors in MRI-based attenuation map generation. In this work, a deep learning convolutional neural network-based algorithm is proposed for the completion of MR images affected by body truncation or metal-induced artifacts. The core of the network utilizes dilated convolutions and residual connections to render an end-to-end 3D shape completion. The training of the network was performed using co-registered PET, CT and MR whole-body images of 15 patients. The evaluation of the proposed method was carried out on 10 patients with severe metal-induced artifacts and truncated MR images. Body contours from the corresponding non-attenuation corrected PET (non-AC PET) or CT images were segmented to estimate the amount of truncation in MR images. The estimated truncated volumes were later used as reference to assess the efficiency of the proposed method to recover the truncated or metal artifact affected areas. Moreover, the impact of the truncation compensation and metal-induced artifact reduction was investigated in the context of segmentation-based PET/MRI attenuation correction. The activity recovery in the affected areas was estimated before and after application of the shape completion method. The body truncation affected 11.1±2.3% of the body volume and consequently the MRI segmentation-based attenuation maps of 10 patients. After shape completion using the proposed method, the amount of truncated volume dropped to 0.7±0.2%. The SUV bias in the truncated area improved from -44.5±10% to -10.5±3% considering PET-CT AC as reference. Likewise, 8.5±1.9% of the head volume was affected by the metal-induced artifact leading to SUV bias of -59.5±11%. These were reduced to 0.3±0.1% and -23.5±9%, respectively, after shape completion. It was concluded that the proposed algorithm exhibited promising results towards the completion of MRI affected by truncation and metal-induced artifacts in whole-body PET/MRI.",1.0,2019,10.1109/NSS/MIC42101.2019.9059660,head
51,Three-dimensional shape completion using deep convolutional neural networks: Application to truncation compensation and metal artifact reduction in PET/MRI attenuation correction,H. Arabi; H. Zaidi,"Accurate attenuation correction (AC) of PET data is a prerequisite for quantitative PET/MR imaging. However, MR images are susceptible to metal artifacts leading to void MR signal around metallic implants. Moreover, for large patients, exceeding the scanner's field-of-view (FOV), MR images of the body will be truncated, mostly in the arms, thus hampering the accurate delineation of the body contour. Both metal-induced artifacts and body truncation affect PET AC by causing segmentation errors in MRI-based attenuation map generation. In this work, a deep learning convolutional neural network-based algorithm is proposed for the completion of MR images affected by body truncation or metal-induced artifacts. The core of the network utilizes dilated convolutions and residual connections to render an end-to-end 3D shape completion. The training of the network was performed using co-registered PET, CT and MR whole-body images of 15 patients. The evaluation of the proposed method was carried out on 10 patients with severe metal-induced artifacts and truncated MR images. Body contours from the corresponding non-attenuation corrected PET (non-AC PET) or CT images were segmented to estimate the amount of truncation in MR images. The estimated truncated volumes were later used as reference to assess the efficiency of the proposed method to recover the truncated or metal artifact affected areas. Moreover, the impact of the truncation compensation and metal-induced artifact reduction was investigated in the context of segmentation-based PET/MRI attenuation correction. The activity recovery in the affected areas was estimated before and after application of the shape completion method. The body truncation affected 11.1±2.3% of the body volume and consequently the MRI segmentation-based attenuation maps of 10 patients. After shape completion using the proposed method, the amount of truncated volume dropped to 0.7±0.2%. The SUV bias in the truncated area improved from -44.5±10% to -10.5±3% considering PET-CT AC as reference. Likewise, 8.5±1.9% of the head volume was affected by the metal-induced artifact leading to SUV bias of -59.5±11%. These were reduced to 0.3±0.1% and -23.5±9%, respectively, after shape completion. It was concluded that the proposed algorithm exhibited promising results towards the completion of MRI affected by truncation and metal-induced artifacts in whole-body PET/MRI.",1.0,2019,10.1109/NSS/MIC42101.2019.9059660,attenuation
52,Weakly Supervised Deep Learning for COVID-19 Infection Detection and Classification From CT Images,S. Hu; Y. Gao; Z. Niu; Y. Jiang; L. Li; X. Xiao; M. Wang; E. F. Fang; W. Menpes-Smith; J. Xia; H. Ye; G. Yang,"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.",131.0,2020,10.1109/ACCESS.2020.3005510,weakly supervision
52,Weakly Supervised Deep Learning for COVID-19 Infection Detection and Classification From CT Images,S. Hu; Y. Gao; Z. Niu; Y. Jiang; L. Li; X. Xiao; M. Wang; E. F. Fang; W. Menpes-Smith; J. Xia; H. Ye; G. Yang,"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.",131.0,2020,10.1109/ACCESS.2020.3005510,convolutional neural network
52,Weakly Supervised Deep Learning for COVID-19 Infection Detection and Classification From CT Images,S. Hu; Y. Gao; Z. Niu; Y. Jiang; L. Li; X. Xiao; M. Wang; E. F. Fang; W. Menpes-Smith; J. Xia; H. Ye; G. Yang,"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.",131.0,2020,10.1109/ACCESS.2020.3005510,covid-19
52,Weakly Supervised Deep Learning for COVID-19 Infection Detection and Classification From CT Images,S. Hu; Y. Gao; Z. Niu; Y. Jiang; L. Li; X. Xiao; M. Wang; E. F. Fang; W. Menpes-Smith; J. Xia; H. Ye; G. Yang,"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.",131.0,2020,10.1109/ACCESS.2020.3005510,hospitals
52,Weakly Supervised Deep Learning for COVID-19 Infection Detection and Classification From CT Images,S. Hu; Y. Gao; Z. Niu; Y. Jiang; L. Li; X. Xiao; M. Wang; E. F. Fang; W. Menpes-Smith; J. Xia; H. Ye; G. Yang,"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.",131.0,2020,10.1109/ACCESS.2020.3005510,lung
53,Fast Dynamic Brain PET Imaging Using a Generative Adversarial Network,A. Sanaat; E. Mirsadeghi; B. Razeghi; N. Ginovart; H. Zaidi,"This work aims to present and evaluate a novel recurrent deep learning model for reduction of the acquisition time in dynamic brain PET imaging without forfeiting clinical information. The clinical dataset included 46 dynamic <sup>18</sup>F-DOPA brain PET/CT images used to evaluate a model for generation of complete dynamic PET images from 27% of the total acquisition time. The dataset was split into 35, 6, and 5 for training, validation, and test, respectively. Each dynamic PET scan lasts 90 minutes acquired in list-mode format used to reconstruct 26 dynamic frames). A video prediction deep learning algorithm consisted of two generative adversarial networks and one variational autoencoder was developed and optimized to depict the tracer variation trend from the initial 13 frames (0 to 25 min) and synthesize the last 13 frames (25 to 90 min), respectively. The generated image was analyzed quantitatively by calculating standard metrics, such as the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and time-activity curve (TAC). The PSNR and SSIM varied from 43.24 ± 0.4 to 38.82 ± 0.74 and from 0.98±0.03 to 0.81±0.09 for synthesized frames (14 to 26), respectively. The TAC trend showed that our model is able to predict images with similar tracer distribution compared to reference images. We demonstrated that the proposed method can generate the last 65 min time frames from the initial 25 min frames in dynamic PET imaging, thus reducing the total scanning time.",,2020,10.1109/NSS/MIC42677.2020.9507894,brain modeling
53,Fast Dynamic Brain PET Imaging Using a Generative Adversarial Network,A. Sanaat; E. Mirsadeghi; B. Razeghi; N. Ginovart; H. Zaidi,"This work aims to present and evaluate a novel recurrent deep learning model for reduction of the acquisition time in dynamic brain PET imaging without forfeiting clinical information. The clinical dataset included 46 dynamic <sup>18</sup>F-DOPA brain PET/CT images used to evaluate a model for generation of complete dynamic PET images from 27% of the total acquisition time. The dataset was split into 35, 6, and 5 for training, validation, and test, respectively. Each dynamic PET scan lasts 90 minutes acquired in list-mode format used to reconstruct 26 dynamic frames). A video prediction deep learning algorithm consisted of two generative adversarial networks and one variational autoencoder was developed and optimized to depict the tracer variation trend from the initial 13 frames (0 to 25 min) and synthesize the last 13 frames (25 to 90 min), respectively. The generated image was analyzed quantitatively by calculating standard metrics, such as the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and time-activity curve (TAC). The PSNR and SSIM varied from 43.24 ± 0.4 to 38.82 ± 0.74 and from 0.98±0.03 to 0.81±0.09 for synthesized frames (14 to 26), respectively. The TAC trend showed that our model is able to predict images with similar tracer distribution compared to reference images. We demonstrated that the proposed method can generate the last 65 min time frames from the initial 25 min frames in dynamic PET imaging, thus reducing the total scanning time.",,2020,10.1109/NSS/MIC42677.2020.9507894,measurement
53,Fast Dynamic Brain PET Imaging Using a Generative Adversarial Network,A. Sanaat; E. Mirsadeghi; B. Razeghi; N. Ginovart; H. Zaidi,"This work aims to present and evaluate a novel recurrent deep learning model for reduction of the acquisition time in dynamic brain PET imaging without forfeiting clinical information. The clinical dataset included 46 dynamic <sup>18</sup>F-DOPA brain PET/CT images used to evaluate a model for generation of complete dynamic PET images from 27% of the total acquisition time. The dataset was split into 35, 6, and 5 for training, validation, and test, respectively. Each dynamic PET scan lasts 90 minutes acquired in list-mode format used to reconstruct 26 dynamic frames). A video prediction deep learning algorithm consisted of two generative adversarial networks and one variational autoencoder was developed and optimized to depict the tracer variation trend from the initial 13 frames (0 to 25 min) and synthesize the last 13 frames (25 to 90 min), respectively. The generated image was analyzed quantitatively by calculating standard metrics, such as the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and time-activity curve (TAC). The PSNR and SSIM varied from 43.24 ± 0.4 to 38.82 ± 0.74 and from 0.98±0.03 to 0.81±0.09 for synthesized frames (14 to 26), respectively. The TAC trend showed that our model is able to predict images with similar tracer distribution compared to reference images. We demonstrated that the proposed method can generate the last 65 min time frames from the initial 25 min frames in dynamic PET imaging, thus reducing the total scanning time.",,2020,10.1109/NSS/MIC42677.2020.9507894,psnr
53,Fast Dynamic Brain PET Imaging Using a Generative Adversarial Network,A. Sanaat; E. Mirsadeghi; B. Razeghi; N. Ginovart; H. Zaidi,"This work aims to present and evaluate a novel recurrent deep learning model for reduction of the acquisition time in dynamic brain PET imaging without forfeiting clinical information. The clinical dataset included 46 dynamic <sup>18</sup>F-DOPA brain PET/CT images used to evaluate a model for generation of complete dynamic PET images from 27% of the total acquisition time. The dataset was split into 35, 6, and 5 for training, validation, and test, respectively. Each dynamic PET scan lasts 90 minutes acquired in list-mode format used to reconstruct 26 dynamic frames). A video prediction deep learning algorithm consisted of two generative adversarial networks and one variational autoencoder was developed and optimized to depict the tracer variation trend from the initial 13 frames (0 to 25 min) and synthesize the last 13 frames (25 to 90 min), respectively. The generated image was analyzed quantitatively by calculating standard metrics, such as the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and time-activity curve (TAC). The PSNR and SSIM varied from 43.24 ± 0.4 to 38.82 ± 0.74 and from 0.98±0.03 to 0.81±0.09 for synthesized frames (14 to 26), respectively. The TAC trend showed that our model is able to predict images with similar tracer distribution compared to reference images. We demonstrated that the proposed method can generate the last 65 min time frames from the initial 25 min frames in dynamic PET imaging, thus reducing the total scanning time.",,2020,10.1109/NSS/MIC42677.2020.9507894,predictive models
53,Fast Dynamic Brain PET Imaging Using a Generative Adversarial Network,A. Sanaat; E. Mirsadeghi; B. Razeghi; N. Ginovart; H. Zaidi,"This work aims to present and evaluate a novel recurrent deep learning model for reduction of the acquisition time in dynamic brain PET imaging without forfeiting clinical information. The clinical dataset included 46 dynamic <sup>18</sup>F-DOPA brain PET/CT images used to evaluate a model for generation of complete dynamic PET images from 27% of the total acquisition time. The dataset was split into 35, 6, and 5 for training, validation, and test, respectively. Each dynamic PET scan lasts 90 minutes acquired in list-mode format used to reconstruct 26 dynamic frames). A video prediction deep learning algorithm consisted of two generative adversarial networks and one variational autoencoder was developed and optimized to depict the tracer variation trend from the initial 13 frames (0 to 25 min) and synthesize the last 13 frames (25 to 90 min), respectively. The generated image was analyzed quantitatively by calculating standard metrics, such as the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), and time-activity curve (TAC). The PSNR and SSIM varied from 43.24 ± 0.4 to 38.82 ± 0.74 and from 0.98±0.03 to 0.81±0.09 for synthesized frames (14 to 26), respectively. The TAC trend showed that our model is able to predict images with similar tracer distribution compared to reference images. We demonstrated that the proposed method can generate the last 65 min time frames from the initial 25 min frames in dynamic PET imaging, thus reducing the total scanning time.",,2020,10.1109/NSS/MIC42677.2020.9507894,market research
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,diseases
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,lesions
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,semantics
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,uncertainty
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,monte carlo dropout
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,multi-zone uncertainty estimate
54,Coarse-to-Fine Adversarial Networks and Zone-Based Uncertainty Analysis for NK/T-Cell Lymphoma Segmentation in CT/PET Images,X. Hu; R. Guo; J. Chen; H. Li; D. Waldmannstetter; Y. Zhao; B. Li; K. Shi; B. Menze,"Extranodal natural killer/T cell lymphoma (ENKL), nasal type is a kind of rare disease with a low survival rate that primarily affects Asian and South American populations. Segmentation of ENKL lesions is crucial for clinical decision support and treatment planning. This paper is the first study on computer-aided diagnosis systems for the ENKL segmentation problem. We propose an automatic, coarse-to-fine approach for ENKL segmentation using adversarial networks. In the coarse stage, we extract the region of interest bounding the lesions utilizing a segmentation neural network. In the fine stage, we use an adversarial segmentation network and further introduce a multi-scale L<sub>1</sub> loss function to drive the network to learn both global and local features. The generator and discriminator are alternately trained by backpropagation in an adversarial fashion in a min-max game. Furthermore, we present the first exploration of zone-based uncertainty estimates based on Monte Carlo dropout technique in the context of deep networks for medical image segmentation. Specifically, we propose the uncertainty criteria based on the lesion and the background, and then linearly normalize them to a specific interval. This is not only the crucial criterion for evaluating the superiority of the algorithm, but also permits subsequent optimization by engineers and revision by clinicians after quantitatively understanding the main source of uncertainty from the background or the lesion zone. Experimental results demonstrate that the proposed method is more effective and lesion-zone stable than state-of-the-art deep-learning based segmentation model.",8.0,2020,10.1109/JBHI.2020.2972694,coarse-to-fine adversarial network
55,Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",8.0,2016,10.1109/ISBI.2016.7493373,manifolds
55,Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",8.0,2016,10.1109/ISBI.2016.7493373,local descriptor learning
55,Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",8.0,2016,10.1109/ISBI.2016.7493373,low-rank approximation
55,Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",8.0,2016,10.1109/ISBI.2016.7493373,attenuation
55,Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",8.0,2016,10.1109/ISBI.2016.7493373,learning systems
55,Predict CT image from MRI data using KNN-regression with learned local descriptors,L. Zhong; L. Lin; Z. Lu; Y. Wu; Z. Lu; M. Huang; W. Yang; Q. Feng,"Accurate prediction of CT image from MRI data is clinically desired for attenuation correction in PET/MR hybrid imaging systems and dose planning in MR-based radiation therapy. We present a k-nearest neighbor (KNN)-regression method to predict CT image from MRI data. In this method the nearest neighbors of each MR image patch are searched in the constraint spatial range. To improve the accuracy and efficiency of CT prediction, we propose to use of supervised descriptor learning based on low-rank approximation and manifold regularization to optimize the local descriptor of an MRimage patch and to reduce its dimensionality. The proposed method is evaluated on a dataset consisting of 13 subjects of paired brain MRI and CT images. Result shows that the proposed method can effectively predict CT images from MRI data and outperforms two state-of-the-art methods for CT prediction.",8.0,2016,10.1109/ISBI.2016.7493373,ct prediction
56,Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",9.0,2006,10.1109/IEMBS.2006.259238,software prototyping
56,Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",9.0,2006,10.1109/IEMBS.2006.259238,lung neoplasms
56,Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",9.0,2006,10.1109/IEMBS.2006.259238,inspection
56,Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",9.0,2006,10.1109/IEMBS.2006.259238,information resources
56,Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",9.0,2006,10.1109/IEMBS.2006.259238,data mining
56,Computerized Detection of Lung Tumors in PET/CT Images,I. Jafar; H. Ying; A. F. Shields; O. Muzik,"More and more hybrid PET/CT machines are being installed in medical centers across the country as combining Computer Tomography (CT) and Positron Emission Tomography (PET) provides powerful and unique means in tumor diagnosis. Visual inspection of the images is a tedious and error-prone task and in many clinics the attenuation-uncorrected PET images are not examined by the physician, potentially missing an important source of information, especially for subtle tumors. We are developing a computer aided diagnosis software prototype that simultaneously processes the CT, attenuation-corrected PET, and attenuation-uncorrected PET volumes to detect tumors in the lungs. The system applies optimal thresholding and multiple gray-level thresholding with volume criterion to extract the lungs and to detect tumor candidates, respectively. A fuzzy logic based approach is used to reduce false-positive tumors. The remaining set of tumor candidates are ranked according to their likelihood of being actual tumors. We show the preliminary results of a retrospective evaluation of clinical PET/CT images.",9.0,2006,10.1109/IEMBS.2006.259238,computer errors
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,thoracic organs
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,medical modalities
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,heart
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,aorta
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,ct segmentation
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,automatic segmentation
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,esophagus
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,task analysis
57,Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study,M. Ashok; A. Gupta,"Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors' results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).",1.0,2021,10.1109/ICAIS50930.2021.9396016,trachea
58,Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",,2022,10.1109/TMI.2021.3120913,deep neural network
58,Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",,2022,10.1109/TMI.2021.3120913,unsupervised learning
58,Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",,2022,10.1109/TMI.2021.3120913,direct reconstruction
58,Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",,2022,10.1109/TMI.2021.3120913,kernel
58,Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",,2022,10.1109/TMI.2021.3120913,kinetic theory
58,Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior,K. Gong; C. Catana; J. Qi; Q. Li,"Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient&#x2019;s anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a <inline-formula> <tex-math notation=""LaTeX"">${1} \times {1} \times {1}$ </tex-math></inline-formula> convolution layer. Evaluations based on dynamic datasets of <sup>18</sup>F-FDG and <sup>11</sup>C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.",,2022,10.1109/TMI.2021.3120913,mathematical models
59,Pseudo CT Estimation using Patch-based Joint Dictionary Learning,Y. Lei; H. K. Shu; S. Tian; T. Wang; T. Liu; H. Mao; H. Shim; W. J. Curran; X. Yang,"Magnetic resonance (MR) simulators have recently gained popularity; it avoids the unnecessary radiation exposure associated with Computed Tomography (CT) when used for radiation therapy planning. We propose a method for pseudo CT estimation from MR images based on joint dictionary learning. Patient-specific anatomical features were extracted from the aligned training images and adopted as signatures for each voxel. The most relevant and informative features were identified to train the joint dictionary learning-based model. The well-trained dictionary was used to predict the pseudo CT of a new patient. This prediction technique was validated with a clinical study of 12 patients with MR and CT images of the brain. The mean absolute error (MAE), peak signal-to-noise ratio (PSNR), normalized cross correlation (NCC) indexes were used to quantify the prediction accuracy. We compared our proposed method with a state-of-the-art dictionary learning method. Overall our proposed method significantly improves the prediction accuracy over the state-of-the-art dictionary learning method. We have investigated a novel joint dictionary Iearning-based approach to predict CT images from routine MRIs and demonstrated its reliability. This CT prediction technique could be a useful tool for MRI-based radiation treatment planning or attenuation correction for quantifying PET images for PET/MR imaging.",5.0,2018,10.1109/EMBC.2018.8513475,planning
59,Pseudo CT Estimation using Patch-based Joint Dictionary Learning,Y. Lei; H. K. Shu; S. Tian; T. Wang; T. Liu; H. Mao; H. Shim; W. J. Curran; X. Yang,"Magnetic resonance (MR) simulators have recently gained popularity; it avoids the unnecessary radiation exposure associated with Computed Tomography (CT) when used for radiation therapy planning. We propose a method for pseudo CT estimation from MR images based on joint dictionary learning. Patient-specific anatomical features were extracted from the aligned training images and adopted as signatures for each voxel. The most relevant and informative features were identified to train the joint dictionary learning-based model. The well-trained dictionary was used to predict the pseudo CT of a new patient. This prediction technique was validated with a clinical study of 12 patients with MR and CT images of the brain. The mean absolute error (MAE), peak signal-to-noise ratio (PSNR), normalized cross correlation (NCC) indexes were used to quantify the prediction accuracy. We compared our proposed method with a state-of-the-art dictionary learning method. Overall our proposed method significantly improves the prediction accuracy over the state-of-the-art dictionary learning method. We have investigated a novel joint dictionary Iearning-based approach to predict CT images from routine MRIs and demonstrated its reliability. This CT prediction technique could be a useful tool for MRI-based radiation treatment planning or attenuation correction for quantifying PET images for PET/MR imaging.",5.0,2018,10.1109/EMBC.2018.8513475,feature extraction
59,Pseudo CT Estimation using Patch-based Joint Dictionary Learning,Y. Lei; H. K. Shu; S. Tian; T. Wang; T. Liu; H. Mao; H. Shim; W. J. Curran; X. Yang,"Magnetic resonance (MR) simulators have recently gained popularity; it avoids the unnecessary radiation exposure associated with Computed Tomography (CT) when used for radiation therapy planning. We propose a method for pseudo CT estimation from MR images based on joint dictionary learning. Patient-specific anatomical features were extracted from the aligned training images and adopted as signatures for each voxel. The most relevant and informative features were identified to train the joint dictionary learning-based model. The well-trained dictionary was used to predict the pseudo CT of a new patient. This prediction technique was validated with a clinical study of 12 patients with MR and CT images of the brain. The mean absolute error (MAE), peak signal-to-noise ratio (PSNR), normalized cross correlation (NCC) indexes were used to quantify the prediction accuracy. We compared our proposed method with a state-of-the-art dictionary learning method. Overall our proposed method significantly improves the prediction accuracy over the state-of-the-art dictionary learning method. We have investigated a novel joint dictionary Iearning-based approach to predict CT images from routine MRIs and demonstrated its reliability. This CT prediction technique could be a useful tool for MRI-based radiation treatment planning or attenuation correction for quantifying PET images for PET/MR imaging.",5.0,2018,10.1109/EMBC.2018.8513475,dictionaries
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,mice
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,independent component analysis
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,heart
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,spatial resolution
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,animals
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,data mining
60,Ensemble Learning (EL) Independent Component Analysis (ICA) Approach to Derive Blood Input Function from FDG-PET Images in Small Animal,Z. Fu; M. N. Tantawy; T. E. Peterson,"To extract the blood time-activity curves (TACs) from the PET image of a mouse heart is very difficult due to the limited spatial resolution of the PET system, small size of the heart, partial volume effects and cardiac motion. Ensemble learning-independent component analysis (EL-ICA), a recently developed Bayesian method, has been implemented to extract clear TACs from the PET images and also been proved to be a useful method for image segmentation. The advantage of EL-ICA is it decomposes the images into different independent components while imposing strong nonnegativity constraints, which can maintain the independence and nonnegativity of the component images and TACs simultaneously. A down-sampled, segmented CT data set has been used to generate simulated PET data to best represent the structure of a real cardiac image. From the results of the simulation, we can show that EL-ICA was able to extract the TACs of the sample data. We have also applied EL-ICA to FDG images in mice. In this study, we show that myocardium and blood pool components can be separated successfully by EL-ICA, and the according TACs obtained. The EL-ICA method can be used to extract the arterial input function directly from the dynamic PET images to avoid the need for multiple blood sampling of the small animal.",2.0,2006,10.1109/NSSMIC.2006.356439,blood
61,An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",4.0,2020,10.1109/ACCESS.2019.2961268,two-level similarity measure
61,An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",4.0,2020,10.1109/ACCESS.2019.2961268,unsupervised learning
61,An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",4.0,2020,10.1109/ACCESS.2019.2961268,task analysis
61,An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",4.0,2020,10.1109/ACCESS.2019.2961268,three-dimensional displays
61,An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",4.0,2020,10.1109/ACCESS.2019.2961268,deformation regularization
61,An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images,H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. -D. Yao,"A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.",4.0,2020,10.1109/ACCESS.2019.2961268,strain
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,computer vision
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,cross-validation
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,measurement
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,computational modeling
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,u-net
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,brain tumours segmentation
62,U-Net training models for efficient brain tumour segmentation on multi-modality CT and PET images,E. Kot; Z. Krawczyk; K. Siwek; K. Pleska; J. Rogalski; P. Czwarnowski,"Medicine, and particularly radiology, is an area where vision systems bring significant benefits, which results in more accurate diagnoses, predictions, and treatment plans. This paper proposes a U-Net training model and simplifies the deep learning based framework for tumour detection and semantic segmentation that shapes frames for computer-aided diagnoses (CADx) and computer-aided detection (CADe) applications. The U-Net was used to segment glioma – tumour area. A training technique is proposed and convolutional neural network models capable of being trained on a dataset of fused full-size CT and PET scans – 512x512x1 – is addressed. The implemented algorithms were executed in a cloud environment, where storage was decoupled from compute (CPU and GPU). The resultant models’ performance was assessed utilizing multiple metrics. Trained models fully automatically detect tumours in a given dataset. The best scored coefficient (Dice Co-Eff) for a model is 0.8750. The paper which follows is a detailed reference with training parameters for U-Net for efficient brain tumour detection on CT and PET scans where the dataset is limited –- in this case, consisting of merely 20 patients.",,2021,10.1109/CPEE54040.2021.9585272,semantics
63,Deep learning approaches for bone and bone lesion segmentation on 18FDG PET/CT imaging in the context of metastatic breast cancer,N. Moreau; C. Rousseau; C. Fourcade; G. Santini; L. Ferrer; M. Lacombe; C. Guillerminet; M. Campone; M. Colombié; M. Rubeaux; a. N. Normand,"<sup>18</sup>FDG PET/CT imaging is commonly used in diagnosis and follow-up of metastatic breast cancer, but its quantitative analysis is complicated by the number and location heterogeneity of metastatic lesions. Considering that bones are the most common location among metastatic sites, this work aims to compare different approaches to segment the bones and bone metastatic lesions in breast cancer.Two deep learning methods based on U-Net were developed and trained to segment either both bones and bone lesions or bone lesions alone on PET/CT images. These methods were cross-validated on 24 patients from the prospective EPICURE<inf>seinmeta</inf> metastatic breast cancer study and were evaluated using recall and precision to measure lesion detection, as well as the Dice score to assess bones and bone lesions segmentation accuracy.Results show that taking into account bone information in the training process allows to improve the precision of the lesions detection as well as the Dice score of the segmented lesions. Moreover, using the obtained bone and bone lesion masks, we were able to compute a PET bone index (PBI) inspired by the recognized Bone Scan Index (BSI). This automatically computed PBI globally agrees with the one calculated from ground truth delineations.Clinical relevance— We propose a completely automatic deep learning based method to detect and segment bones and bone lesions on <sup>18</sup>FDG PET/CT in the context of metastatic breast cancer. We also introduce an automatic PET bone index which could be incorporated in the monitoring and decision process.",3.0,2020,10.1109/EMBC44109.2020.9175904,bones
63,Deep learning approaches for bone and bone lesion segmentation on 18FDG PET/CT imaging in the context of metastatic breast cancer,N. Moreau; C. Rousseau; C. Fourcade; G. Santini; L. Ferrer; M. Lacombe; C. Guillerminet; M. Campone; M. Colombié; M. Rubeaux; a. N. Normand,"<sup>18</sup>FDG PET/CT imaging is commonly used in diagnosis and follow-up of metastatic breast cancer, but its quantitative analysis is complicated by the number and location heterogeneity of metastatic lesions. Considering that bones are the most common location among metastatic sites, this work aims to compare different approaches to segment the bones and bone metastatic lesions in breast cancer.Two deep learning methods based on U-Net were developed and trained to segment either both bones and bone lesions or bone lesions alone on PET/CT images. These methods were cross-validated on 24 patients from the prospective EPICURE<inf>seinmeta</inf> metastatic breast cancer study and were evaluated using recall and precision to measure lesion detection, as well as the Dice score to assess bones and bone lesions segmentation accuracy.Results show that taking into account bone information in the training process allows to improve the precision of the lesions detection as well as the Dice score of the segmented lesions. Moreover, using the obtained bone and bone lesion masks, we were able to compute a PET bone index (PBI) inspired by the recognized Bone Scan Index (BSI). This automatically computed PBI globally agrees with the one calculated from ground truth delineations.Clinical relevance— We propose a completely automatic deep learning based method to detect and segment bones and bone lesions on <sup>18</sup>FDG PET/CT in the context of metastatic breast cancer. We also introduce an automatic PET bone index which could be incorporated in the monitoring and decision process.",3.0,2020,10.1109/EMBC44109.2020.9175904,lesions
63,Deep learning approaches for bone and bone lesion segmentation on 18FDG PET/CT imaging in the context of metastatic breast cancer,N. Moreau; C. Rousseau; C. Fourcade; G. Santini; L. Ferrer; M. Lacombe; C. Guillerminet; M. Campone; M. Colombié; M. Rubeaux; a. N. Normand,"<sup>18</sup>FDG PET/CT imaging is commonly used in diagnosis and follow-up of metastatic breast cancer, but its quantitative analysis is complicated by the number and location heterogeneity of metastatic lesions. Considering that bones are the most common location among metastatic sites, this work aims to compare different approaches to segment the bones and bone metastatic lesions in breast cancer.Two deep learning methods based on U-Net were developed and trained to segment either both bones and bone lesions or bone lesions alone on PET/CT images. These methods were cross-validated on 24 patients from the prospective EPICURE<inf>seinmeta</inf> metastatic breast cancer study and were evaluated using recall and precision to measure lesion detection, as well as the Dice score to assess bones and bone lesions segmentation accuracy.Results show that taking into account bone information in the training process allows to improve the precision of the lesions detection as well as the Dice score of the segmented lesions. Moreover, using the obtained bone and bone lesion masks, we were able to compute a PET bone index (PBI) inspired by the recognized Bone Scan Index (BSI). This automatically computed PBI globally agrees with the one calculated from ground truth delineations.Clinical relevance— We propose a completely automatic deep learning based method to detect and segment bones and bone lesions on <sup>18</sup>FDG PET/CT in the context of metastatic breast cancer. We also introduce an automatic PET bone index which could be incorporated in the monitoring and decision process.",3.0,2020,10.1109/EMBC44109.2020.9175904,indexes
63,Deep learning approaches for bone and bone lesion segmentation on 18FDG PET/CT imaging in the context of metastatic breast cancer,N. Moreau; C. Rousseau; C. Fourcade; G. Santini; L. Ferrer; M. Lacombe; C. Guillerminet; M. Campone; M. Colombié; M. Rubeaux; a. N. Normand,"<sup>18</sup>FDG PET/CT imaging is commonly used in diagnosis and follow-up of metastatic breast cancer, but its quantitative analysis is complicated by the number and location heterogeneity of metastatic lesions. Considering that bones are the most common location among metastatic sites, this work aims to compare different approaches to segment the bones and bone metastatic lesions in breast cancer.Two deep learning methods based on U-Net were developed and trained to segment either both bones and bone lesions or bone lesions alone on PET/CT images. These methods were cross-validated on 24 patients from the prospective EPICURE<inf>seinmeta</inf> metastatic breast cancer study and were evaluated using recall and precision to measure lesion detection, as well as the Dice score to assess bones and bone lesions segmentation accuracy.Results show that taking into account bone information in the training process allows to improve the precision of the lesions detection as well as the Dice score of the segmented lesions. Moreover, using the obtained bone and bone lesion masks, we were able to compute a PET bone index (PBI) inspired by the recognized Bone Scan Index (BSI). This automatically computed PBI globally agrees with the one calculated from ground truth delineations.Clinical relevance— We propose a completely automatic deep learning based method to detect and segment bones and bone lesions on <sup>18</sup>FDG PET/CT in the context of metastatic breast cancer. We also introduce an automatic PET bone index which could be incorporated in the monitoring and decision process.",3.0,2020,10.1109/EMBC44109.2020.9175904,breast cancer
64,Investigation of Spatial-Temporal Kernel Method for Dynamic Imaging in Short and Long Range PET Scanners,Y. Li; Y. Zhao; Y. Lv; J. Zhao,"Dynamic positron emission tomography (PET) imaging has a limited temporal resolution, and always suffers from poor signal to noise ratio (SNR) due to low count statistics from short time frames and the ill-posed nature of expectation-maximization (EM) based reconstruction algorithm. Increasing acquisition time for each frame improves image SNR, but may introduce motion artefacts to the region of heart, lung and head. So, a trade-off has to be made between temporal resolution and image quality. Previous studies have demonstrated that aided by prior information from long time frames and/or anatomical images, the kernel method can effectively improve the SNR of the dynamic PET images from very low-count data. In this study, we have investigated the performance of kernel method in long range and simulated short range scanners. We implemented a spatial-temporal kernel method (ST-KEM) and applied it to the data collected from the 194-cm PET/CT scanner (uEXPLORER). Two short range PET scanners were simulated by using partial of the total eight units of uEXPLORER. Additionally, we exploited the way to improve prior images of short range scanners through deep learning. The results showed that ST-KEM outperformed the kernelized Expectation-Maximization (KEM) and conventional Ordered Subsets Expectation Maximization (OSEM) for frames as short as 0.1 second in uEXPLORER, from which noiseless cardiac motion signal could be extracted. In simulated short range scanners, similar image quality could be achieved when the frame duration extends to 1 to 2-second.",,2020,10.1109/NSS/MIC42677.2020.9507931,dynamics
64,Investigation of Spatial-Temporal Kernel Method for Dynamic Imaging in Short and Long Range PET Scanners,Y. Li; Y. Zhao; Y. Lv; J. Zhao,"Dynamic positron emission tomography (PET) imaging has a limited temporal resolution, and always suffers from poor signal to noise ratio (SNR) due to low count statistics from short time frames and the ill-posed nature of expectation-maximization (EM) based reconstruction algorithm. Increasing acquisition time for each frame improves image SNR, but may introduce motion artefacts to the region of heart, lung and head. So, a trade-off has to be made between temporal resolution and image quality. Previous studies have demonstrated that aided by prior information from long time frames and/or anatomical images, the kernel method can effectively improve the SNR of the dynamic PET images from very low-count data. In this study, we have investigated the performance of kernel method in long range and simulated short range scanners. We implemented a spatial-temporal kernel method (ST-KEM) and applied it to the data collected from the 194-cm PET/CT scanner (uEXPLORER). Two short range PET scanners were simulated by using partial of the total eight units of uEXPLORER. Additionally, we exploited the way to improve prior images of short range scanners through deep learning. The results showed that ST-KEM outperformed the kernelized Expectation-Maximization (KEM) and conventional Ordered Subsets Expectation Maximization (OSEM) for frames as short as 0.1 second in uEXPLORER, from which noiseless cardiac motion signal could be extracted. In simulated short range scanners, similar image quality could be achieved when the frame duration extends to 1 to 2-second.",,2020,10.1109/NSS/MIC42677.2020.9507931,heuristic algorithms
64,Investigation of Spatial-Temporal Kernel Method for Dynamic Imaging in Short and Long Range PET Scanners,Y. Li; Y. Zhao; Y. Lv; J. Zhao,"Dynamic positron emission tomography (PET) imaging has a limited temporal resolution, and always suffers from poor signal to noise ratio (SNR) due to low count statistics from short time frames and the ill-posed nature of expectation-maximization (EM) based reconstruction algorithm. Increasing acquisition time for each frame improves image SNR, but may introduce motion artefacts to the region of heart, lung and head. So, a trade-off has to be made between temporal resolution and image quality. Previous studies have demonstrated that aided by prior information from long time frames and/or anatomical images, the kernel method can effectively improve the SNR of the dynamic PET images from very low-count data. In this study, we have investigated the performance of kernel method in long range and simulated short range scanners. We implemented a spatial-temporal kernel method (ST-KEM) and applied it to the data collected from the 194-cm PET/CT scanner (uEXPLORER). Two short range PET scanners were simulated by using partial of the total eight units of uEXPLORER. Additionally, we exploited the way to improve prior images of short range scanners through deep learning. The results showed that ST-KEM outperformed the kernelized Expectation-Maximization (KEM) and conventional Ordered Subsets Expectation Maximization (OSEM) for frames as short as 0.1 second in uEXPLORER, from which noiseless cardiac motion signal could be extracted. In simulated short range scanners, similar image quality could be achieved when the frame duration extends to 1 to 2-second.",,2020,10.1109/NSS/MIC42677.2020.9507931,reconstruction algorithms
64,Investigation of Spatial-Temporal Kernel Method for Dynamic Imaging in Short and Long Range PET Scanners,Y. Li; Y. Zhao; Y. Lv; J. Zhao,"Dynamic positron emission tomography (PET) imaging has a limited temporal resolution, and always suffers from poor signal to noise ratio (SNR) due to low count statistics from short time frames and the ill-posed nature of expectation-maximization (EM) based reconstruction algorithm. Increasing acquisition time for each frame improves image SNR, but may introduce motion artefacts to the region of heart, lung and head. So, a trade-off has to be made between temporal resolution and image quality. Previous studies have demonstrated that aided by prior information from long time frames and/or anatomical images, the kernel method can effectively improve the SNR of the dynamic PET images from very low-count data. In this study, we have investigated the performance of kernel method in long range and simulated short range scanners. We implemented a spatial-temporal kernel method (ST-KEM) and applied it to the data collected from the 194-cm PET/CT scanner (uEXPLORER). Two short range PET scanners were simulated by using partial of the total eight units of uEXPLORER. Additionally, we exploited the way to improve prior images of short range scanners through deep learning. The results showed that ST-KEM outperformed the kernelized Expectation-Maximization (KEM) and conventional Ordered Subsets Expectation Maximization (OSEM) for frames as short as 0.1 second in uEXPLORER, from which noiseless cardiac motion signal could be extracted. In simulated short range scanners, similar image quality could be achieved when the frame duration extends to 1 to 2-second.",,2020,10.1109/NSS/MIC42677.2020.9507931,lung
65,PET-CT based automated lung nodule detection,N. Zsoter; P. Bandi; G. Szabo; Z. Toth; R. A. Bundschuh; J. Dinges; L. Papp,"An automatic method is presented in order to detect lung nodules in PET-CT studies. Using the foreground and background mean ratio independently in every nodule, we can detect the region of the nodules properly. The size and intensity of the lesions do not affect the result of the algorithm, although size constraints are present in the final classification step. The CT image is also used to classify the found lesions built on lung segmentation. We also deal with those cases when nearby and similar nodules are merged into one by a split-up post-processing step. With our method the time of the localization can be decreased from more than one hour to maximum five minutes. The method had been implemented and validated on real clinical cases in Interview Fusion clinical evaluation software (Mediso). Results indicate that our approach is very effective in detecting lung nodules and can be a valuable aid for physicians working in the daily routine of oncology.",3.0,2012,10.1109/EMBC.2012.6347109,lesions
65,PET-CT based automated lung nodule detection,N. Zsoter; P. Bandi; G. Szabo; Z. Toth; R. A. Bundschuh; J. Dinges; L. Papp,"An automatic method is presented in order to detect lung nodules in PET-CT studies. Using the foreground and background mean ratio independently in every nodule, we can detect the region of the nodules properly. The size and intensity of the lesions do not affect the result of the algorithm, although size constraints are present in the final classification step. The CT image is also used to classify the found lesions built on lung segmentation. We also deal with those cases when nearby and similar nodules are merged into one by a split-up post-processing step. With our method the time of the localization can be decreased from more than one hour to maximum five minutes. The method had been implemented and validated on real clinical cases in Interview Fusion clinical evaluation software (Mediso). Results indicate that our approach is very effective in detecting lung nodules and can be a valuable aid for physicians working in the daily routine of oncology.",3.0,2012,10.1109/EMBC.2012.6347109,cancer
65,PET-CT based automated lung nodule detection,N. Zsoter; P. Bandi; G. Szabo; Z. Toth; R. A. Bundschuh; J. Dinges; L. Papp,"An automatic method is presented in order to detect lung nodules in PET-CT studies. Using the foreground and background mean ratio independently in every nodule, we can detect the region of the nodules properly. The size and intensity of the lesions do not affect the result of the algorithm, although size constraints are present in the final classification step. The CT image is also used to classify the found lesions built on lung segmentation. We also deal with those cases when nearby and similar nodules are merged into one by a split-up post-processing step. With our method the time of the localization can be decreased from more than one hour to maximum five minutes. The method had been implemented and validated on real clinical cases in Interview Fusion clinical evaluation software (Mediso). Results indicate that our approach is very effective in detecting lung nodules and can be a valuable aid for physicians working in the daily routine of oncology.",3.0,2012,10.1109/EMBC.2012.6347109,muscles
65,PET-CT based automated lung nodule detection,N. Zsoter; P. Bandi; G. Szabo; Z. Toth; R. A. Bundschuh; J. Dinges; L. Papp,"An automatic method is presented in order to detect lung nodules in PET-CT studies. Using the foreground and background mean ratio independently in every nodule, we can detect the region of the nodules properly. The size and intensity of the lesions do not affect the result of the algorithm, although size constraints are present in the final classification step. The CT image is also used to classify the found lesions built on lung segmentation. We also deal with those cases when nearby and similar nodules are merged into one by a split-up post-processing step. With our method the time of the localization can be decreased from more than one hour to maximum five minutes. The method had been implemented and validated on real clinical cases in Interview Fusion clinical evaluation software (Mediso). Results indicate that our approach is very effective in detecting lung nodules and can be a valuable aid for physicians working in the daily routine of oncology.",3.0,2012,10.1109/EMBC.2012.6347109,lungs
66,Deep Neural Network for Automatic Characterization of Lesions on 68Ga-PSMA PET/CT Images,Y. Zhao; A. Gafita; G. Tetteh; F. Haupt; A. Afshar-Oromieh; B. Menze; M. Eiber; A. Rominger; K. Shi,"The emerging PSMA-targeted radionuclide therapy provides an effective method for the treatment of advanced metastatic prostate cancer. To optimize the therapeutic effect and maximize the theranostic benefit, there is a need to identify and quantify target lesions prior to treatment. However, this is extremely challenging considering that a high number of lesions of heterogeneous size and uptake may distribute in a variety of anatomical context with different backgrounds. This study proposes an end-to-end deep neural network to characterize the prostate cancer lesions on PSMA imaging automatically. A <sup>68</sup>Ga-PSMA-11 PET/CT image dataset including 71 patients with metastatic prostate cancer was collected from three medical centres for training and evaluating the proposed network. For proof-of-concept, we focus on the detection of bone and lymph node lesions in the pelvic area suggestive for metastases of prostate cancer. The preliminary test on pelvic area confirms the potential of deep learning methods. Increasing the amount of training data may further enhance the performance of the proposed deep learning method.",,2019,10.1109/EMBC.2019.8857955,prostate cancer
66,Deep Neural Network for Automatic Characterization of Lesions on 68Ga-PSMA PET/CT Images,Y. Zhao; A. Gafita; G. Tetteh; F. Haupt; A. Afshar-Oromieh; B. Menze; M. Eiber; A. Rominger; K. Shi,"The emerging PSMA-targeted radionuclide therapy provides an effective method for the treatment of advanced metastatic prostate cancer. To optimize the therapeutic effect and maximize the theranostic benefit, there is a need to identify and quantify target lesions prior to treatment. However, this is extremely challenging considering that a high number of lesions of heterogeneous size and uptake may distribute in a variety of anatomical context with different backgrounds. This study proposes an end-to-end deep neural network to characterize the prostate cancer lesions on PSMA imaging automatically. A <sup>68</sup>Ga-PSMA-11 PET/CT image dataset including 71 patients with metastatic prostate cancer was collected from three medical centres for training and evaluating the proposed network. For proof-of-concept, we focus on the detection of bone and lymph node lesions in the pelvic area suggestive for metastases of prostate cancer. The preliminary test on pelvic area confirms the potential of deep learning methods. Increasing the amount of training data may further enhance the performance of the proposed deep learning method.",,2019,10.1109/EMBC.2019.8857955,lymph nodes
66,Deep Neural Network for Automatic Characterization of Lesions on 68Ga-PSMA PET/CT Images,Y. Zhao; A. Gafita; G. Tetteh; F. Haupt; A. Afshar-Oromieh; B. Menze; M. Eiber; A. Rominger; K. Shi,"The emerging PSMA-targeted radionuclide therapy provides an effective method for the treatment of advanced metastatic prostate cancer. To optimize the therapeutic effect and maximize the theranostic benefit, there is a need to identify and quantify target lesions prior to treatment. However, this is extremely challenging considering that a high number of lesions of heterogeneous size and uptake may distribute in a variety of anatomical context with different backgrounds. This study proposes an end-to-end deep neural network to characterize the prostate cancer lesions on PSMA imaging automatically. A <sup>68</sup>Ga-PSMA-11 PET/CT image dataset including 71 patients with metastatic prostate cancer was collected from three medical centres for training and evaluating the proposed network. For proof-of-concept, we focus on the detection of bone and lymph node lesions in the pelvic area suggestive for metastases of prostate cancer. The preliminary test on pelvic area confirms the potential of deep learning methods. Increasing the amount of training data may further enhance the performance of the proposed deep learning method.",,2019,10.1109/EMBC.2019.8857955,bones
66,Deep Neural Network for Automatic Characterization of Lesions on 68Ga-PSMA PET/CT Images,Y. Zhao; A. Gafita; G. Tetteh; F. Haupt; A. Afshar-Oromieh; B. Menze; M. Eiber; A. Rominger; K. Shi,"The emerging PSMA-targeted radionuclide therapy provides an effective method for the treatment of advanced metastatic prostate cancer. To optimize the therapeutic effect and maximize the theranostic benefit, there is a need to identify and quantify target lesions prior to treatment. However, this is extremely challenging considering that a high number of lesions of heterogeneous size and uptake may distribute in a variety of anatomical context with different backgrounds. This study proposes an end-to-end deep neural network to characterize the prostate cancer lesions on PSMA imaging automatically. A <sup>68</sup>Ga-PSMA-11 PET/CT image dataset including 71 patients with metastatic prostate cancer was collected from three medical centres for training and evaluating the proposed network. For proof-of-concept, we focus on the detection of bone and lymph node lesions in the pelvic area suggestive for metastases of prostate cancer. The preliminary test on pelvic area confirms the potential of deep learning methods. Increasing the amount of training data may further enhance the performance of the proposed deep learning method.",,2019,10.1109/EMBC.2019.8857955,lesions
66,Deep Neural Network for Automatic Characterization of Lesions on 68Ga-PSMA PET/CT Images,Y. Zhao; A. Gafita; G. Tetteh; F. Haupt; A. Afshar-Oromieh; B. Menze; M. Eiber; A. Rominger; K. Shi,"The emerging PSMA-targeted radionuclide therapy provides an effective method for the treatment of advanced metastatic prostate cancer. To optimize the therapeutic effect and maximize the theranostic benefit, there is a need to identify and quantify target lesions prior to treatment. However, this is extremely challenging considering that a high number of lesions of heterogeneous size and uptake may distribute in a variety of anatomical context with different backgrounds. This study proposes an end-to-end deep neural network to characterize the prostate cancer lesions on PSMA imaging automatically. A <sup>68</sup>Ga-PSMA-11 PET/CT image dataset including 71 patients with metastatic prostate cancer was collected from three medical centres for training and evaluating the proposed network. For proof-of-concept, we focus on the detection of bone and lymph node lesions in the pelvic area suggestive for metastases of prostate cancer. The preliminary test on pelvic area confirms the potential of deep learning methods. Increasing the amount of training data may further enhance the performance of the proposed deep learning method.",,2019,10.1109/EMBC.2019.8857955,convolution
67,Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",8.0,2019,10.1109/CIBCB.2019.8791473,data models
67,Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",8.0,2019,10.1109/CIBCB.2019.8791473,feature extraction
67,Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",8.0,2019,10.1109/CIBCB.2019.8791473,radiomics
67,Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",8.0,2019,10.1109/CIBCB.2019.8791473,cancer
67,Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",8.0,2019,10.1109/CIBCB.2019.8791473,integration
67,Integrating deep and radiomics features in cancer bioimaging,A. Bizzego; N. Bussola; D. Salvalai; M. Chierici; V. Maggio; G. Jurman; C. Furlanello,"Almost every clinical specialty will use artificial intelligence in the future. The first area of practical impact is expected to be the rapid and accurate interpretation of image streams such as radiology scans, histo-pathology slides, ophthalmic imaging, and any other bioimaging diagnostic systems, enriched by clinical phenotypes used as outcome labels or additional descriptors. In this study, we introduce a machine learning framework for automatic image interpretation that combines the current pattern recognition approach (“radiomics”) with Deep Learning (DL). As a first application in cancer bioimaging, we apply the framework for prognosis of locoregional recurrence in head and neck squamous cell carcinoma (N=298)from Computed Tomography (CT)and Positron Emission Tomography (PET)imaging. The DL architecture is composed of two parallel cascades of Convolutional Neural Network (CNN)layers merging in a softmax classification layer. The network is first pretrained on head and neck tumor stage diagnosis, then fine-tuned on the prognostic task by internal transfer learning. In parallel, radiomics features (e.g., shape of the tumor mass, texture and pixels intensity statistics)are derived by predefined feature extractors on the CT/PET pairs. We compare and mix deep learning and radiomics features into a unifying classification pipeline (RADLER), where model selection and evaluation are based on a data analysis plan developed in the MAQC initiative for reproducible biomarkers. On the multimodal CT/PET cancer dataset, the mixed deep learning/radiomics approach is more accurate than using only one feature type, or image mode. Further, RADLER significantly improves over published results on the same data.",8.0,2019,10.1109/CIBCB.2019.8791473,pipelines
68,Combining Superpixels and Deep Learning Approaches to Segment Active Organs in Metastatic Breast Cancer PET Images,C. Fourcade; L. Ferrer; G. Santini; N. Moreau; C. Rousseau; M. Lacombe; C. Guillerminet; M. Colombié; M. Campone; D. Mateus; M. Rubeaux,"Semi-automatic measurements are performed on <sup>18</sup>FDG PET-CT images to monitor the evolution of metastatic sites in the clinical follow-up of metastatic breast cancer patients. Apart from being time-consuming and prone to subjective approximation, semi-automatic tools cannot make the difference between cancerous regions and active organs, presenting a high <sup>18</sup>FDG uptake.In this work, we combine a deep learning-based approach with a superpixel segmentation method to segment the main active organs (brain, heart, bladder) from full-body PET images. In particular, we integrate a superpixel SLIC algorithm at different levels of a convolutional network. Results are compared with a deep learning segmentation network alone. The methods are cross-validated on full-body PET images of 36 patients and tested on the acquisitions of 24 patients from a different study center, in the context of the ongoing EPICUREseinmeta study. The similarity between the manually defined organ masks and the results is evaluated with the Dice score. Moreover, the amount of false positives is evaluated through the positive predictive value (PPV).According to the computed Dice scores, all approaches allow to accurately segment the target organs. However, the networks integrating superpixels are better suited to transfer knowledge across datasets acquired on multiple sites (domain adaptation) and are less likely to segment structures outside of the target organs, according to the PPV.Hence, combining deep learning with superpixels allows to segment organs presenting a high <sup>18</sup>FDG uptake on PET images without selecting cancerous lesion, and thus improves the precision of the semi-automatic tools monitoring the evolution of breast cancer metastasis.Clinical relevance— We demonstrate the utility of combining deep learning and superpixel segmentation methods to accurately find the contours of active organs from metastatic breast cancer images, to different dataset distributions.",4.0,2020,10.1109/EMBC44109.2020.9175683,lesions
68,Combining Superpixels and Deep Learning Approaches to Segment Active Organs in Metastatic Breast Cancer PET Images,C. Fourcade; L. Ferrer; G. Santini; N. Moreau; C. Rousseau; M. Lacombe; C. Guillerminet; M. Colombié; M. Campone; D. Mateus; M. Rubeaux,"Semi-automatic measurements are performed on <sup>18</sup>FDG PET-CT images to monitor the evolution of metastatic sites in the clinical follow-up of metastatic breast cancer patients. Apart from being time-consuming and prone to subjective approximation, semi-automatic tools cannot make the difference between cancerous regions and active organs, presenting a high <sup>18</sup>FDG uptake.In this work, we combine a deep learning-based approach with a superpixel segmentation method to segment the main active organs (brain, heart, bladder) from full-body PET images. In particular, we integrate a superpixel SLIC algorithm at different levels of a convolutional network. Results are compared with a deep learning segmentation network alone. The methods are cross-validated on full-body PET images of 36 patients and tested on the acquisitions of 24 patients from a different study center, in the context of the ongoing EPICUREseinmeta study. The similarity between the manually defined organ masks and the results is evaluated with the Dice score. Moreover, the amount of false positives is evaluated through the positive predictive value (PPV).According to the computed Dice scores, all approaches allow to accurately segment the target organs. However, the networks integrating superpixels are better suited to transfer knowledge across datasets acquired on multiple sites (domain adaptation) and are less likely to segment structures outside of the target organs, according to the PPV.Hence, combining deep learning with superpixels allows to segment organs presenting a high <sup>18</sup>FDG uptake on PET images without selecting cancerous lesion, and thus improves the precision of the semi-automatic tools monitoring the evolution of breast cancer metastasis.Clinical relevance— We demonstrate the utility of combining deep learning and superpixel segmentation methods to accurately find the contours of active organs from metastatic breast cancer images, to different dataset distributions.",4.0,2020,10.1109/EMBC44109.2020.9175683,three-dimensional displays
68,Combining Superpixels and Deep Learning Approaches to Segment Active Organs in Metastatic Breast Cancer PET Images,C. Fourcade; L. Ferrer; G. Santini; N. Moreau; C. Rousseau; M. Lacombe; C. Guillerminet; M. Colombié; M. Campone; D. Mateus; M. Rubeaux,"Semi-automatic measurements are performed on <sup>18</sup>FDG PET-CT images to monitor the evolution of metastatic sites in the clinical follow-up of metastatic breast cancer patients. Apart from being time-consuming and prone to subjective approximation, semi-automatic tools cannot make the difference between cancerous regions and active organs, presenting a high <sup>18</sup>FDG uptake.In this work, we combine a deep learning-based approach with a superpixel segmentation method to segment the main active organs (brain, heart, bladder) from full-body PET images. In particular, we integrate a superpixel SLIC algorithm at different levels of a convolutional network. Results are compared with a deep learning segmentation network alone. The methods are cross-validated on full-body PET images of 36 patients and tested on the acquisitions of 24 patients from a different study center, in the context of the ongoing EPICUREseinmeta study. The similarity between the manually defined organ masks and the results is evaluated with the Dice score. Moreover, the amount of false positives is evaluated through the positive predictive value (PPV).According to the computed Dice scores, all approaches allow to accurately segment the target organs. However, the networks integrating superpixels are better suited to transfer knowledge across datasets acquired on multiple sites (domain adaptation) and are less likely to segment structures outside of the target organs, according to the PPV.Hence, combining deep learning with superpixels allows to segment organs presenting a high <sup>18</sup>FDG uptake on PET images without selecting cancerous lesion, and thus improves the precision of the semi-automatic tools monitoring the evolution of breast cancer metastasis.Clinical relevance— We demonstrate the utility of combining deep learning and superpixel segmentation methods to accurately find the contours of active organs from metastatic breast cancer images, to different dataset distributions.",4.0,2020,10.1109/EMBC44109.2020.9175683,breast cancer
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,local recurrence
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,head
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,head and neck cancer
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,three-dimensional displays
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,neck
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,feature extraction
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,redundancy
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,predictive models
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,multi-view convolutional neural network
69,Multi-Modality and Multi-View 2D CNN to Predict Locoregional Recurrence in Head & Neck Cancer,J. Guo; R. Wang; Z. Zhou; K. Wang; R. Xu; J. Wang,"Locoregional recurrence (LRR) remains one of leading causes in head and neck (H&N) cancer treatment failure despite the advancement of multidisciplinary management. Accurately predicting LRR in early stage can help physicians make an optimal personalized treatment strategy. In this study, we propose an end-to-end multi-modality and multi-view convolutional neural network model (mMmV-CNN) for LRR prediction in H&N cancer. In mMmV, a dimension reduction operator is designed, projecting the 3D volume onto 2D images in different directions, and a multi-view strategy is used to replace the original 3D method, which reduces the complexity of the algorithm while preserving important 3D information. Meanwhile, multi-modal data is used for the classification by making full use of the complementary information from cross modality data. Furthermore, we design a multi-modality deep neural network which is trained in an end-to-end manner and jointly optimize the deep features of CT, PET and clinical features. A H&N dataset which consists of 206 patients was used to evaluate the performance. Experimental results demonstrated that mMm V-CNN can obtain an AUC value of 0.81 and outperform a state of the art CNN-based method.",,2021,10.1109/IJCNN52387.2021.9533703,outcome prediction
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,heterogeneous data
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,brain modeling
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,neurons
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,deep neural network (dnn)
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,feature extraction
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,electroencephalography
70,Design and Development of Integrated Deep Convolution Neural Network Approach for Handling Heterogeneous Medical Data,A. J. Shikalgar; S. Sonavane,"In recent years, Neural Network (NN) is developed as an optimal technique for the prediction of tasks which include image classification, speech recognition and also useful in biomedical analysis. Biomedical data consists of diverse modalities like X-ray, CT, MRI, PET, EEG and ECG signals. There are several NNs techniques such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Deep Neural Network (DNN) that are used for various prediction applications in handling multimodal heterogeneous data. However, learning and prediction of such multimodal data limits the scope of existing neural techniques. One of the limitations of ANN observes that addition of layer cause back propagation stuck in local minima and reduction of learning speed. Where, DNN causes higher computational complexity in training the features which are based on contrastive divergence. In CNN there is loses of spatial information due to the weight factors variation. To overcome these issues, this paper proposes a novel learning technique in which the weight factor of DNN is integrated with CNN for handling multimodal heterogeneous data. The simulation results prove that the integrated learning technique (IDCNN) obtains better learning performance than ANN, CNN and DNN models in terms of Root Mean Square Error (RMSE) and efficiency in terms of cross entropy.",,2019,10.1109/AICAI.2019.8701358,convolutional neural network (cnn)
71,Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,2019,10.1109/WiSPNET45539.2019.9032747,conjugate gradient
71,Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,2019,10.1109/WiSPNET45539.2019.9032747,performance metrics
71,Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,2019,10.1109/WiSPNET45539.2019.9032747,gradient descent
71,Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,2019,10.1109/WiSPNET45539.2019.9032747,compression
71,Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,2019,10.1109/WiSPNET45539.2019.9032747,neurons
71,Computed Tomography Medical Image Compression using Conjugate Gradient,G. S. Rao; S. S. Rani; B. P. Rao,"Image compression which is a subset of data compression plays a crucial task in medical field. The medical images like CT, MRI, PET scan and X-Ray imagery which is a huge data, should be compressed to facilitate storage capacity without losing its details to diagnose the patient correctly. Now a days artificial neural network is being widely researched in the field of image processing. This paper examines the performance of a feed forward artificial neural network with learning algorithm as conjugate gradient. This work performs a comparison between Conjugate gradient technique and Gradient Descent algorithm is done. MSE and PSNR are used as quality metrics. The investigation is carried on CT scan of lower abdomen medical image.",,2019,10.1109/WiSPNET45539.2019.9032747,neural network
72,Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",16.0,2018,10.1109/TMI.2018.2790962,bones
72,Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",16.0,2018,10.1109/TMI.2018.2790962,low-rank approximation
72,Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",16.0,2018,10.1109/TMI.2018.2790962,attenuation
72,Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",16.0,2018,10.1109/TMI.2018.2790962,ct prediction
72,Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",16.0,2018,10.1109/TMI.2018.2790962,knn regression
72,Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,W. Yang; L. Zhong; Y. Chen; L. Lin; Z. Lu; S. Liu; Y. Wu; Q. Feng; W. Chen,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",16.0,2018,10.1109/TMI.2018.2790962,nonlinear descriptor
73,A deformable model-based system for 3D analysis and visualization of tumor in PET/CT images,J. Landre; S. Lebonvallet; S. Ruan; Li Xiaobing; Qiu Tianshuang; F. Brunotte,"This paper presents a tumor detecting system that allows interactive 3D tumor visualization and tumor volume measurements. An improved level set method is proposed to automatically segment the tumor images slice by slice. PET images are used to detect the tumor while CT images make a 3D representation of the patient's body possible. An initial slice with a seed within the tumor is firstly chosen by the operator. The system then performs automatically the tumor volume segmentation that allows the clinician to visualize the tumor, to measure it and to evaluate the best medical treatment adapted to the patient.",,2008,10.1109/IEMBS.2008.4649867,None
74,Medical image understanding and Computational Anatomy,Y. Masutani,"By the rapid development of medical imaging equipments such as X-ray CT, MRI, PET, etc., data quantity yielded in hospitals is still explosively increasing. For instance, it often reaches to more than 1000 slices of X-ray CT and MRI images in a single examination. This is mainly due to improvement in spatial and temporal resolution of images, and acquisition of multi-modal information from various imaging physics. In contrast to such rich information, image-reading workload for radiologists becomes extremely heavier. In some cases, radiologists can take only less than one second per slice image in average and oversights of abnormalities may possibly occur. Therefore, full or partial automation of such image-reading tasks is a natural demand. Generally, image-reading task includes visual search of abnormalities in images such as tumors, deformation or degeneration of tissues. The computational support technology for assisting radiologists, so-called “Computer-Assisted Diagnosis/Detection (CAD)”, based on image analysis and pattern recognition have a long history over 30 years. In the early phases of CAD technology development, simple schemes such as search of round-shaped structures were employed to obtain limited success due to lack of anatomical information. Recently, information of shape and structure of the inner organs as image analysis priors becomes indispensable for reliable results. That is, computational image understanding with anatomical knowledge is a certain standard of medical image analysis. Especially, thanks to machine learning approaches with high computational powers and large database, studies on statistical analysis and mathematical description of anatomical structures opened a new discipline called “Computational Anatomy”. In this lecture, several examples of state-of-the-art techniques and systems are introduced and discussed with the practical problems in clinical situations.",,2015,10.1109/IWCIA.2015.7449449,urban areas
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,bones
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,educational institutions
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,principal component analysis
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,mathematical model
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,regularizer based on pca
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,landmarks
75,Non-rigid registration guided by landmarks and learning,J. Eckl; V. Daum; J. Hornegger; K. M. Pohl,"Registration methods frequently rely on prior information in order to generate anatomical meaningful transformations between medical scans. In this paper, we propose a novel intensity based non-rigid registration framework, which is guided by landmarks and a regularizer based on Principle Component Analysis (PCA). Unlike existing methods in this domain, the computational complexity of our approach reduces with the number of landmarks. Furthermore, our PCA is invariant to translations. The additional regularizer is based on the outcome of this PCA. We register a skull CT scan to MR scans aquired by a MR/PET hybrid scanner. This aligned CT scan can then be used to gain an attenuation map for PET reconstruction. As a result we have a Dice coefficient for bone areas at 0.71 and a Dice coefficient for bone and soft issue areas at 0.97.",,2012,10.1109/ISBI.2012.6235645,non-rigid registration
76,Hybrid Medical Image Fusion based on Fast Filtering and Wavelet Analysis,S. A. El-Masry; S. Y. El-Mashad; N. E. El-Attar; W. A. Awad,"Within medical imaging, there are various modalities of medical images like CT, X-rays, MRI and other modalities that provide information about a human body in different ways. Each modality has distinctive characteristics that provide various sources of information. Therefore, there are some problems like image comparison such as CT/PET, CT /MRI, and MRI/ PET were usually meet by the clinical treatment and diagnosis. Hence the need to combine the different images' information and this process is known as `medical image fusion'. In this paper, two techniques for the `medical image fusion' are introduced. The first proposed fusion technique is the combination of the fast filtering with the discrete wavelet transform `DWT' methods for overcoming the low spatial resolution fused image provided by DWT and preserve the source images' salient features. Where we used the fast filtering method procedures for combining the corresponding `low-frequency coefficients' to maintain the `salient features' of the initial images, and the maximum rule with the high-frequency coefficients which lead getting better the resultant image contrast. The second proposed technique is the combination of fast filtering with stationary wavelet transform (SWT) methods, where `SWT' has the shift-invariant property which enables to overcome the shift-variance DWT's drawback. The performance of the fused output is tested and compared with five of the common fusion methods like the Gradient pyramid, Contrast pyramid, DWT, Fast Filtering, and SWT techniques, using performance parameters: E, SNR, SD, and PSNR.",,2019,10.1109/ICICIS46948.2019.9014677,discrete wavelet transforms
76,Hybrid Medical Image Fusion based on Fast Filtering and Wavelet Analysis,S. A. El-Masry; S. Y. El-Mashad; N. E. El-Attar; W. A. Awad,"Within medical imaging, there are various modalities of medical images like CT, X-rays, MRI and other modalities that provide information about a human body in different ways. Each modality has distinctive characteristics that provide various sources of information. Therefore, there are some problems like image comparison such as CT/PET, CT /MRI, and MRI/ PET were usually meet by the clinical treatment and diagnosis. Hence the need to combine the different images' information and this process is known as `medical image fusion'. In this paper, two techniques for the `medical image fusion' are introduced. The first proposed fusion technique is the combination of the fast filtering with the discrete wavelet transform `DWT' methods for overcoming the low spatial resolution fused image provided by DWT and preserve the source images' salient features. Where we used the fast filtering method procedures for combining the corresponding `low-frequency coefficients' to maintain the `salient features' of the initial images, and the maximum rule with the high-frequency coefficients which lead getting better the resultant image contrast. The second proposed technique is the combination of fast filtering with stationary wavelet transform (SWT) methods, where `SWT' has the shift-invariant property which enables to overcome the shift-variance DWT's drawback. The performance of the fused output is tested and compared with five of the common fusion methods like the Gradient pyramid, Contrast pyramid, DWT, Fast Filtering, and SWT techniques, using performance parameters: E, SNR, SD, and PSNR.",,2019,10.1109/ICICIS46948.2019.9014677,‘wavelet transform’
76,Hybrid Medical Image Fusion based on Fast Filtering and Wavelet Analysis,S. A. El-Masry; S. Y. El-Mashad; N. E. El-Attar; W. A. Awad,"Within medical imaging, there are various modalities of medical images like CT, X-rays, MRI and other modalities that provide information about a human body in different ways. Each modality has distinctive characteristics that provide various sources of information. Therefore, there are some problems like image comparison such as CT/PET, CT /MRI, and MRI/ PET were usually meet by the clinical treatment and diagnosis. Hence the need to combine the different images' information and this process is known as `medical image fusion'. In this paper, two techniques for the `medical image fusion' are introduced. The first proposed fusion technique is the combination of the fast filtering with the discrete wavelet transform `DWT' methods for overcoming the low spatial resolution fused image provided by DWT and preserve the source images' salient features. Where we used the fast filtering method procedures for combining the corresponding `low-frequency coefficients' to maintain the `salient features' of the initial images, and the maximum rule with the high-frequency coefficients which lead getting better the resultant image contrast. The second proposed technique is the combination of fast filtering with stationary wavelet transform (SWT) methods, where `SWT' has the shift-invariant property which enables to overcome the shift-variance DWT's drawback. The performance of the fused output is tested and compared with five of the common fusion methods like the Gradient pyramid, Contrast pyramid, DWT, Fast Filtering, and SWT techniques, using performance parameters: E, SNR, SD, and PSNR.",,2019,10.1109/ICICIS46948.2019.9014677,‘fast filtering’
76,Hybrid Medical Image Fusion based on Fast Filtering and Wavelet Analysis,S. A. El-Masry; S. Y. El-Mashad; N. E. El-Attar; W. A. Awad,"Within medical imaging, there are various modalities of medical images like CT, X-rays, MRI and other modalities that provide information about a human body in different ways. Each modality has distinctive characteristics that provide various sources of information. Therefore, there are some problems like image comparison such as CT/PET, CT /MRI, and MRI/ PET were usually meet by the clinical treatment and diagnosis. Hence the need to combine the different images' information and this process is known as `medical image fusion'. In this paper, two techniques for the `medical image fusion' are introduced. The first proposed fusion technique is the combination of the fast filtering with the discrete wavelet transform `DWT' methods for overcoming the low spatial resolution fused image provided by DWT and preserve the source images' salient features. Where we used the fast filtering method procedures for combining the corresponding `low-frequency coefficients' to maintain the `salient features' of the initial images, and the maximum rule with the high-frequency coefficients which lead getting better the resultant image contrast. The second proposed technique is the combination of fast filtering with stationary wavelet transform (SWT) methods, where `SWT' has the shift-invariant property which enables to overcome the shift-variance DWT's drawback. The performance of the fused output is tested and compared with five of the common fusion methods like the Gradient pyramid, Contrast pyramid, DWT, Fast Filtering, and SWT techniques, using performance parameters: E, SNR, SD, and PSNR.",,2019,10.1109/ICICIS46948.2019.9014677,filtering
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,ct
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,computer vision
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,traditional learning
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,surgery
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,feature extraction
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,cancer
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,learning systems
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,oesophagic cancer
77,Classification of oesophagic early-stage cancers: deep learning versus traditional learning approaches,J. Ferreira; I. Domingues; O. Sousa; I. L. Sampaio; J. A. M. Santos,"Esophageal cancer is a disease with a high prevalence which can be evaluated by a variety of imaging modalities. Computer vision techniques could provide a valuable help in the analysis of these images, for it would allow an enhancement in diagnostic and staging accuracies, a decrease in medical workflow time and preventing patients' loss of quality of life.Traditional learning techniques are frequently used in the biomedical imaging field, and deep learning algorithms are starting to see their rise in usage in this field as well. In this paper, both traditional and deep learning algorithms are applied on a dataset provided by Instituto Portugues de Oncologia (IPO) consisting of CT and three PET scans acquired at different treatment phases of 14 patients with oesophageal cancer.The main goal is to distinguish patients that need surgery from the ones that do not. The traditional learning method consisted of manually extracting the features and applying feature selection algorithms for further classification. Feature level and decision level fusion were also conducted. The deep learning method consisted of using convolutional neural networks to extract and classify the image features. Moreover, traditional and deep learning techniques were used simultaneously, where the features were extracted and selected by a pretrained network and classified using the traditional learning classifiers.Traditional Learning methods achieved 92.86% accuracy, while for feature extraction with deep learning followed by classification with a traditional classifier was able to reach 100% accuracy. The difference has, however, proven not to be statistically significant. In this way, for this particular problem and conditions, it can be said that traditional techniques are capable of achieving results as good as with deep learning.",1.0,2020,10.1109/BIBE50027.2020.00127,two dimensional displays
78,A Two-Level Dynamic Adaptive Network for Medical Image Fusion,W. Huang; H. Zhang; X. Quan; J. Wang,"The research of deep learning-based methods for image fusion has become a current hotspot. Medical image fusion with the problem of few samples also lacks a unified end-to-end model for the input of different modal pairs. In this article, we propose a two-level dynamic adaptive network for medical image fusion, which addresses the above two problems and provides a unified fusion framework to take the advantage of different modal pairs. Specifically, we develop a dynamic meta-learning method on task level, which achieves a dynamical meta-knowledge transfer from the heterogeneous task of multifocus image fusion to medical image fusion by dynamic convolution decomposition (DCD). Then, we provide an efficient adaptive fusion method on multimodal feature level, which uses dynamic attention mechanism and dynamic channel fusion mechanism to fuse features of different aspects. For model evaluation, we have done the qualitative and quantitative tests on the transferred multifocus deep network and verified its superior fusion performance. On this basis, the experiments are carried out on the public datasets of the two most commonly used modal pairs (computerized tomography (CT)-magnetic resonance imaging (MRI) and positron emission tomography (PET)-MRI) and show that our hierarchical model is superior to the state-of-the-art methods in terms of visual effects and quantitative measurement. Our code is publicly available at <uri>https://github.com/zhanglabNKU/TDAN</uri>.",,2022,10.1109/TIM.2022.3169546,fuses
78,A Two-Level Dynamic Adaptive Network for Medical Image Fusion,W. Huang; H. Zhang; X. Quan; J. Wang,"The research of deep learning-based methods for image fusion has become a current hotspot. Medical image fusion with the problem of few samples also lacks a unified end-to-end model for the input of different modal pairs. In this article, we propose a two-level dynamic adaptive network for medical image fusion, which addresses the above two problems and provides a unified fusion framework to take the advantage of different modal pairs. Specifically, we develop a dynamic meta-learning method on task level, which achieves a dynamical meta-knowledge transfer from the heterogeneous task of multifocus image fusion to medical image fusion by dynamic convolution decomposition (DCD). Then, we provide an efficient adaptive fusion method on multimodal feature level, which uses dynamic attention mechanism and dynamic channel fusion mechanism to fuse features of different aspects. For model evaluation, we have done the qualitative and quantitative tests on the transferred multifocus deep network and verified its superior fusion performance. On this basis, the experiments are carried out on the public datasets of the two most commonly used modal pairs (computerized tomography (CT)-magnetic resonance imaging (MRI) and positron emission tomography (PET)-MRI) and show that our hierarchical model is superior to the state-of-the-art methods in terms of visual effects and quantitative measurement. Our code is publicly available at <uri>https://github.com/zhanglabNKU/TDAN</uri>.",,2022,10.1109/TIM.2022.3169546,feature extraction
78,A Two-Level Dynamic Adaptive Network for Medical Image Fusion,W. Huang; H. Zhang; X. Quan; J. Wang,"The research of deep learning-based methods for image fusion has become a current hotspot. Medical image fusion with the problem of few samples also lacks a unified end-to-end model for the input of different modal pairs. In this article, we propose a two-level dynamic adaptive network for medical image fusion, which addresses the above two problems and provides a unified fusion framework to take the advantage of different modal pairs. Specifically, we develop a dynamic meta-learning method on task level, which achieves a dynamical meta-knowledge transfer from the heterogeneous task of multifocus image fusion to medical image fusion by dynamic convolution decomposition (DCD). Then, we provide an efficient adaptive fusion method on multimodal feature level, which uses dynamic attention mechanism and dynamic channel fusion mechanism to fuse features of different aspects. For model evaluation, we have done the qualitative and quantitative tests on the transferred multifocus deep network and verified its superior fusion performance. On this basis, the experiments are carried out on the public datasets of the two most commonly used modal pairs (computerized tomography (CT)-magnetic resonance imaging (MRI) and positron emission tomography (PET)-MRI) and show that our hierarchical model is superior to the state-of-the-art methods in terms of visual effects and quantitative measurement. Our code is publicly available at <uri>https://github.com/zhanglabNKU/TDAN</uri>.",,2022,10.1109/TIM.2022.3169546,convolution
78,A Two-Level Dynamic Adaptive Network for Medical Image Fusion,W. Huang; H. Zhang; X. Quan; J. Wang,"The research of deep learning-based methods for image fusion has become a current hotspot. Medical image fusion with the problem of few samples also lacks a unified end-to-end model for the input of different modal pairs. In this article, we propose a two-level dynamic adaptive network for medical image fusion, which addresses the above two problems and provides a unified fusion framework to take the advantage of different modal pairs. Specifically, we develop a dynamic meta-learning method on task level, which achieves a dynamical meta-knowledge transfer from the heterogeneous task of multifocus image fusion to medical image fusion by dynamic convolution decomposition (DCD). Then, we provide an efficient adaptive fusion method on multimodal feature level, which uses dynamic attention mechanism and dynamic channel fusion mechanism to fuse features of different aspects. For model evaluation, we have done the qualitative and quantitative tests on the transferred multifocus deep network and verified its superior fusion performance. On this basis, the experiments are carried out on the public datasets of the two most commonly used modal pairs (computerized tomography (CT)-magnetic resonance imaging (MRI) and positron emission tomography (PET)-MRI) and show that our hierarchical model is superior to the state-of-the-art methods in terms of visual effects and quantitative measurement. Our code is publicly available at <uri>https://github.com/zhanglabNKU/TDAN</uri>.",,2022,10.1109/TIM.2022.3169546,task analysis
78,A Two-Level Dynamic Adaptive Network for Medical Image Fusion,W. Huang; H. Zhang; X. Quan; J. Wang,"The research of deep learning-based methods for image fusion has become a current hotspot. Medical image fusion with the problem of few samples also lacks a unified end-to-end model for the input of different modal pairs. In this article, we propose a two-level dynamic adaptive network for medical image fusion, which addresses the above two problems and provides a unified fusion framework to take the advantage of different modal pairs. Specifically, we develop a dynamic meta-learning method on task level, which achieves a dynamical meta-knowledge transfer from the heterogeneous task of multifocus image fusion to medical image fusion by dynamic convolution decomposition (DCD). Then, we provide an efficient adaptive fusion method on multimodal feature level, which uses dynamic attention mechanism and dynamic channel fusion mechanism to fuse features of different aspects. For model evaluation, we have done the qualitative and quantitative tests on the transferred multifocus deep network and verified its superior fusion performance. On this basis, the experiments are carried out on the public datasets of the two most commonly used modal pairs (computerized tomography (CT)-magnetic resonance imaging (MRI) and positron emission tomography (PET)-MRI) and show that our hierarchical model is superior to the state-of-the-art methods in terms of visual effects and quantitative measurement. Our code is publicly available at <uri>https://github.com/zhanglabNKU/TDAN</uri>.",,2022,10.1109/TIM.2022.3169546,meta-learning
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,support vector machines
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,random forest
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,linear regression
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,gradient boosting algorithm
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,boosting
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,alzheimer's disease
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,alzheimer's disease symptoms
79,Diagnosis of Alzheimer's Disease Using Machine Learning,P. Lodha; A. Talele; K. Degaonkar,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.",11.0,2018,10.1109/ICCUBEA.2018.8697386,neural network
80,Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",1.0,2018,10.1109/NSSMIC.2018.8824292,lung cancer
80,Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",1.0,2018,10.1109/NSSMIC.2018.8824292,lesions
80,Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",1.0,2018,10.1109/NSSMIC.2018.8824292,standards
80,Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",1.0,2018,10.1109/NSSMIC.2018.8824292,lesion detection
80,Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",1.0,2018,10.1109/NSSMIC.2018.8824292,cancer
80,Improving Lung Lesion Detection in Low Dose Positron Emission Tomography Images Using Machine Learning,Y. Nai; J. D. Schaefferkoetter; D. Fakhry-Darian; M. Conti; X. Shi; D. W. Townsend; A. K. Sinha; I. Tham; D. C. Alexander; A. Reilhac,"Lung cancer suffers from poor prognosis, leading to high death rates. Combined PET/CT improves lung lesion detection but requires low dose protocols for frequent disease screening and monitoring. In this study, we investigate the feasibility of using machine learning to improve low dose PET images to standard dose, high-quality images for better lesion detection at low dose PET scans. We employ image quality transfer (IQT), which is a machine learning algorithm that uses patch-regression to map parameters from low to high-quality images e.g. enhancing resolution or information content. We acquired 20 standard dose PET images and simulated low dose PET images with 9 different count levels from the standard dose PET images. For each count levels, 10 pairs of standard dose PET images with one simulated low dose PET images were used to train linear, single non-linear regression tree, and random regression-forest models for IQT. The models were then used to estimate standard dose images from low dose images for each count levels for 10 different subjects. Improvement in image quality and lesion detection could be observed in the images estimated from the low dose images using IQT. Among the models employed, the regression tree model produced the best estimates of standard dose PET images. An average bias of less than 20% in SUV<sub>mean</sub> of 25 lesions in the estimated images from the standard dose PET images can be obtained down to 7.5 × 10<sup>6</sup> counts. Overall, despite the increase in bias, the improvement in image quality shows the potential of IQT in improving the accuracy in lesion detection.",1.0,2018,10.1109/NSSMIC.2018.8824292,lung
81,Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",2.0,2022,10.1109/TMI.2021.3113365,ct
81,Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",2.0,2022,10.1109/TMI.2021.3113365,noise reduction
81,Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",2.0,2022,10.1109/TMI.2021.3113365,low dose
81,Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",2.0,2022,10.1109/TMI.2021.3113365,task analysis
81,Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",2.0,2022,10.1109/TMI.2021.3113365,generative adversarial networks
81,Content-Noise Complementary Learning for Medical Image Denoising,M. Geng; X. Meng; J. Yu; L. Zhu; L. Jin; Z. Jiang; B. Qiu; H. Li; H. Kong; J. Yuan; K. Yang; H. Shan; H. Han; Z. Yang; Q. Ren; Y. Lu,"Medical imaging denoising faces great challenges, yet is in great demand. With its distinctive characteristics, medical imaging denoising in the image domain requires innovative deep learning strategies. In this study, we propose a simple yet effective strategy, the content-noise complementary learning (CNCL) strategy, in which two deep learning predictors are used to learn the respective content and noise of the image dataset complementarily. A medical image denoising pipeline based on the CNCL strategy is presented, and is implemented as a generative adversarial network, where various representative networks (including U-Net, DnCNN, and SRDenseNet) are investigated as the predictors. The performance of these implemented models has been validated on medical imaging datasets including CT, MR, and PET. The results show that this strategy outperforms state-of-the-art denoising algorithms in terms of visual quality and quantitative metrics, and the strategy demonstrates a robust generalization capability. These findings validate that this simple yet effective strategy demonstrates promising potential for medical image denoising tasks, which could exert a clinical impact in the future. Code is available at: <uri>https://github.com/gengmufeng/CNCL-denoising</uri>.",2.0,2022,10.1109/TMI.2021.3113365,mr
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,convolutional neural network
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,heart
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,stacking noise reduction self-coding network
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,heart segmentation
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,itti model
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,grabcut algorithm
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,feature extraction
82,Automatic Segmentation Algorithm of Ultrasound Heart Image Based on Convolutional Neural Network and Image Saliency,H. Liu; W. Chu; H. Wang,"The emergence of 4D heart images makes the data volume of the images multiply. It is more urgent to require an effective and fast segmentation algorithm. Therefore, a heart image can be accurately segmented from a large amount of image data and an area of interest can be extracted The segmentation algorithm is very necessary. Based on the segmentation and recognition of medical images, this paper proposes a neural network and image saliency based on the obvious difference between the heart image and other tissues in the slice, and the high similarity between adjacent slices in the CT image sequence. Fully automatic segmentation algorithm and 3D visual reconstruction is the segmented heart image. Convolutional neural network is a special deep neural network model of artificial intelligence. Its connections between neurons are not fully connected. The weights of connections between certain neurons in the same layer are shared, and the network model is reduced. The complexity reduces the number of weights. The use of visual saliency techniques to achieve cardiac segmentation based on CT images. An image saliency detection algorithm is adopted to introduce the image segmentation algorithm based on the saliency technique. In this paper, considering the PET image as grayscale image with low resolution, an improved Itti model and an improved GrabCut image segmentation algorithm are proposed to solve the problem of the original algorithm in grayscale image. At the same time, the operation steps of the user division area are cancelled, and the automatic processing is realized, and the running time of the algorithm is improved while optimizing the image segmentation effect. The convolutional neural network is constructed to realize the positioning function of the heart in the image. The original cardiac CT image is cropped by the positioning result, and some non-target areas are removed. A stacking noise reduction self-coding network is constructed, and the network is manually segmented. Training, realize the classification and recognition of the pixels belonging to the heart tissue in the CT image of the heart, and finally realize the segmentation of the heart image based on the classification result. The results of the above segmentation algorithm are quantitatively evaluated and analyzed with the artificial segmentation results, and the segmentation results are visually reconstructed by surface rendering and volume rendering. The algorithm has better accuracy, reliability and higher. The segmentation efficiency is more simplified for user operations.",3.0,2020,10.1109/ACCESS.2020.2989819,visualization
83,Deep learning for MRI-based CT synthesis: a comparison of MRI sequences and neural network architectures,A. Larroza; L. Moliner; J. M. Álvarez-Gómez; S. Oliver; H. Espinós-Morató; M. Vergara-Díaz; M. J. Rodríguez-Álvarez,"Synthetic computed tomography (CT) images derived from magnetic resonance images (MRI) are of interest for radiotherapy planning and positron emission tomography (PET) attenuation correction. In recent years, deep learning implementations have demonstrated improvement over atlas-based and segmentation-based methods. Nevertheless, several open questions remain to be addressed, such as which is the best of MRI sequences and neural network architectures. In this work, we compared the performance of different combinations of two common MRI sequences (T1- and T2-weighted), and three state-of-the-art neural networks designed for medical image processing (Vnet, HighRes3dNet and ScaleNet). The experiments were conducted on brain datasets from a public database. Our results suggest that T1 images perform better than T2, but the results further improve when combining both sequences. The lowest mean average error over the entire head (MAE = 101.76 ± 10.4 HU) was achieved combining T1 and T2 scans with HighRes3dNet. All tested deep learning models achieved significantly lower MAE (p <; 0.01) than a well-known atlas-based method.",1.0,2019,10.1109/NSS/MIC42101.2019.9060051,head
83,Deep learning for MRI-based CT synthesis: a comparison of MRI sequences and neural network architectures,A. Larroza; L. Moliner; J. M. Álvarez-Gómez; S. Oliver; H. Espinós-Morató; M. Vergara-Díaz; M. J. Rodríguez-Álvarez,"Synthetic computed tomography (CT) images derived from magnetic resonance images (MRI) are of interest for radiotherapy planning and positron emission tomography (PET) attenuation correction. In recent years, deep learning implementations have demonstrated improvement over atlas-based and segmentation-based methods. Nevertheless, several open questions remain to be addressed, such as which is the best of MRI sequences and neural network architectures. In this work, we compared the performance of different combinations of two common MRI sequences (T1- and T2-weighted), and three state-of-the-art neural networks designed for medical image processing (Vnet, HighRes3dNet and ScaleNet). The experiments were conducted on brain datasets from a public database. Our results suggest that T1 images perform better than T2, but the results further improve when combining both sequences. The lowest mean average error over the entire head (MAE = 101.76 ± 10.4 HU) was achieved combining T1 and T2 scans with HighRes3dNet. All tested deep learning models achieved significantly lower MAE (p <; 0.01) than a well-known atlas-based method.",1.0,2019,10.1109/NSS/MIC42101.2019.9060051,convolution
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,dictionary learning (dl)
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,transform learning
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,structured models
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,compressed sensing (cs)
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,multilayer models
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,data models
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,mathematical model
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,efficient algorithms
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,sparse and low-rank models
84,Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning,S. Ravishankar; J. C. Ye; J. A. Fessler,"The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.",62.0,2020,10.1109/JPROC.2019.2936204,nonconvex optimization
85,Deep Learning-Assisted Whole-Body Voxel-Based Internal Dosimetry,A. Akhavanallaf; I. Shiri; H. Arabi; H. Zaidi,"We propose a novel methodology to conduct whole-body organ-level dosimetry taking into account the heterogeneity of activity distribution as well as patient-specific anatomy using Monte Carlo (MC) simulations and machine learning algorithms. We extended the core idea of the voxel-scale MIRD approach that utilizes a single S-value kernel for internal dosimetry by generating specific S-value kernels corresponding to patient-specific anatomy. In this context, we employed deep learning algorithms to predict the deposited energy distribution, representing the S-value kernel. The training dataset consists of density maps obtained from CT images along with the ground-truth dose distribution obtained from MC simulations. Accordingly, whole-body dose maps are constructed through convolving specific S-values with the activity map. The Deep Neural Network (DNN) predicted dose map was compared with the reference (Monte Carlo-based) and two MIRD-based methods, including single-voxel S-value (SSV) and multiple voxel S-value (MSV) approaches. The Mean Relative Absolute Errors (MRAE) of the estimated absorbed dose between DNN, MSV, and SSV against reference MC simulations were 2.6%, 3%, and 49%, respectively. MRAEs of 23.5%, 5.1%, and 21.8% were obtained between the proposed method and MSV, SSV, and Olinda dosimetry package in organ-level dosimetry, respectively. The proposed internal dosimetry technique exhibited comparable performance to the direct Monte Carlo approach while overcoming the computational burden limitation of MC simulations.",,2020,10.1109/NSS/MIC42677.2020.9507983,computational modeling
85,Deep Learning-Assisted Whole-Body Voxel-Based Internal Dosimetry,A. Akhavanallaf; I. Shiri; H. Arabi; H. Zaidi,"We propose a novel methodology to conduct whole-body organ-level dosimetry taking into account the heterogeneity of activity distribution as well as patient-specific anatomy using Monte Carlo (MC) simulations and machine learning algorithms. We extended the core idea of the voxel-scale MIRD approach that utilizes a single S-value kernel for internal dosimetry by generating specific S-value kernels corresponding to patient-specific anatomy. In this context, we employed deep learning algorithms to predict the deposited energy distribution, representing the S-value kernel. The training dataset consists of density maps obtained from CT images along with the ground-truth dose distribution obtained from MC simulations. Accordingly, whole-body dose maps are constructed through convolving specific S-values with the activity map. The Deep Neural Network (DNN) predicted dose map was compared with the reference (Monte Carlo-based) and two MIRD-based methods, including single-voxel S-value (SSV) and multiple voxel S-value (MSV) approaches. The Mean Relative Absolute Errors (MRAE) of the estimated absorbed dose between DNN, MSV, and SSV against reference MC simulations were 2.6%, 3%, and 49%, respectively. MRAEs of 23.5%, 5.1%, and 21.8% were obtained between the proposed method and MSV, SSV, and Olinda dosimetry package in organ-level dosimetry, respectively. The proposed internal dosimetry technique exhibited comparable performance to the direct Monte Carlo approach while overcoming the computational burden limitation of MC simulations.",,2020,10.1109/NSS/MIC42677.2020.9507983,monte carlo
85,Deep Learning-Assisted Whole-Body Voxel-Based Internal Dosimetry,A. Akhavanallaf; I. Shiri; H. Arabi; H. Zaidi,"We propose a novel methodology to conduct whole-body organ-level dosimetry taking into account the heterogeneity of activity distribution as well as patient-specific anatomy using Monte Carlo (MC) simulations and machine learning algorithms. We extended the core idea of the voxel-scale MIRD approach that utilizes a single S-value kernel for internal dosimetry by generating specific S-value kernels corresponding to patient-specific anatomy. In this context, we employed deep learning algorithms to predict the deposited energy distribution, representing the S-value kernel. The training dataset consists of density maps obtained from CT images along with the ground-truth dose distribution obtained from MC simulations. Accordingly, whole-body dose maps are constructed through convolving specific S-values with the activity map. The Deep Neural Network (DNN) predicted dose map was compared with the reference (Monte Carlo-based) and two MIRD-based methods, including single-voxel S-value (SSV) and multiple voxel S-value (MSV) approaches. The Mean Relative Absolute Errors (MRAE) of the estimated absorbed dose between DNN, MSV, and SSV against reference MC simulations were 2.6%, 3%, and 49%, respectively. MRAEs of 23.5%, 5.1%, and 21.8% were obtained between the proposed method and MSV, SSV, and Olinda dosimetry package in organ-level dosimetry, respectively. The proposed internal dosimetry technique exhibited comparable performance to the direct Monte Carlo approach while overcoming the computational burden limitation of MC simulations.",,2020,10.1109/NSS/MIC42677.2020.9507983,monte carlo methods
85,Deep Learning-Assisted Whole-Body Voxel-Based Internal Dosimetry,A. Akhavanallaf; I. Shiri; H. Arabi; H. Zaidi,"We propose a novel methodology to conduct whole-body organ-level dosimetry taking into account the heterogeneity of activity distribution as well as patient-specific anatomy using Monte Carlo (MC) simulations and machine learning algorithms. We extended the core idea of the voxel-scale MIRD approach that utilizes a single S-value kernel for internal dosimetry by generating specific S-value kernels corresponding to patient-specific anatomy. In this context, we employed deep learning algorithms to predict the deposited energy distribution, representing the S-value kernel. The training dataset consists of density maps obtained from CT images along with the ground-truth dose distribution obtained from MC simulations. Accordingly, whole-body dose maps are constructed through convolving specific S-values with the activity map. The Deep Neural Network (DNN) predicted dose map was compared with the reference (Monte Carlo-based) and two MIRD-based methods, including single-voxel S-value (SSV) and multiple voxel S-value (MSV) approaches. The Mean Relative Absolute Errors (MRAE) of the estimated absorbed dose between DNN, MSV, and SSV against reference MC simulations were 2.6%, 3%, and 49%, respectively. MRAEs of 23.5%, 5.1%, and 21.8% were obtained between the proposed method and MSV, SSV, and Olinda dosimetry package in organ-level dosimetry, respectively. The proposed internal dosimetry technique exhibited comparable performance to the direct Monte Carlo approach while overcoming the computational burden limitation of MC simulations.",,2020,10.1109/NSS/MIC42677.2020.9507983,internal dosimetry
85,Deep Learning-Assisted Whole-Body Voxel-Based Internal Dosimetry,A. Akhavanallaf; I. Shiri; H. Arabi; H. Zaidi,"We propose a novel methodology to conduct whole-body organ-level dosimetry taking into account the heterogeneity of activity distribution as well as patient-specific anatomy using Monte Carlo (MC) simulations and machine learning algorithms. We extended the core idea of the voxel-scale MIRD approach that utilizes a single S-value kernel for internal dosimetry by generating specific S-value kernels corresponding to patient-specific anatomy. In this context, we employed deep learning algorithms to predict the deposited energy distribution, representing the S-value kernel. The training dataset consists of density maps obtained from CT images along with the ground-truth dose distribution obtained from MC simulations. Accordingly, whole-body dose maps are constructed through convolving specific S-values with the activity map. The Deep Neural Network (DNN) predicted dose map was compared with the reference (Monte Carlo-based) and two MIRD-based methods, including single-voxel S-value (SSV) and multiple voxel S-value (MSV) approaches. The Mean Relative Absolute Errors (MRAE) of the estimated absorbed dose between DNN, MSV, and SSV against reference MC simulations were 2.6%, 3%, and 49%, respectively. MRAEs of 23.5%, 5.1%, and 21.8% were obtained between the proposed method and MSV, SSV, and Olinda dosimetry package in organ-level dosimetry, respectively. The proposed internal dosimetry technique exhibited comparable performance to the direct Monte Carlo approach while overcoming the computational burden limitation of MC simulations.",,2020,10.1109/NSS/MIC42677.2020.9507983,conferences
86,Predicting Lymph Node Metastasis in Head and Neck Cancer by Combining Many-objective Radiomics and 3-dimensioal Convolutional Neural Network through Evidential Reasoning,Z. Zhou; L. Chen; D. Sher; Q. Zhang; J. Shah; N. -L. Pham; S. Jiang; J. Wang,"Lymph node metastasis (LNM) is a significant prognostic factor in patients with head and neck cancer, and the ability to predict it accurately is essential for treatment optimization. PET and CT imaging are routinely used for LNM identification. However, uncertainties of LNM always exist especially for small size or reactive nodes. Radiomics and deep learning are the two preferred imaging-based strategies for node malignancy prediction. Radiomics models are built based on handcrafted features, and deep learning can learn the features automatically. We proposed a hybrid predictive model that combines many-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural network (3D-CNN) through evidential reasoning (ER) approach. To build a more reliable model, we proposed a new many-objective radiomics model. Meanwhile, we designed a 3D-CNN that fully utilizes spatial contextual information. Finally, the outputs were fused through the ER approach. To study the predictability of the two modalities, three models were built for PET, CT, and PET& CT. The results showed that the model performed best when the two modalities were combined. Moreover, we showed that the quantitative results obtained from the hybrid model were better than those obtained from MO-radiomics and 3D-CNN.",8.0,2018,10.1109/EMBC.2018.8513070,lymph nodes
86,Predicting Lymph Node Metastasis in Head and Neck Cancer by Combining Many-objective Radiomics and 3-dimensioal Convolutional Neural Network through Evidential Reasoning,Z. Zhou; L. Chen; D. Sher; Q. Zhang; J. Shah; N. -L. Pham; S. Jiang; J. Wang,"Lymph node metastasis (LNM) is a significant prognostic factor in patients with head and neck cancer, and the ability to predict it accurately is essential for treatment optimization. PET and CT imaging are routinely used for LNM identification. However, uncertainties of LNM always exist especially for small size or reactive nodes. Radiomics and deep learning are the two preferred imaging-based strategies for node malignancy prediction. Radiomics models are built based on handcrafted features, and deep learning can learn the features automatically. We proposed a hybrid predictive model that combines many-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural network (3D-CNN) through evidential reasoning (ER) approach. To build a more reliable model, we proposed a new many-objective radiomics model. Meanwhile, we designed a 3D-CNN that fully utilizes spatial contextual information. Finally, the outputs were fused through the ER approach. To study the predictability of the two modalities, three models were built for PET, CT, and PET& CT. The results showed that the model performed best when the two modalities were combined. Moreover, we showed that the quantitative results obtained from the hybrid model were better than those obtained from MO-radiomics and 3D-CNN.",8.0,2018,10.1109/EMBC.2018.8513070,erbium
86,Predicting Lymph Node Metastasis in Head and Neck Cancer by Combining Many-objective Radiomics and 3-dimensioal Convolutional Neural Network through Evidential Reasoning,Z. Zhou; L. Chen; D. Sher; Q. Zhang; J. Shah; N. -L. Pham; S. Jiang; J. Wang,"Lymph node metastasis (LNM) is a significant prognostic factor in patients with head and neck cancer, and the ability to predict it accurately is essential for treatment optimization. PET and CT imaging are routinely used for LNM identification. However, uncertainties of LNM always exist especially for small size or reactive nodes. Radiomics and deep learning are the two preferred imaging-based strategies for node malignancy prediction. Radiomics models are built based on handcrafted features, and deep learning can learn the features automatically. We proposed a hybrid predictive model that combines many-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural network (3D-CNN) through evidential reasoning (ER) approach. To build a more reliable model, we proposed a new many-objective radiomics model. Meanwhile, we designed a 3D-CNN that fully utilizes spatial contextual information. Finally, the outputs were fused through the ER approach. To study the predictability of the two modalities, three models were built for PET, CT, and PET& CT. The results showed that the model performed best when the two modalities were combined. Moreover, we showed that the quantitative results obtained from the hybrid model were better than those obtained from MO-radiomics and 3D-CNN.",8.0,2018,10.1109/EMBC.2018.8513070,feature extraction
86,Predicting Lymph Node Metastasis in Head and Neck Cancer by Combining Many-objective Radiomics and 3-dimensioal Convolutional Neural Network through Evidential Reasoning,Z. Zhou; L. Chen; D. Sher; Q. Zhang; J. Shah; N. -L. Pham; S. Jiang; J. Wang,"Lymph node metastasis (LNM) is a significant prognostic factor in patients with head and neck cancer, and the ability to predict it accurately is essential for treatment optimization. PET and CT imaging are routinely used for LNM identification. However, uncertainties of LNM always exist especially for small size or reactive nodes. Radiomics and deep learning are the two preferred imaging-based strategies for node malignancy prediction. Radiomics models are built based on handcrafted features, and deep learning can learn the features automatically. We proposed a hybrid predictive model that combines many-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural network (3D-CNN) through evidential reasoning (ER) approach. To build a more reliable model, we proposed a new many-objective radiomics model. Meanwhile, we designed a 3D-CNN that fully utilizes spatial contextual information. Finally, the outputs were fused through the ER approach. To study the predictability of the two modalities, three models were built for PET, CT, and PET& CT. The results showed that the model performed best when the two modalities were combined. Moreover, we showed that the quantitative results obtained from the hybrid model were better than those obtained from MO-radiomics and 3D-CNN.",8.0,2018,10.1109/EMBC.2018.8513070,cancer
86,Predicting Lymph Node Metastasis in Head and Neck Cancer by Combining Many-objective Radiomics and 3-dimensioal Convolutional Neural Network through Evidential Reasoning,Z. Zhou; L. Chen; D. Sher; Q. Zhang; J. Shah; N. -L. Pham; S. Jiang; J. Wang,"Lymph node metastasis (LNM) is a significant prognostic factor in patients with head and neck cancer, and the ability to predict it accurately is essential for treatment optimization. PET and CT imaging are routinely used for LNM identification. However, uncertainties of LNM always exist especially for small size or reactive nodes. Radiomics and deep learning are the two preferred imaging-based strategies for node malignancy prediction. Radiomics models are built based on handcrafted features, and deep learning can learn the features automatically. We proposed a hybrid predictive model that combines many-objective radiomics (MO-radiomics) and 3-dimensional convolutional neural network (3D-CNN) through evidential reasoning (ER) approach. To build a more reliable model, we proposed a new many-objective radiomics model. Meanwhile, we designed a 3D-CNN that fully utilizes spatial contextual information. Finally, the outputs were fused through the ER approach. To study the predictability of the two modalities, three models were built for PET, CT, and PET& CT. The results showed that the model performed best when the two modalities were combined. Moreover, we showed that the quantitative results obtained from the hybrid model were better than those obtained from MO-radiomics and 3D-CNN.",8.0,2018,10.1109/EMBC.2018.8513070,predictive models
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,entropy
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,sparse representation
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,standard deviation
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,max absolute rule
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,discrete wavelet transforms
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,transforms
87,Image Fusion Algorithm for Medical images using DWT and SR,S. Praveen kumar; S. Sridevi,"Diagnosis of ailments require accurate information from same modality images or different modality images like Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) etc which can be obtained by Image fusion technique In Image fusion, there are various methods implemented based on Discrete wavelet transformation. The image after fusion will have significant and accurate information from different images than the individual images. The main advantage of image fusion is that, it increases the quality of the particular image and also reduces redundancy and randomness. In this paper, transform method and sparse representation method are implemented for Image fusion of MRI, PET and CT images of brain regions to obtain a better entropy. Discrete Wavelet Transform (DWT) method is applied to obtain low pass and high pass patches. The low pass patches undergo Sparse representation (SR) to form the fused patch of the image. The Max Absolute rule is applied to the high pass patch to form single coefficients patch. The patches from low pass and high pass fusion are combined using inverse DWT reconstruction to form single fused image. Various parametric values Elapsed time, Entropy, Standard Deviation and Mean are evaluated. Entropy is measurement of information content. Higher the entropy value means, more the detailed information in the image. The entropy of the proposed method is 2.9299 which is significantly higher than state of art.",,2021,10.1109/ICAIS50930.2021.9396001,discrete wavelet transform
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,lung cancer
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,tumors
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,feature extraction
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,radiomics
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,multi-scale
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,predictive models
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,lung tumor.
88,MDR-SURV: A Multi-Scale Deep Learning-Based Radiomics for Survival Prediction in Pulmonary Malignancies,P. Afshar; A. Oikonomou; K. N. Plataniotis; A. Mohammadi,"Predicting death in lung cancer patients before initiating treatment is of paramount importance as this may guide decision-making towards more aggressive or combination of different types of treatment. In this work, we propose a Multi-scale Deep learning-based Radiomics model, referred to as ""MDR-SURV"" that exploits the information from positron emission tomography/computed tomography (PET/CT) images, combined with other clinical factors, to predict the overall survival (OS). Deep learning-based radiomics has the advantage of learning what features to extract, on its own. Furthermore, it does not require the exact segmentation of the tumor. The proposed MDR-SURV, which is a multi-scale framework, incorporates the tumor region and its surroundings, from different scales, and can extract both local and global tumor features. PET/CT images of 132 lung cancer patients who underwent stereotactic body radiotherapy (SBRT) were used to predict OS with the proposed model. Our results show that the MDR-SURV model outperforms its single-scale counterparts in predicting OS. Furthermore, the proposed MDR-SURV model achieves significantly high concordance index (C-index) of 73% in predicting the OS, which is noticeably higher than the results reported in existing literature.",,2020,10.1109/ICASSP40776.2020.9053243,survival prediction
89,Towards an Automatic Imaging Biopsy of Non-Small Cell Lung Cancer,E. D'Arnese; G. W. di Donato; E. del Sozzo; M. D. Santambrogio,"Because of the high aggressiveness and lethality of lung cancer, its early detection and accurate characterization are among the most investigated challenges in the last years. Biomedical imaging is an important technology in lung cancer assessment, strongly impacting decision making in clinical practice, also employed as a provider of predictive imaging biomarkers. In this context, radiomics approach, which consists of mining vast arrays of quantitative features derived from digital images, has shown promising application perspectives, but suboptimal standardization and controversial results emerged. This work presents the design of a completely automated pipeline for the non invasive in-vivo characterization of Non-Small Cell Lung Cancer (NSCLC), devised to be a support for radiologists and physicians, and to speed up the diagnostic process. Our pipeline exploits data from routinely acquired PET and CT images in order to automatically obtain a reliable segmentation of the tumor lesion: accurate textural features are computed in the detected Volume of Interest (VOI), thus providing data for the characterization of lung lesion through machine learning algorithms. We evaluated our pipeline on real datasets supplied by a private hospital. Our approach reached a mean accuracy of 94.2±5.0% for the VOI segmentation, and it showed the potential of PET/CT features in differentiating both between primary and metastatic lung lesions and between primary lung cancer subtypes.",3.0,2019,10.1109/BHI.2019.8834485,pipelines
89,Towards an Automatic Imaging Biopsy of Non-Small Cell Lung Cancer,E. D'Arnese; G. W. di Donato; E. del Sozzo; M. D. Santambrogio,"Because of the high aggressiveness and lethality of lung cancer, its early detection and accurate characterization are among the most investigated challenges in the last years. Biomedical imaging is an important technology in lung cancer assessment, strongly impacting decision making in clinical practice, also employed as a provider of predictive imaging biomarkers. In this context, radiomics approach, which consists of mining vast arrays of quantitative features derived from digital images, has shown promising application perspectives, but suboptimal standardization and controversial results emerged. This work presents the design of a completely automated pipeline for the non invasive in-vivo characterization of Non-Small Cell Lung Cancer (NSCLC), devised to be a support for radiologists and physicians, and to speed up the diagnostic process. Our pipeline exploits data from routinely acquired PET and CT images in order to automatically obtain a reliable segmentation of the tumor lesion: accurate textural features are computed in the detected Volume of Interest (VOI), thus providing data for the characterization of lung lesion through machine learning algorithms. We evaluated our pipeline on real datasets supplied by a private hospital. Our approach reached a mean accuracy of 94.2±5.0% for the VOI segmentation, and it showed the potential of PET/CT features in differentiating both between primary and metastatic lung lesions and between primary lung cancer subtypes.",3.0,2019,10.1109/BHI.2019.8834485,feature extraction
89,Towards an Automatic Imaging Biopsy of Non-Small Cell Lung Cancer,E. D'Arnese; G. W. di Donato; E. del Sozzo; M. D. Santambrogio,"Because of the high aggressiveness and lethality of lung cancer, its early detection and accurate characterization are among the most investigated challenges in the last years. Biomedical imaging is an important technology in lung cancer assessment, strongly impacting decision making in clinical practice, also employed as a provider of predictive imaging biomarkers. In this context, radiomics approach, which consists of mining vast arrays of quantitative features derived from digital images, has shown promising application perspectives, but suboptimal standardization and controversial results emerged. This work presents the design of a completely automated pipeline for the non invasive in-vivo characterization of Non-Small Cell Lung Cancer (NSCLC), devised to be a support for radiologists and physicians, and to speed up the diagnostic process. Our pipeline exploits data from routinely acquired PET and CT images in order to automatically obtain a reliable segmentation of the tumor lesion: accurate textural features are computed in the detected Volume of Interest (VOI), thus providing data for the characterization of lung lesion through machine learning algorithms. We evaluated our pipeline on real datasets supplied by a private hospital. Our approach reached a mean accuracy of 94.2±5.0% for the VOI segmentation, and it showed the potential of PET/CT features in differentiating both between primary and metastatic lung lesions and between primary lung cancer subtypes.",3.0,2019,10.1109/BHI.2019.8834485,cancer
89,Towards an Automatic Imaging Biopsy of Non-Small Cell Lung Cancer,E. D'Arnese; G. W. di Donato; E. del Sozzo; M. D. Santambrogio,"Because of the high aggressiveness and lethality of lung cancer, its early detection and accurate characterization are among the most investigated challenges in the last years. Biomedical imaging is an important technology in lung cancer assessment, strongly impacting decision making in clinical practice, also employed as a provider of predictive imaging biomarkers. In this context, radiomics approach, which consists of mining vast arrays of quantitative features derived from digital images, has shown promising application perspectives, but suboptimal standardization and controversial results emerged. This work presents the design of a completely automated pipeline for the non invasive in-vivo characterization of Non-Small Cell Lung Cancer (NSCLC), devised to be a support for radiologists and physicians, and to speed up the diagnostic process. Our pipeline exploits data from routinely acquired PET and CT images in order to automatically obtain a reliable segmentation of the tumor lesion: accurate textural features are computed in the detected Volume of Interest (VOI), thus providing data for the characterization of lung lesion through machine learning algorithms. We evaluated our pipeline on real datasets supplied by a private hospital. Our approach reached a mean accuracy of 94.2±5.0% for the VOI segmentation, and it showed the potential of PET/CT features in differentiating both between primary and metastatic lung lesions and between primary lung cancer subtypes.",3.0,2019,10.1109/BHI.2019.8834485,lung
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,coincidence weighting
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,mutual information
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,kernel
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,proposals
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,normalized mutual information
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,histograms
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,adaptive binning
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,background noise
90,Automatic Parameter Selection for Multimodal Image Registration,D. A. Hahn; V. Daum; J. Hornegger,"Over the past ten years similarity measures based on intensity distributions have become state-of-the-art in automatic multimodal image registration. An implementation for clinical usage has to support a plurality of images. However, a generally applicable parameter configuration for the number and sizes of histogram bins, optimal Parzen-window kernel widths or background thresholds cannot be found. This explains why various research groups present partly contradictory empirical proposals for these parameters. This paper proposes a set of data-driven estimation schemes for a parameter-free implementation that eliminates major caveats of heuristic trial and error. We present the following novel approaches: a new coincidence weighting scheme to reduce the influence of background noise on the similarity measure in combination with Max-Lloyd requantization, and a tradeoff for the automatic estimation of the number of histogram bins. These methods have been integrated into a state-of-the-art rigid registration that is based on normalized mutual information and applied to CT-MR, PET-MR, and MR-MR image pairs of the RIRE 2.0 database. We compare combinations of the proposed techniques to a standard implementation using default parameters, which can be found in the literature, and to a manual registration by a medical expert. Additionally, we analyze the effects of various histogram sizes, sampling rates, and error thresholds for the number of histogram bins. The comparison of the parameter selection techniques yields 25 approaches in total, with 114 registrations each. The number of bins has no significant influence on the proposed implementation that performs better than both the manual and the standard method in terms of acceptance rates and target registration error (TRE). The overall mean TRE is 2.34 mm compared to 2.54 mm for the manual registration and 6.48 mm for a standard implementation. Our results show a significant TRE reduction for distortion-corrected magnetic resonance images.",17.0,2010,10.1109/TMI.2010.2041358,noise measurement
91,A novel approach for medical image fusion using fuzzy logic type-2,H. R. Ramya; B. K. Sujatha,"Medical image fusion is used to integrate the essential features present in different medical images into a single image to improve the clinical accuracy to take better decisions. Multimodal medical image fusion combines the images obtained from different modalities like Positron Emission Tomography (PET), Computed Tomography (CT), Magnetic Resonance Imaging (MRI) and others. CT scan provides detailed information on bony structures whereas MRI scan provides details on soft tissues. Fusion of these images is useful for doctors to diagnose and plan treatment for patients. Various methods have been proposed during recent years for medical image fusion but these methods suffer from the issue of fusion performance due to different sensor modalities. In this paper, we validate that for such images, fusion performance can be improved using a learning based fusion scheme which uses training and testing phase by considering pixel wise processing of image. In order to improve image fusion performance, here we implement fuzzy logic type-2 based approaches for medical image fusion for CT and MRI images. Final fused image is achieved by applying rule based method which includes fuzzification inference, type reduction, and defuzzification of the input image. Experiments are conducted by applying fuzzy logic type-1, neural network, Neuro-fuzzy approach and fuzzy logic type-2 based fusion scheme. Performance of image fusion based on Fuzzy Logic Type-2 is compared with other state-of-art techniques using various performance metrics such as Mutual information, standard deviation and edge based similarity. Experimental results shows that fuzzy logic Type-2 performs better when compared to neural network based approach.",1.0,2016,10.1109/CIMCA.2016.8053286,transforms
91,A novel approach for medical image fusion using fuzzy logic type-2,H. R. Ramya; B. K. Sujatha,"Medical image fusion is used to integrate the essential features present in different medical images into a single image to improve the clinical accuracy to take better decisions. Multimodal medical image fusion combines the images obtained from different modalities like Positron Emission Tomography (PET), Computed Tomography (CT), Magnetic Resonance Imaging (MRI) and others. CT scan provides detailed information on bony structures whereas MRI scan provides details on soft tissues. Fusion of these images is useful for doctors to diagnose and plan treatment for patients. Various methods have been proposed during recent years for medical image fusion but these methods suffer from the issue of fusion performance due to different sensor modalities. In this paper, we validate that for such images, fusion performance can be improved using a learning based fusion scheme which uses training and testing phase by considering pixel wise processing of image. In order to improve image fusion performance, here we implement fuzzy logic type-2 based approaches for medical image fusion for CT and MRI images. Final fused image is achieved by applying rule based method which includes fuzzification inference, type reduction, and defuzzification of the input image. Experiments are conducted by applying fuzzy logic type-1, neural network, Neuro-fuzzy approach and fuzzy logic type-2 based fusion scheme. Performance of image fusion based on Fuzzy Logic Type-2 is compared with other state-of-art techniques using various performance metrics such as Mutual information, standard deviation and edge based similarity. Experimental results shows that fuzzy logic Type-2 performs better when compared to neural network based approach.",1.0,2016,10.1109/CIMCA.2016.8053286,neuro-fuzzy
91,A novel approach for medical image fusion using fuzzy logic type-2,H. R. Ramya; B. K. Sujatha,"Medical image fusion is used to integrate the essential features present in different medical images into a single image to improve the clinical accuracy to take better decisions. Multimodal medical image fusion combines the images obtained from different modalities like Positron Emission Tomography (PET), Computed Tomography (CT), Magnetic Resonance Imaging (MRI) and others. CT scan provides detailed information on bony structures whereas MRI scan provides details on soft tissues. Fusion of these images is useful for doctors to diagnose and plan treatment for patients. Various methods have been proposed during recent years for medical image fusion but these methods suffer from the issue of fusion performance due to different sensor modalities. In this paper, we validate that for such images, fusion performance can be improved using a learning based fusion scheme which uses training and testing phase by considering pixel wise processing of image. In order to improve image fusion performance, here we implement fuzzy logic type-2 based approaches for medical image fusion for CT and MRI images. Final fused image is achieved by applying rule based method which includes fuzzification inference, type reduction, and defuzzification of the input image. Experiments are conducted by applying fuzzy logic type-1, neural network, Neuro-fuzzy approach and fuzzy logic type-2 based fusion scheme. Performance of image fusion based on Fuzzy Logic Type-2 is compared with other state-of-art techniques using various performance metrics such as Mutual information, standard deviation and edge based similarity. Experimental results shows that fuzzy logic Type-2 performs better when compared to neural network based approach.",1.0,2016,10.1109/CIMCA.2016.8053286,fuzzy logic
92,Deep Learning-Based Image Segmentation on Multimodal Medical Imaging,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"Multimodality medical imaging techniques have been increasingly applied in clinical practice and research studies. Corresponding multimodal image analysis and ensemble learning schemes have seen rapid growth and bring unique value to medical applications. Motivated by the recent success of applying deep learning methods to medical image processing, we first propose an algorithmic architecture for supervised multimodal image analysis with cross-modality fusion at the feature learning level, classifier level, and decision-making level. We then design and implement an image segmentation system based on deep convolutional neural networks to contour the lesions of soft tissue sarcomas using multimodal images, including those from magnetic resonance imaging, computed tomography, and positron emission tomography. The network trained with multimodal images shows superior performance compared to networks trained with single-modal images. For the task of tumor segmentation, performing image fusion within the network (i.e., fusing at convolutional or fully connected layers) is generally better than fusing images at the network output (i.e., voting). This paper provides empirical guidance for the design and application of multimodal image analysis.",101.0,2019,10.1109/TRPMS.2018.2890359,tumors
92,Deep Learning-Based Image Segmentation on Multimodal Medical Imaging,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"Multimodality medical imaging techniques have been increasingly applied in clinical practice and research studies. Corresponding multimodal image analysis and ensemble learning schemes have seen rapid growth and bring unique value to medical applications. Motivated by the recent success of applying deep learning methods to medical image processing, we first propose an algorithmic architecture for supervised multimodal image analysis with cross-modality fusion at the feature learning level, classifier level, and decision-making level. We then design and implement an image segmentation system based on deep convolutional neural networks to contour the lesions of soft tissue sarcomas using multimodal images, including those from magnetic resonance imaging, computed tomography, and positron emission tomography. The network trained with multimodal images shows superior performance compared to networks trained with single-modal images. For the task of tumor segmentation, performing image fusion within the network (i.e., fusing at convolutional or fully connected layers) is generally better than fusing images at the network output (i.e., voting). This paper provides empirical guidance for the design and application of multimodal image analysis.",101.0,2019,10.1109/TRPMS.2018.2890359,feature extraction
92,Deep Learning-Based Image Segmentation on Multimodal Medical Imaging,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"Multimodality medical imaging techniques have been increasingly applied in clinical practice and research studies. Corresponding multimodal image analysis and ensemble learning schemes have seen rapid growth and bring unique value to medical applications. Motivated by the recent success of applying deep learning methods to medical image processing, we first propose an algorithmic architecture for supervised multimodal image analysis with cross-modality fusion at the feature learning level, classifier level, and decision-making level. We then design and implement an image segmentation system based on deep convolutional neural networks to contour the lesions of soft tissue sarcomas using multimodal images, including those from magnetic resonance imaging, computed tomography, and positron emission tomography. The network trained with multimodal images shows superior performance compared to networks trained with single-modal images. For the task of tumor segmentation, performing image fusion within the network (i.e., fusing at convolutional or fully connected layers) is generally better than fusing images at the network output (i.e., voting). This paper provides empirical guidance for the design and application of multimodal image analysis.",101.0,2019,10.1109/TRPMS.2018.2890359,convolutional neural network (cnn)
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,nonnegative matrix tri-factorization
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,unsupervised learning
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,patient stratification
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,rectal cancer
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,principal component analysis
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,collaboration
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,radiomics
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,cancer
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,feature extraction
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,predictive models
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,collaborative clustering
93,Collaborative Clustering Of Subjects And Radiomic Features For Predicting Clinical Outcomes Of Rectal Cancer Patients,H. Liu; H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Most machine learning approaches in radiomics studies ignore the underlying difference of radiomic features computed from heterogeneous groups of patients, and intrinsic correlations of the features are not fully exploited yet. In order to better predict clinical outcomes of cancer patients, we adopt an unsupervised machine learning method to simultaneously stratify cancer patients into distinct risk groups based on their radiomic features and learn low-dimensional representations of the radiomic features for robust prediction of their clinical outcomes. Based on nonnegative matrix tri-factorization techniques, the proposed method applies collaborative clustering to radiomic features of cancer patients to obtain clusters of both the patients and their radiomic features so that patients with distinct imaging patterns are stratified into different risk groups and highly correlated radiomic features are grouped in the same radiomic feature clusters. Experiments on a FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method facilitates better stratification of patients with distinct survival patterns and learning of more effective low-dimensional feature representations that ultimately leads to accurate prediction of patient survival, outperforming conventional methods under comparison.",2.0,2019,10.1109/ISBI.2019.8759512,dimensionality reduction
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,regression
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,distance metric
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,optimization
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,measurement
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,sparsity
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,vectors
94,Regression and classification based distance metric learning for medical image retrieval,W. Cai; Y. Song; D. D. Feng,"Better utilizing the vast amount of valuable information stored in the medical imaging databases is always an interesting research area, and one way is to retrieve similar images as a reference dataset to assist the diagnosis. Distance metric is a core component in image retrieval; and in this paper, we propose a new learning-based distance metric design, based on regression and classification techniques. We design a weight learning approach by classifying the similar-dissimilar data samples, and a further optimization with a sparsity-constraint regression algorithm for feature selection. The learned distance metric is generally applicable for medical image retrievals. We evaluate the proposed method on clinical PET-CT images, and demonstrate clear performance improvements.",4.0,2012,10.1109/ISBI.2012.6235925,lungs
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,noise
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,frequency-domain analysis
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,brushlets
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,histograms
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,rendering (computer graphics)
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,transforms
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,transfer functions
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,volume rendering
95,Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs,M. Alper Selver,"Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.",9.0,2015,10.1109/TVCG.2014.2359462,texture analysis
96,Medical image segmentation based on multi-modal convolutional neural network: Study on image fusion schemes,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"Motivated by the recent success in applying deep learning for natural image analysis, we designed an image segmentation system based on deep Convolutional Neural Network (CNN) to detect the presence of soft tissue sarcoma from multi-modality medical images, including Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and Positron Emission Tomography (PET). Multi-modality imaging analysis using deep learning has been increasingly applied in the field of biomedical imaging and brought unique value to medical applications. However, it is still challenging to perform the multi-modal analysis owing to a major difficulty that is how to fuse the information derived from different modalities. There exist varies of possible schemes which are application-dependent and lack of a unified framework to guide their designs. Aiming at lesion segmentation with multi-modality images, we innovatively propose a conceptual image fusion architecture for supervised biomedical image analysis. The architecture has been optimized by testing different fusion schemes within the CNN structure, including fusing at the feature learning level, fusing at the classifier level, and the fusing at the decision-making level. It is found from the results that while all the fusion schemes outperform the single-modality schemes, fusing at the feature level can generally achieve the best performance in terms of both accuracy and computational cost, but can also suffer from the decreased robustness due to the presence of large errors in one or more image modalities.",29.0,2018,10.1109/ISBI.2018.8363717,testing
96,Medical image segmentation based on multi-modal convolutional neural network: Study on image fusion schemes,Z. Guo; X. Li; H. Huang; N. Guo; Q. Li,"Motivated by the recent success in applying deep learning for natural image analysis, we designed an image segmentation system based on deep Convolutional Neural Network (CNN) to detect the presence of soft tissue sarcoma from multi-modality medical images, including Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and Positron Emission Tomography (PET). Multi-modality imaging analysis using deep learning has been increasingly applied in the field of biomedical imaging and brought unique value to medical applications. However, it is still challenging to perform the multi-modal analysis owing to a major difficulty that is how to fuse the information derived from different modalities. There exist varies of possible schemes which are application-dependent and lack of a unified framework to guide their designs. Aiming at lesion segmentation with multi-modality images, we innovatively propose a conceptual image fusion architecture for supervised biomedical image analysis. The architecture has been optimized by testing different fusion schemes within the CNN structure, including fusing at the feature learning level, fusing at the classifier level, and the fusing at the decision-making level. It is found from the results that while all the fusion schemes outperform the single-modality schemes, fusing at the feature level can generally achieve the best performance in terms of both accuracy and computational cost, but can also suffer from the decreased robustness due to the presence of large errors in one or more image modalities.",29.0,2018,10.1109/ISBI.2018.8363717,tumors
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,individualized dose treatment planning
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,radiation-induced lung damage
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,clinical prognostic models
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,non-small cell lung cancer
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,tumors
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,cancer
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,three dimensional displays
97,"The Combination of Clinical, Dose-Related and Imaging Features Helps Predict Radiation-Induced Normal-Tissue Toxicity in Lung-cancer Patients -- An in-silico Trial Using Machine Learning Techniques",G. Nalbantov; A. Dekker; D. De Ruysscher; P. Lambin; E. N. Smirnov,"The amount of delivered radiation dose to the tumor in non-small cell lung cancer (NSCLC) patients is limited by the negative side effects on normal tissues. The most dose-limiting factor in radiotherapy is the radiation-induced lung toxicity (RILT). RILT is generally measured semi-quantitatively, by a dyspnea, or shortness-of-breath, score. In general, about 20-30% of patients develop RILT several months after treatment, and in about 70% of the patients the delivered dose is insufficient to control the tumor growth. Ideally, if the RILT score would be known in advance, then the dose treatment plan for the low-toxicity-risk patients could be adjusted so that higher dose is delivered to the tumor to better control it. A number of possible predictors of RILT have been proposed in the literature, including dose-related and clinical/demographic patient characteristics available prior to radiotherapy. In addition, the use of imaging features -- which are noninvasive in nature - has been gaining momentum. Thus, anatomic as well as functional/metabolic information from CT and PET scanner images respectively are used in daily clinical practice, which provide further information about the status of a patient. In this study we assessed whether machine learning techniques can successfully be applied to predict post-radiation lung damage, proxied by dyspnea score, based on clinical, dose-related (dosimetric) and image features. Our dataset included 78 NSCLC patients. The patients were divided into two groups: no-deterioration-of-dyspnea, and deterioration-of-dyspnea patients. Several machine-learning binary classifiers were applied to discriminate the two groups. The results, evaluated using the area under the ROC curve in a cross-validation procedure, are highly promising. This outcome could open the possibility to deliver better, individualized dose-treatment plans for lung cancer patients and help the overall clinical decision making (treatment) process.",1.0,2011,10.1109/ICMLA.2011.139,lungs
98,DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks,V. S. S. Kandarpa; A. Bousse; D. Benoit; D. Visvikis,"This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.",2.0,2021,10.1109/TRPMS.2020.3033172,computer architecture
98,DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks,V. S. S. Kandarpa; A. Bousse; D. Benoit; D. Visvikis,"This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.",2.0,2021,10.1109/TRPMS.2020.3033172,noise reduction
98,DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks,V. S. S. Kandarpa; A. Bousse; D. Benoit; D. Visvikis,"This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.",2.0,2021,10.1109/TRPMS.2020.3033172,generative adversarial networks (gans)
99,Cross-Modality Medical Image Retrieval with Deep Features,A. Mbilinyi; H. Schuldt,"In medical imaging, modality refers to the technique and process used to create visual representations of a particular part of the body, organs, or tissues. Conventional modalities include X-ray, CT-scan, Magnetic Resonance Imaging (MRI), Ultrasound, and Positron Emission Tomography (PET). Depending on the modality used, the same disease can be detected differently, making a modality an essential filter in evaluating the relevance of search results when retrieving similar medical images. Traditionally, texture features have been used for content-based medical image retrieval. However, texture features are limited in capturing the semantic similarity between medical images, let alone their modalities. This paper explores deep features (features extracted by deep convolutional neural networks (CNN)) and analyzes their effectiveness in retrieving similar medical images, semantic-wise and modality-wise, from a collection with different medical image modalities. We have examined CNNs of different architectures pre-trained in natural images and CNNs we fine-tuned and fully-trained from scratch in medical images to extract deep features. Based on retrieval performance evaluation, we show that deep features, even though extracted by CNN pre-trained in natural images, still outperform texture features. On the other end, we show that deep features extracted by a smaller, simpler, and yet computationally efficient CNN we trained in medical images can compete with large and complex ImageNet CNNs fine-tuned or fully trained in medical images.",2.0,2020,10.1109/BIBM49941.2020.9313211,task analysis
99,Cross-Modality Medical Image Retrieval with Deep Features,A. Mbilinyi; H. Schuldt,"In medical imaging, modality refers to the technique and process used to create visual representations of a particular part of the body, organs, or tissues. Conventional modalities include X-ray, CT-scan, Magnetic Resonance Imaging (MRI), Ultrasound, and Positron Emission Tomography (PET). Depending on the modality used, the same disease can be detected differently, making a modality an essential filter in evaluating the relevance of search results when retrieving similar medical images. Traditionally, texture features have been used for content-based medical image retrieval. However, texture features are limited in capturing the semantic similarity between medical images, let alone their modalities. This paper explores deep features (features extracted by deep convolutional neural networks (CNN)) and analyzes their effectiveness in retrieving similar medical images, semantic-wise and modality-wise, from a collection with different medical image modalities. We have examined CNNs of different architectures pre-trained in natural images and CNNs we fine-tuned and fully-trained from scratch in medical images to extract deep features. Based on retrieval performance evaluation, we show that deep features, even though extracted by CNN pre-trained in natural images, still outperform texture features. On the other end, we show that deep features extracted by a smaller, simpler, and yet computationally efficient CNN we trained in medical images can compete with large and complex ImageNet CNNs fine-tuned or fully trained in medical images.",2.0,2020,10.1109/BIBM49941.2020.9313211,deep features
99,Cross-Modality Medical Image Retrieval with Deep Features,A. Mbilinyi; H. Schuldt,"In medical imaging, modality refers to the technique and process used to create visual representations of a particular part of the body, organs, or tissues. Conventional modalities include X-ray, CT-scan, Magnetic Resonance Imaging (MRI), Ultrasound, and Positron Emission Tomography (PET). Depending on the modality used, the same disease can be detected differently, making a modality an essential filter in evaluating the relevance of search results when retrieving similar medical images. Traditionally, texture features have been used for content-based medical image retrieval. However, texture features are limited in capturing the semantic similarity between medical images, let alone their modalities. This paper explores deep features (features extracted by deep convolutional neural networks (CNN)) and analyzes their effectiveness in retrieving similar medical images, semantic-wise and modality-wise, from a collection with different medical image modalities. We have examined CNNs of different architectures pre-trained in natural images and CNNs we fine-tuned and fully-trained from scratch in medical images to extract deep features. Based on retrieval performance evaluation, we show that deep features, even though extracted by CNN pre-trained in natural images, still outperform texture features. On the other end, we show that deep features extracted by a smaller, simpler, and yet computationally efficient CNN we trained in medical images can compete with large and complex ImageNet CNNs fine-tuned or fully trained in medical images.",2.0,2020,10.1109/BIBM49941.2020.9313211,feature extraction
99,Cross-Modality Medical Image Retrieval with Deep Features,A. Mbilinyi; H. Schuldt,"In medical imaging, modality refers to the technique and process used to create visual representations of a particular part of the body, organs, or tissues. Conventional modalities include X-ray, CT-scan, Magnetic Resonance Imaging (MRI), Ultrasound, and Positron Emission Tomography (PET). Depending on the modality used, the same disease can be detected differently, making a modality an essential filter in evaluating the relevance of search results when retrieving similar medical images. Traditionally, texture features have been used for content-based medical image retrieval. However, texture features are limited in capturing the semantic similarity between medical images, let alone their modalities. This paper explores deep features (features extracted by deep convolutional neural networks (CNN)) and analyzes their effectiveness in retrieving similar medical images, semantic-wise and modality-wise, from a collection with different medical image modalities. We have examined CNNs of different architectures pre-trained in natural images and CNNs we fine-tuned and fully-trained from scratch in medical images to extract deep features. Based on retrieval performance evaluation, we show that deep features, even though extracted by CNN pre-trained in natural images, still outperform texture features. On the other end, we show that deep features extracted by a smaller, simpler, and yet computationally efficient CNN we trained in medical images can compete with large and complex ImageNet CNNs fine-tuned or fully trained in medical images.",2.0,2020,10.1109/BIBM49941.2020.9313211,cross-modality
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,visibility attribute
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,crf
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,vocabulary
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,cnn
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,encoding
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,feature extraction
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,gru
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,anomaly detection
100,Visibility Attribute Extraction and Anomaly Detection for Chinese Diagnostic Report Based on Cascade Networks,J. Zhang; H. Jiang; L. Huang; Y. -D. Yao; S. Li,"In the positron emission tomography/computed tomography (PET/CT) image diagnosis report, the semantic analysis of image findings section is an important part of the automatic diagnosis of medical image, which is an essential step for extracting keywords and abnormal sentences in the diagnostic report. To this end, this paper combines visibility attribute extraction network (VAE-Net) and bi-directional gated recurrent unit (BiGRU) into cascade networks to solve the tasks of attribute extraction and anomaly detection. First, a visibility attribute (VA) is defined to summary the vocabulary into 12 patterns based on the language characteristics in image findings. Second, a visibility attribute extraction network (VAE-Net) is developed to automatically extract VA from word embeddings, which is composed of residual convolutional neural network (residual CNN), BiGRU, and conditional random field (CRF). Finally, word embeddings and the corresponding VA are input into BiGRU and softmax to perform sentence-level anomaly detections. We evaluate the proposed method on a proprietary Chinese PET/CT diagnostic report dataset with an F1-score of 94.35% in the attribute extraction, an F1-score of 96.40% in sentence-level anomaly detection, and an F1-score of 96.77% in case-level anomaly detection. Besides, a publicity English national center for biotechnology information (NCBI) disease corpus dataset is used for externed validation with an F1-score of 95.81% in disease detection. The experimental results demonstrate the advantage of the proposed cascade networks as compared to other related methods.",,2019,10.1109/ACCESS.2019.2932842,task analysis
101,Application of Deep Learning in Imaging diagnosis of Brain diseases,J. Liang; Z. Wang; X. Ye,"The brain is one of the most important parts of the human body, and the diagnosis of its diseases is of great significance to the treatment of diseases. With the rapid development of deep learning in recent years, its automatically extracted image features have significant advantages compared to traditional artificially extracted features. Therefore, more and more recognition methods based on deep learning are widely used in medical image recognition tasks (such as CT, MRI, PET-CT.). This paper will introduce the application of traditional methods and deep learning methods in various brain diseases. These methods are compared, analyzed, and summarized, then we explored their development status and future development trends.",,2021,10.1109/MLBDBI54094.2021.00040,feature extraction
101,Application of Deep Learning in Imaging diagnosis of Brain diseases,J. Liang; Z. Wang; X. Ye,"The brain is one of the most important parts of the human body, and the diagnosis of its diseases is of great significance to the treatment of diseases. With the rapid development of deep learning in recent years, its automatically extracted image features have significant advantages compared to traditional artificially extracted features. Therefore, more and more recognition methods based on deep learning are widely used in medical image recognition tasks (such as CT, MRI, PET-CT.). This paper will introduce the application of traditional methods and deep learning methods in various brain diseases. These methods are compared, analyzed, and summarized, then we explored their development status and future development trends.",,2021,10.1109/MLBDBI54094.2021.00040,brain disease
101,Application of Deep Learning in Imaging diagnosis of Brain diseases,J. Liang; Z. Wang; X. Ye,"The brain is one of the most important parts of the human body, and the diagnosis of its diseases is of great significance to the treatment of diseases. With the rapid development of deep learning in recent years, its automatically extracted image features have significant advantages compared to traditional artificially extracted features. Therefore, more and more recognition methods based on deep learning are widely used in medical image recognition tasks (such as CT, MRI, PET-CT.). This paper will introduce the application of traditional methods and deep learning methods in various brain diseases. These methods are compared, analyzed, and summarized, then we explored their development status and future development trends.",,2021,10.1109/MLBDBI54094.2021.00040,computer-assisted diagnosis
101,Application of Deep Learning in Imaging diagnosis of Brain diseases,J. Liang; Z. Wang; X. Ye,"The brain is one of the most important parts of the human body, and the diagnosis of its diseases is of great significance to the treatment of diseases. With the rapid development of deep learning in recent years, its automatically extracted image features have significant advantages compared to traditional artificially extracted features. Therefore, more and more recognition methods based on deep learning are widely used in medical image recognition tasks (such as CT, MRI, PET-CT.). This paper will introduce the application of traditional methods and deep learning methods in various brain diseases. These methods are compared, analyzed, and summarized, then we explored their development status and future development trends.",,2021,10.1109/MLBDBI54094.2021.00040,medical diagnosis
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,automated multi-objective learning (automo)
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,sensitivity and specificity
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,multi-objective optimization
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,cancer
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,reliability
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,head & neck cancer
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,predictive models
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,lymph node metastasis
102,Reliable lymph node metastasis prediction in head & neck cancer through automated multi-objective model,Z. Zhou; M. Dohopolski; L. Chen; X. Chen; S. Jiang; D. Sher; J. Wang,"Lymph node metastasis (LNM) plays an important role for accurately diagnosing and treating the patients with head & neck cancer. Positron emission tomography (PET) and computed tomography (CT) are two primary imaging modalities used for identifying LNM status. However, the uncertainty of LNM may exist especially for reactive or small nodes. Furthermore, identifying the LNM on PET or CT is greatly dependent on the physician's experience. Therefore, developing a reliable and automatic model is essential for accurately identifying LNM. Multi-objective models have shown promising predictive results by considering different objectives such as sensitivity and specificity. However, most multi-objective models need to choose an optimal model manually. In this work, we proposed an automated multi-objective learning model (AutoMO) for predicting LNM reliably. Instead of picking one optimal model, all the Pareto-optimal models with the calculated relative weights are used in AutoMO. Then the evidential reasoning (ER) approach is used for fusing the output probability for obtaining more reliable results than traditional fusion method. We built three models for PET, CT and PET&CT and the results showed that PET&CT outperformed two single modality based models. The comparative study demonstrated that AutoMO obtained better performance than current available multi-objective and deep learning methods, and more reliable results can be acquired when using ER fusion.",2.0,2019,10.1109/BHI.2019.8834658,evidential reasoning
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,feature extraction
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,predictive models
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,handheld computers
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,inceptionv3
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,extract features
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,ppmi
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,mri
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,vgg16
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,vgg19
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,computer architecture
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,accuracy
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,medical services
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,neurological condition
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,cnn
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,dopaminergic neurons
103,Parkinson’s Disease Detection Using FMRI Images Leveraging Transfer Learning on Convolutional Neural Network,A. SAJEEB; A. F. M. NAZMUS SAKIB; S. ALI SHUSHMITA; S. M. ASHRAF KABIR; M. T. REZA; M. Z. PARVEZ,"Parkinson's disease(PD) is a neurological condition that is dynamic and steadily influences the movement of the human body. PD influences the central apprehensive system which happens because of the hardship of dopaminergic neurons brought about in a neuro-degenerative incubation. The patients who have PD usually suffer from tremor, unyielding nature, postural shifts, and lessen in unconstrained advancements. There is no particular diagnosis process for PD. PD varies from one person to another person depending on the situation and the family history.Magnetic Resonance Imaging (MRI), Computed Tomography (CT), ultrasound of the brain, Positron Emission Tomography (PET) scans are common imaging tests to figure out this disease but these tests are not particularly effective. In this research, several tests are run on two types of data group - control and PD affected people. The dataset is collected from the Parkinson's Progression Markers Initiative (PPMI) repository. Then MRI slices are processed from the selected data group into the CNN models. Three different Convolutional Neural Network (CNN) architectures are used in this work to extract features from the data group. The CNN models are InceptionV3, VGG16 and VGG19. These models are used in this research to compare and to get better accuracy. Among these models, VGG19 worked best in the dataset because the accuracy for VGG19 is 91.5% where VGG16 gives 88.5% and inceptionV3 gives 89.5% accuracy on detecting PD.",,2020,10.1109/ICMLC51923.2020.9469530,parkinson’s disease (pd)
104,Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",,2021,10.1109/CONECCT52877.2021.9622568,support vector machines
104,Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",,2021,10.1109/CONECCT52877.2021.9622568,lung cancer
104,Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",,2021,10.1109/CONECCT52877.2021.9622568,adaboost
104,Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",,2021,10.1109/CONECCT52877.2021.9622568,feature extraction
104,Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",,2021,10.1109/CONECCT52877.2021.9622568,sensitivity
104,Lung Cancer Types Prediction Using Machine Learning Approach,K. Ingle; U. Chaskar; S. Rathod,"Lung cancer is the most common type of cancer in India, along with prostate, mouth, breast cancer. Due to smoking, excessive increases in pollution and the inhalation of cancerous elements cause cancer in men and women. Still, the amount of cancer in men is more compared to women. Different imaging modalities are used for diagnoses, such as CT, MRI, PET, and X-Ray. Oncologists and physicians diagnose cancer by seeing gray-scale saturation in the image. It requires experience in both anatomy and physiology of humans with imaging technique knowledge. It takes too much time for diagnosing, and accuracy is less due to a lack of imaging expertise. To avoid time consumption and provide easy diagnosis, the researcher has used an AI-based application technique for diagnosis purposes. In this study, we have used the Ensemble Machine Learning algorithm, i.e., AdaBoost, to predict different lung cancer types. We trained ensemble learners using features extracted from the lung CT images and evaluations done using performance metrics. The Accuracy of Adaboost is 90.74%, with sensitivity is 81.80%, specificity is 93.99%, F1 score 0.8, kappa is 0.753, and AUC is 0.93. The performance of the AdaBoost classifier is compared with the different machine learning algorithms.",,2021,10.1109/CONECCT52877.2021.9622568,lung
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,daubechies wavelet - dw
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,alzheimer's disease -ad
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,linear vector quantization - lvq
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,discrete wavelet transform - dwt
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,tumors
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,self organizing maps - som
105,Wavelets based decomposition and classification of diseased fMRI brain images for inter racial disease types of Alzheimer's Vs tumors using SOFM and enhancement by LVQ neural networks,N. V. Manokar; V. Manokar; Rinesh; K. P. Sridhar; L. M. Patnaik,"This research addresses to the problem of establishment patterns, training of the optimal desired network, classification and accuracy prediction. Once the accuracy issue had been addressed, the issue of efficiency detection and maintenance is normally introduced and experimented in most of the scenarios. The later stage of development is dealt with the inter racial classification of diseases which gets over lapped. The problem of detection of brain images into corresponding diseased and Non-Diseased syndrome types for functional Magnetic Resonance Images. The classification problem gets intensified when there comes the inter lapped diseased contents. Among such, overlapped problems occur when there are significant inter-related similar symptoms, similar types of traditional symptoms, common symptoms, where there needs to be minute differences in disease identification. A case occurs with relevance to the shrinking of the frontal lobes and expansion of the back lobes. When a Tumor or a cyst starts protruding, pain occurs followed blurred vision and nausea, unbearable pain and others symptoms. Some of the Alzheimer's symptoms are the shrinking of the frontal lobes and expansion of the behind lobes, followed by memory loss, abnormal behaviour and etc., The process involves the classification among Tumour Vs Alzheimers' classification. The images differ with capturing intensity and noise content. This happens during motion capturing. The choosing up of the images for featuring detection, feature vector calculation and subjecting to training and testing is all carried out from fMRI images. The reason being the functional magnetic Resonance Image properties gets inducted in the fMRI images compared to the other types of images like the PET, CT images etc., A Daubechies Wavelet Transform acting as a smoothening filter was applied playing a dual role in denoising, Decomposition and extraction of frequency feature components for feature vector calculation. A modified median filter removes random noises better which has been induced. A fourth level order of Daubechies Discrete Wavelet decomposition was used for feature vector formation. This was performed because of the blurring results of the images due to the wavelet implication. This naturally destroy the feature coefficients but yet another prominent and effective way in order the classification of the brain images into diseased and non - diseased brain images. The approximated sub - band images were chosen from the input images to train the network. In designing a network using a competitive neural network as a subclass to classify the normal and abnormal image type into corresponding diseased and non - disease types. Initially subjecting to a Competitive Learning, Self - Organized Maps are enhanced to Self - Organized Feature Maps and finally enhanced to Linear Vector Quantization. On comparing to the earlier competitive network, the network of Competitive Learning, SOM and SOFM, and invoking Linear Vector Quantization were considered as improvised for classification and efficient for detection.",1.0,2012,10.1109/PDGC.2012.6449929,self -organized feature maps -sofm
106,Skull Segmentation in MRI by a Support Vector Machine Combining Local and Global Features,J. Sjölund; A. E. Järlideni; M. Andersson; H. Knutsson; H. Nordström,"Magnetic resonance (MR) images lack information about radiation transport-a fact which is problematic in applications such as radiotherapy planning and attenuation correction in combined PET/MR imaging. To remedy this, a crude but common approach is to approximate all tissue properties as equivalent to those of water. We improve upon this using an algorithm that automatically identifies bone tissue in MR. More specifically, we focus on segmenting the skull prior to stereotactic neurosurgery, where it is common that only MR images are available. In the proposed approach, a machine learning algorithm known as a support vector machine is trained on patients for which both a CT and an MR scan are available. As input, a combination of local and global information is used. The latter is needed to distinguish between bone and air as this is not possible based only on the local image intensity. A whole skull segmentation is achievable in minutes. In a comparison with two other methods, one based on mathematical morphology and the other on deformable registration, the proposed method was found to yield consistently better segmentations.",4.0,2014,10.1109/ICPR.2014.564,support vector machines
106,Skull Segmentation in MRI by a Support Vector Machine Combining Local and Global Features,J. Sjölund; A. E. Järlideni; M. Andersson; H. Knutsson; H. Nordström,"Magnetic resonance (MR) images lack information about radiation transport-a fact which is problematic in applications such as radiotherapy planning and attenuation correction in combined PET/MR imaging. To remedy this, a crude but common approach is to approximate all tissue properties as equivalent to those of water. We improve upon this using an algorithm that automatically identifies bone tissue in MR. More specifically, we focus on segmenting the skull prior to stereotactic neurosurgery, where it is common that only MR images are available. In the proposed approach, a machine learning algorithm known as a support vector machine is trained on patients for which both a CT and an MR scan are available. As input, a combination of local and global information is used. The latter is needed to distinguish between bone and air as this is not possible based only on the local image intensity. A whole skull segmentation is achievable in minutes. In a comparison with two other methods, one based on mathematical morphology and the other on deformable registration, the proposed method was found to yield consistently better segmentations.",4.0,2014,10.1109/ICPR.2014.564,bones
107,A Deep Residual Learning Network for Practical Voxel Dosimetry in Radionuclide Therapy,Z. Li; J. A. Fessler; J. K. Mikell; S. J. Wilderman; Y. K. Dewaraja,"Current standard methods for voxel-level dosimetry in radionuclide therapy suffers from a tradeoff between accuracy and computational efficiency. Monte Carlo (MC) radiation transport algorithms are considered as the gold standard, but are associated with long computation time, while fast voxel dose kernel (VDK) based methods can be inaccurate in the presence of tissue density heterogeneities. This paper investigates a deep residual Convolutional Neural Networks (CNN) approach that learns the difference between the MC and the VDK dose-rate maps to address the speed-accuracy trade-off issue. As with MC and VDK-based dosimetry, the input to the CNN was the patient's SPECT activity map and CT-based density map. MC dosimetry was used only during the training process to generate ground truth training labels. Furthermore, to potentially account for the degradation of dose-rate maps due to poor SPECT spatial resolution, we trained the CNN using dose-rate maps directly corresponding to phantom activity/density maps that were generated from patient's PET scans. The test data consisted of phantom simulations and one patient who underwent 177Lu DOTATATE therapy for neuroendocrine tumors. In phantom cases, the lesion/organ mean dose-rates from ground truth (GT) agreed better with the CNN dose-rates compared to VDK with density scaling, with an average of 60% improvement for lesions and 55%, 63% improvement for left/right kidney, respectively. For all regions, the normalized root mean square error (NRMSE) relative to GT was substantially lower with CNN than with VDK and MC, i.e., an average of 23%, 22% improvement for lesion, respectively. Using a GPU, the CNN took only about 2.0 seconds to generate a patient's 512×512×130 absorbed dose-rate map while the same calculation took about 40 minutes using our fast in-house Dose Planning Method (DPM) MC algorithm that runs on a CPU. In conclusion, the proposed CNN approach demonstrated consistently higher accuracy than VDK-density scaling and comparable accuracy versus MC and is fast enough to be used clinically.",,2020,10.1109/NSS/MIC42677.2020.9507764,dosimetry
107,A Deep Residual Learning Network for Practical Voxel Dosimetry in Radionuclide Therapy,Z. Li; J. A. Fessler; J. K. Mikell; S. J. Wilderman; Y. K. Dewaraja,"Current standard methods for voxel-level dosimetry in radionuclide therapy suffers from a tradeoff between accuracy and computational efficiency. Monte Carlo (MC) radiation transport algorithms are considered as the gold standard, but are associated with long computation time, while fast voxel dose kernel (VDK) based methods can be inaccurate in the presence of tissue density heterogeneities. This paper investigates a deep residual Convolutional Neural Networks (CNN) approach that learns the difference between the MC and the VDK dose-rate maps to address the speed-accuracy trade-off issue. As with MC and VDK-based dosimetry, the input to the CNN was the patient's SPECT activity map and CT-based density map. MC dosimetry was used only during the training process to generate ground truth training labels. Furthermore, to potentially account for the degradation of dose-rate maps due to poor SPECT spatial resolution, we trained the CNN using dose-rate maps directly corresponding to phantom activity/density maps that were generated from patient's PET scans. The test data consisted of phantom simulations and one patient who underwent 177Lu DOTATATE therapy for neuroendocrine tumors. In phantom cases, the lesion/organ mean dose-rates from ground truth (GT) agreed better with the CNN dose-rates compared to VDK with density scaling, with an average of 60% improvement for lesions and 55%, 63% improvement for left/right kidney, respectively. For all regions, the normalized root mean square error (NRMSE) relative to GT was substantially lower with CNN than with VDK and MC, i.e., an average of 23%, 22% improvement for lesion, respectively. Using a GPU, the CNN took only about 2.0 seconds to generate a patient's 512×512×130 absorbed dose-rate map while the same calculation took about 40 minutes using our fast in-house Dose Planning Method (DPM) MC algorithm that runs on a CPU. In conclusion, the proposed CNN approach demonstrated consistently higher accuracy than VDK-density scaling and comparable accuracy versus MC and is fast enough to be used clinically.",,2020,10.1109/NSS/MIC42677.2020.9507764,phantoms
107,A Deep Residual Learning Network for Practical Voxel Dosimetry in Radionuclide Therapy,Z. Li; J. A. Fessler; J. K. Mikell; S. J. Wilderman; Y. K. Dewaraja,"Current standard methods for voxel-level dosimetry in radionuclide therapy suffers from a tradeoff between accuracy and computational efficiency. Monte Carlo (MC) radiation transport algorithms are considered as the gold standard, but are associated with long computation time, while fast voxel dose kernel (VDK) based methods can be inaccurate in the presence of tissue density heterogeneities. This paper investigates a deep residual Convolutional Neural Networks (CNN) approach that learns the difference between the MC and the VDK dose-rate maps to address the speed-accuracy trade-off issue. As with MC and VDK-based dosimetry, the input to the CNN was the patient's SPECT activity map and CT-based density map. MC dosimetry was used only during the training process to generate ground truth training labels. Furthermore, to potentially account for the degradation of dose-rate maps due to poor SPECT spatial resolution, we trained the CNN using dose-rate maps directly corresponding to phantom activity/density maps that were generated from patient's PET scans. The test data consisted of phantom simulations and one patient who underwent 177Lu DOTATATE therapy for neuroendocrine tumors. In phantom cases, the lesion/organ mean dose-rates from ground truth (GT) agreed better with the CNN dose-rates compared to VDK with density scaling, with an average of 60% improvement for lesions and 55%, 63% improvement for left/right kidney, respectively. For all regions, the normalized root mean square error (NRMSE) relative to GT was substantially lower with CNN than with VDK and MC, i.e., an average of 23%, 22% improvement for lesion, respectively. Using a GPU, the CNN took only about 2.0 seconds to generate a patient's 512×512×130 absorbed dose-rate map while the same calculation took about 40 minutes using our fast in-house Dose Planning Method (DPM) MC algorithm that runs on a CPU. In conclusion, the proposed CNN approach demonstrated consistently higher accuracy than VDK-density scaling and comparable accuracy versus MC and is fast enough to be used clinically.",,2020,10.1109/NSS/MIC42677.2020.9507764,degradation
107,A Deep Residual Learning Network for Practical Voxel Dosimetry in Radionuclide Therapy,Z. Li; J. A. Fessler; J. K. Mikell; S. J. Wilderman; Y. K. Dewaraja,"Current standard methods for voxel-level dosimetry in radionuclide therapy suffers from a tradeoff between accuracy and computational efficiency. Monte Carlo (MC) radiation transport algorithms are considered as the gold standard, but are associated with long computation time, while fast voxel dose kernel (VDK) based methods can be inaccurate in the presence of tissue density heterogeneities. This paper investigates a deep residual Convolutional Neural Networks (CNN) approach that learns the difference between the MC and the VDK dose-rate maps to address the speed-accuracy trade-off issue. As with MC and VDK-based dosimetry, the input to the CNN was the patient's SPECT activity map and CT-based density map. MC dosimetry was used only during the training process to generate ground truth training labels. Furthermore, to potentially account for the degradation of dose-rate maps due to poor SPECT spatial resolution, we trained the CNN using dose-rate maps directly corresponding to phantom activity/density maps that were generated from patient's PET scans. The test data consisted of phantom simulations and one patient who underwent 177Lu DOTATATE therapy for neuroendocrine tumors. In phantom cases, the lesion/organ mean dose-rates from ground truth (GT) agreed better with the CNN dose-rates compared to VDK with density scaling, with an average of 60% improvement for lesions and 55%, 63% improvement for left/right kidney, respectively. For all regions, the normalized root mean square error (NRMSE) relative to GT was substantially lower with CNN than with VDK and MC, i.e., an average of 23%, 22% improvement for lesion, respectively. Using a GPU, the CNN took only about 2.0 seconds to generate a patient's 512×512×130 absorbed dose-rate map while the same calculation took about 40 minutes using our fast in-house Dose Planning Method (DPM) MC algorithm that runs on a CPU. In conclusion, the proposed CNN approach demonstrated consistently higher accuracy than VDK-density scaling and comparable accuracy versus MC and is fast enough to be used clinically.",,2020,10.1109/NSS/MIC42677.2020.9507764,graphics processing units
107,A Deep Residual Learning Network for Practical Voxel Dosimetry in Radionuclide Therapy,Z. Li; J. A. Fessler; J. K. Mikell; S. J. Wilderman; Y. K. Dewaraja,"Current standard methods for voxel-level dosimetry in radionuclide therapy suffers from a tradeoff between accuracy and computational efficiency. Monte Carlo (MC) radiation transport algorithms are considered as the gold standard, but are associated with long computation time, while fast voxel dose kernel (VDK) based methods can be inaccurate in the presence of tissue density heterogeneities. This paper investigates a deep residual Convolutional Neural Networks (CNN) approach that learns the difference between the MC and the VDK dose-rate maps to address the speed-accuracy trade-off issue. As with MC and VDK-based dosimetry, the input to the CNN was the patient's SPECT activity map and CT-based density map. MC dosimetry was used only during the training process to generate ground truth training labels. Furthermore, to potentially account for the degradation of dose-rate maps due to poor SPECT spatial resolution, we trained the CNN using dose-rate maps directly corresponding to phantom activity/density maps that were generated from patient's PET scans. The test data consisted of phantom simulations and one patient who underwent 177Lu DOTATATE therapy for neuroendocrine tumors. In phantom cases, the lesion/organ mean dose-rates from ground truth (GT) agreed better with the CNN dose-rates compared to VDK with density scaling, with an average of 60% improvement for lesions and 55%, 63% improvement for left/right kidney, respectively. For all regions, the normalized root mean square error (NRMSE) relative to GT was substantially lower with CNN than with VDK and MC, i.e., an average of 23%, 22% improvement for lesion, respectively. Using a GPU, the CNN took only about 2.0 seconds to generate a patient's 512×512×130 absorbed dose-rate map while the same calculation took about 40 minutes using our fast in-house Dose Planning Method (DPM) MC algorithm that runs on a CPU. In conclusion, the proposed CNN approach demonstrated consistently higher accuracy than VDK-density scaling and comparable accuracy versus MC and is fast enough to be used clinically.",,2020,10.1109/NSS/MIC42677.2020.9507764,medical treatment
108,CMIM: Cross-Modal Information Maximization For Medical Imaging,T. Sylvain; F. Dutil; T. Berthier; L. Di Jorio; M. Luck; D. Hjelm; Y. Bengio,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",,2021,10.1109/ICASSP39728.2021.9414132,segmentation
108,CMIM: Cross-Modal Information Maximization For Medical Imaging,T. Sylvain; F. Dutil; T. Berthier; L. Di Jorio; M. Luck; D. Hjelm; Y. Bengio,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",,2021,10.1109/ICASSP39728.2021.9414132,adaptation models
108,CMIM: Cross-Modal Information Maximization For Medical Imaging,T. Sylvain; F. Dutil; T. Berthier; L. Di Jorio; M. Luck; D. Hjelm; Y. Bengio,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",,2021,10.1109/ICASSP39728.2021.9414132,multimodal data
108,CMIM: Cross-Modal Information Maximization For Medical Imaging,T. Sylvain; F. Dutil; T. Berthier; L. Di Jorio; M. Luck; D. Hjelm; Y. Bengio,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",,2021,10.1109/ICASSP39728.2021.9414132,task analysis
108,CMIM: Cross-Modal Information Maximization For Medical Imaging,T. Sylvain; F. Dutil; T. Berthier; L. Di Jorio; M. Luck; D. Hjelm; Y. Bengio,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time.In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.",,2021,10.1109/ICASSP39728.2021.9414132,signal processing
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,white matter
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,hyperparamter
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,brain tumor
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,optimization
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,three-dimensional displays
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,medical decathlon
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,loss function
109,Brain Tumor Segmentation using 3D U-Net with Hyperparameter Optimization,A. Gamal; K. Bedda; N. Ashraf; S. Ayman; M. AbdAllah; M. A. Rushdi,"For the sake of proper diagnosis and treatment, accurate brain tumour segmentation is required. Because manual brain tumour segmentation is a time-consuming, costly, and subjective task, effective automated approaches for this purpose are generally desired. However, because brain tumours vary greatly in terms of location, shape, and size, establishing automatic segmentation algorithms has remained challenging throughout the years. Automatic segmentation of brain tumour is the process of separating abnormal tissues from normal tissues, such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Brian segmentation needs typically to be carried out for different image modalities in order to reveal important metabolic and physiological information. These modalities include positron emission tomography (PET), computer tomography (CT) image and magnetic resonance image (MRI). Multimodal imaging techniques (such as PET/CT and PET/MRI) that combine the information from multiple imaging modalities contribute more for accurate brain tumour segmentation. In this work, we introduce a deep learning framework for automated segmentation of 3D brain tumors that can save physicians time and provide an accurate reproducible solution for further tumor analysis and monitoring. In particular, a 3D U-Net was trained on brain MRI data obtained from the 2018 Brain tumor Image Segmentation (BraTS) challenge. Three optimizers (RMSProp, Adam and Nadam) and three loss functions (Dice loss, focal Tversky loss, Log-Cosh loss functions) were used. We demonstrated that some loss functions and optimizers combinations perform better than other ones. For example, using the Log-Cosh loss function along with RMSProp optimizer resulted in the highest Dice coefficient, 0.75. Indeed, we also optimized the network hyperparameters in order to enhance the segmentation outcomes. These results demonstrate the feasibility and effectiveness of the proposed deep learning scheme with optimized hyperparemeters and appropriate selection of the optimizer and loss function.",,2021,10.1109/NILES53778.2021.9600556,3d u-net
110,MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network,J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang,"In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.",9.0,2020,10.1109/ACCESS.2020.2982016,unified method
110,MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network,J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang,"In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.",9.0,2020,10.1109/ACCESS.2020.2982016,end-to-end
110,MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network,J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang,"In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.",9.0,2020,10.1109/ACCESS.2020.2982016,generative adversarial networks
110,MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network,J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang,"In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.",9.0,2020,10.1109/ACCESS.2020.2982016,generative adversarial network
110,MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network,J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang,"In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.",9.0,2020,10.1109/ACCESS.2020.2982016,different resolutions
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,prostate cancer
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,fusion of multimodal medical data
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,big medical data decision-making model
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,machine-assisted diagnosis
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,fuzzy logic
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,fuzzy inference logic
111,Big Medical Data Decision-Making Intelligent System Exploiting Fuzzy Inference Logic for Prostate Cancer in Developing Countries,K. Liu; Z. Chen; J. Wu; Y. Tan; L. Wang; Y. Yan; H. Zhang; J. Long,"In most developing countries, it has become a severe challenge for the limited medical resources and outdated healthcare technology to meet the high demand of large population. From the perspective of social development, this unbalanced healthcare system in developing counties has also exacerbated the contradiction between physicians and patients, particularly those suffering from malignant diseases (such as prostate cancer). Rapid improvements in artificial intelligence, computing power, parallel operation, and data storage management have contributed significantly to a credible medical data decision-making on the detection, diagnosis, treatment, and prognosis of malignant diseases. Consequently, to address these existing problems in the current healthcare field of developing countries, this paper proposes a novel big medical data decision-making model exploiting fuzzy inference logic for prostate cancer in developing countries, constructing an intelligent medical system for disease detection, medical data analysis and fusion, treatment recommendations, and risk management. Based on 1 933 535 items of hospitalization information from over 8000 prostate cancer cases in China, the experimental results demonstrate that the intelligent medical system could be adopted to assist physicians and medical specialists in coming up with a more dependable diagnosis scheme.",13.0,2019,10.1109/ACCESS.2018.2886198,intelligent medical system
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,mutual information
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,gold
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,information theory
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,magnetic resonance
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,radioactive decay
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,robustness
112,f-information measures in medical image registration,J. P. W. Pluim; J. B. A. Maintz; M. A. Viergever,"A measure for registration of medical images that currently draws much attention is mutual information. The measure originates from information theory, but has been shown to be successful for image registration as well. Information theory, however, offers many more measures that may be suitable for image registration. These all measure the divergence of the joint distribution of the images' grey values from the joint distribution that would have been found had the images been completely independent. This paper compares the performance of mutual information as a registration measure with that of other f-information measures. The measures are applied to rigid registration of positron emission tomography(PET)/magnetic resonance (MR) and MR/computed tomography (CT) images, for 35 and 41 image pairs, respectively. An accurate gold standard transformation is available for the images, based on implanted markers. The registration performance, robustness and accuracy of the measures are studied. Some of the measures are shown to perform poorly on all aspects. The majority of measures produces results similar to those of mutual information. An important finding, however, is that several measures, although slightly more difficult to optimize, can potentially yield significantly more accurate results than mutual information.",95.0,2004,10.1109/TMI.2004.836872,current measurement
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,support vector machines
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,health care
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,ehr
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,technological innovation
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,hospitals
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,prediction
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,systematics
113,Machine Learning based Prediction Model for Health Care Sector - A Survey,S. K; S. Sarathambekai,"Machine Learning is the subset of artificial intelligence where the machines are programmed to learn without any human intervention. The hidden complex patterns inside the data can be extracted with the help of machine learning algorithms. Health care industry can generate, store and analyze huge heterogeneous data like CT scan, MRI, fMRI, PET, SPECT, DTI, DOT etc. Apart from these data hospitals is responsible for various sources include hospital administrative records, medical records of patients, results of medical examinations, and data generated by the devices. Dealing with this type of multi-dimensional data manually is the challenging task and it may lead to reduce the prediction accuracy which will affect directly to the life of the patient. Hence we are in need of proper smart data management and analysis mechanism for deriving the accurate and meaningful information. Machine learning can play vital role in modern health care industry by providing the relevant solutions in less time with high accuracy. It also provides systematic and algorithmic approach tools for data management, analysis and interpretation. This paper presents the state-of-the -art of works related to machine learning techniques in health care sector.",,2021,10.1109/i-PACT52855.2021.9696646,industries
114,Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",7.0,2009,10.1109/CVPR.2009.5206840,mutual information
114,Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",7.0,2009,10.1109/CVPR.2009.5206840,bones
114,Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",7.0,2009,10.1109/CVPR.2009.5206840,histograms
114,Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",7.0,2009,10.1109/CVPR.2009.5206840,pixel
114,Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",7.0,2009,10.1109/CVPR.2009.5206840,biology
114,Learning similarity measure for multi-modal 3D image registration,Daewon Lee; M. Hofmann; F. Steinke; Y. Altun; N. D. Cahill; B. Scholkopf,"Multi-modal image registration is a challenging problem in medical imaging. The goal is to align anatomically identical structures; however, their appearance in images acquired with different imaging devices, such as CT or MR, may be very different. Registration algorithms generally deform one image, the floating image, such that it matches with a second, the reference image, by maximizing some similarity score between the deformed and the reference image. Instead of using a universal, but a priori fixed similarity criterion such as mutual information, we propose learning a similarity measure in a discriminative manner such that the reference and correctly deformed floating images receive high similarity scores. To this end, we develop an algorithm derived from max-margin structured output learning, and employ the learned similarity measure within a standard rigid registration algorithm. Compared to other approaches, our method adapts to the specific registration problem at hand and exploits correlations between neighboring pixels in the reference and the floating image. Empirical evaluation on CT-MR/PET-MR rigid registration tasks demonstrates that our approach yields robust performance and outperforms the state of the art methods for multi-modal medical image registration.",7.0,2009,10.1109/CVPR.2009.5206840,cybernetics
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,kidney
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,heart
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,biomarkers
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,retina
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,diseases
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,health risk factors
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,cerebrovascular diseases
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,stroke (medical condition)
115,A Review: Prediction of Multiple Adverse Health Conditions from Retinal Images,L. K. Pampana; M. S. Rayudu,"During the recent years, lifestyle related diseases and disorders such as stress, hypertension and diabetes are increasing at a rapid rate in the middle aged population also These disorders may have greater likelihood of developing multiple adverse health conditions like cardiovascular strokes, cerebrovascular strokes, kidney failures, depression etc. Preventive diagnosis measures are required to diagnose these adverse health hazards in the middle aged group. These days the health care sector is equipped with sophisticated instruments to diagnose the abnormalities in specific organs using different modalities like Computer Tomography(CT), Magnetic Resonance Imaging(MRI), Positron Emission Tomography(PET), Ultrasonography scans, X-Ray, etc. Retinal vascular imaging has its popularity in diagnosing several ocular diseases viz, Diabetic Retinopathy(DR), Age related Macular Degeneration(AMD), Edema, Glaucoma, etc using the latest advancements in Artificial Intelligence with the aid of modalities like Retinal Fundoscopy, Optical Coherence Tomography(OCT), confocal scanning laser ophthalmoscope (cSLO). As per the clinical based studies, the retina shares similar physiological and anatomical features with vital organs hence it is million worthwhile to say that retinal vascular imaging could predict the multiple adverse health conditions like Cardiovascular(CVD), Cerebrovascular(CVS), Chronicle Kidney Diseases(CKD), Breast Cancer and Pulmonary Diseases. The main objective of this review is to study and present the retinal associations related to these life threat adverse diseases., by considering sources from various clinical based studies. The review is concluded by addressing the potential and candidate retinal biomarkers for each of these diseases.",2.0,2020,10.1109/B-HTC50970.2020.9297936,cardiovascular diseases
116,Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks,F. Munawar; S. Azmat; T. Iqbal; C. Grönlund; H. Ali,"Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",4.0,2020,10.1109/ACCESS.2020.3017915,generators
116,Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks,F. Munawar; S. Azmat; T. Iqbal; C. Grönlund; H. Ali,"Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",4.0,2020,10.1109/ACCESS.2020.3017915,generative adversarial networks
116,Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks,F. Munawar; S. Azmat; T. Iqbal; C. Grönlund; H. Ali,"Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",4.0,2020,10.1109/ACCESS.2020.3017915,lung segmentation
116,Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks,F. Munawar; S. Azmat; T. Iqbal; C. Grönlund; H. Ali,"Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",4.0,2020,10.1109/ACCESS.2020.3017915,diseases
116,Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks,F. Munawar; S. Azmat; T. Iqbal; C. Grönlund; H. Ali,"Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",4.0,2020,10.1109/ACCESS.2020.3017915,lung
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,generalized clonic-tonic)
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,measurement
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,prediction algorithms
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,epilepsy
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,brain
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,surgery
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,neurological diseases
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,feature extraction
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,es prediction
117,Epileptic Seizure Prediction: A Review,K. P. Kamini; R. Arthi,"Epilepsy is one of the most familiar form of neurological diseases in the globe. Epilepsy is a central nervous system ie neuro logical disorder in which brain activity shows some unusual behavior triggering seizures , sensations and occasionally consciousness loss. Seizures in epilepsy are caused by some disorders in the brain electrical activity. The epilepsy seizure (ES) can be correlated to a brain damage or a family tendency, then most of the time period the reason is unpredictable. Many treatments and therapies are also available to support the epileptic patients to become as seizure-free, with the help of medication, anti-seizure devices and surgery. There are several modalities available to find out epileptic seizure such as EEG, MRI, CT, PET and also SPECT. Initial prediction and notice of seizures can deliver appropriate treatment and aid for patients and also improves the excellence of life. In this paper, a complete analysis of ES prediction by using different methods of machine learning techniques will be reviewed.",,2021,10.1109/SMART52563.2021.9676229,different seizure (partial
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,unsupervised learning
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,"nonnegative matrix tri-factorization,radiomics"
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,sparsity
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,collaboration
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,radiomics
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,feature extraction
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,cancer
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,collaborative clustering
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,dimensionality reduction
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,matrix decomposition
118,Robust Collaborative Clustering of Subjects and Radiomic Features for Cancer Prognosis,H. Liu; H. Li; M. Habes; Y. Li; P. Boimel; J. Janopaul-Naylor; Y. Xiao; E. Ben-Josef; Y. Fan,"Feature dimensionality reduction plays an important role in radiomic studies with a large number of features. However, conventional radiomic approaches may suffer from noise, and feature dimensionality reduction techniques are not equipped to utilize latent supervision information of patient data under study, such as differences in patients, to learn discriminative low dimensional representations. To achieve robustness to noise and feature dimensionality reduction with improved discriminative power, we develop a robust collaborative clustering method to simultaneously cluster patients and radiomic features into distinct groups respectively under adaptive sparse regularization. Our method is built upon matrix tri-factorization enhanced by adaptive sparsity regularization for simultaneous feature dimensionality reduction and denoising. Particularly, latent grouping information of patients with distinct radiomic features is learned and utilized as supervision information to guide the feature dimensionality reduction, and noise in radiomic features is adaptively isolated in a Bayesian framework under a general assumption of Laplacian distributions of transform-domain coefficients. Experiments on synthetic data have demonstrated the effectiveness of the proposed approach in data clustering, and evaluation results on an FDG-PET/CT dataset of rectal cancer patients have demonstrated that the proposed method outperforms alternative methods in terms of both patient stratification and prediction of patient clinical outcomes.",4.0,2020,10.1109/TBME.2020.2969839,dictionaries
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,proportional hazards model
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,tumor recurrence
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,rectal cancer
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,survival analysis
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,hazards
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,cnns
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,tumors
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,cancer
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,feature extraction
119,Deep Convolutional Neural Networks For Imaging Data Based Survival Analysis Of Rectal Cancer,H. Li; P. Boimel; J. Janopaul-Naylor; H. Zhong; Y. Xiao; E. Ben-Josef; Y. Fan,"Recent radiomic studies have witnessed promising performance of deep learning techniques in learning radiomic features and fusing multimodal imaging data. Most existing deep learning based radiomic studies build predictive models in a setting of pattern classification, not appropriate for survival analysis studies where some data samples have incomplete observations. To improve existing survival analysis techniques whose performance is hinged on imaging features, we propose a deep learning method to build survival regression models by optimizing imaging features with deep convolutional neural networks (CNNs) in a proportional hazards model. To make the CNNs applicable to tumors with varied sizes, a spatial pyramid pooling strategy is adopted. Our method has been validated based on a simulated imaging dataset and a FDG-PET/CT dataset of rectal cancer patients treated for locally advanced rectal cancer. Compared with survival prediction models built upon hand-crafted radiomic features using Cox proportional hazards model and random survival forests, our method achieved competitive prediction performance.",15.0,2019,10.1109/ISBI.2019.8759301,predictive models
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,melanoma
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,hough circle
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,eye tumour
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,iris
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,cnn
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,cameras
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,cancer
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,lenet
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,ocular melanoma
120,Eye Tumour Detection Using Deep Learning,A. Sinha; A. R P; N. N. S,"Iris melanocytic tumours, are the most dangerous tumours in the eye , commonly known as eye tumours. This includes freckle, nevus, melanocytoma, Lisch nodule, and melanoma. The detection of eye tumour is very difficult in early stages. Many research works are being carried out to detect eye diseases. But few research works in the eye tumour were published. Most of the system needs specific data acquisition devices to capture the region. This is very expensive. To diagnose eye melanoma, doctors recommend PET - CT, eye ultrasound, angiogram, optical coherence tomography, etc. Here, a new approach is presented to detect the eye tumour from eye images using deep learning technique. The deep network model created with modified LeNet architecture. The model created with the segmented eyeball images. Hough circle transformation could predict the eyeball and iris regions. As the deep learning technique needs more data for training, the number of image data has been increased with image augmentation method. Successful testing of this method with an accuracy of 95% shows that this method can be implemented in real time applications.",,2021,10.1109/ICBSII51839.2021.9445172,grayscale conversion
121,Towards Extreme-Resolution Image Registration with Deep Learning,A. Nazib; C. Fookes; D. Perrin,"Image registration plays an important role in comparing images. It is particularly important in analysing medical images like CT, MRI and PET, to quantify different biological samples, to monitor disease progression, and to fuse different modalities to support better diagnosis. The recent emergence of tissue clearing protocols enable us to take images at cellular level resolution. Image registration tools developed for other modalities are currently unable to manage images of entire organs at such resolution. The popularity of deep learning based methods in the computer vision community justifies a rigorous investigation of deep-learning based methods on tissue cleared images along with their traditional counterparts. In this paper, we investigate and compare the performance of a deep learning based registration method with traditional optimization based methods on samples from tissue-clearing methods. From the comparative results it is found that a deep-learning based method outperforms all traditional registration tools in terms of registration time and has achieved promising registration accuracy.",1.0,2019,10.1109/ISBI.2019.8759291,optimization
121,Towards Extreme-Resolution Image Registration with Deep Learning,A. Nazib; C. Fookes; D. Perrin,"Image registration plays an important role in comparing images. It is particularly important in analysing medical images like CT, MRI and PET, to quantify different biological samples, to monitor disease progression, and to fuse different modalities to support better diagnosis. The recent emergence of tissue clearing protocols enable us to take images at cellular level resolution. Image registration tools developed for other modalities are currently unable to manage images of entire organs at such resolution. The popularity of deep learning based methods in the computer vision community justifies a rigorous investigation of deep-learning based methods on tissue cleared images along with their traditional counterparts. In this paper, we investigate and compare the performance of a deep learning based registration method with traditional optimization based methods on samples from tissue-clearing methods. From the comparative results it is found that a deep-learning based method outperforms all traditional registration tools in terms of registration time and has achieved promising registration accuracy.",1.0,2019,10.1109/ISBI.2019.8759291,tools
121,Towards Extreme-Resolution Image Registration with Deep Learning,A. Nazib; C. Fookes; D. Perrin,"Image registration plays an important role in comparing images. It is particularly important in analysing medical images like CT, MRI and PET, to quantify different biological samples, to monitor disease progression, and to fuse different modalities to support better diagnosis. The recent emergence of tissue clearing protocols enable us to take images at cellular level resolution. Image registration tools developed for other modalities are currently unable to manage images of entire organs at such resolution. The popularity of deep learning based methods in the computer vision community justifies a rigorous investigation of deep-learning based methods on tissue cleared images along with their traditional counterparts. In this paper, we investigate and compare the performance of a deep learning based registration method with traditional optimization based methods on samples from tissue-clearing methods. From the comparative results it is found that a deep-learning based method outperforms all traditional registration tools in terms of registration time and has achieved promising registration accuracy.",1.0,2019,10.1109/ISBI.2019.8759291,tissue clearing
122,Guest Editorial Generative Adversarial Networks in Biomedical Image Computing,H. Fu; T. Zhou; S. Li; A. F. Frangi,"The papers in this special section focus on generative adversarial networks in biomedical image computing. The field of biomedical imaging has obtained great progress from Roentgen’s original discovery of the X-ray to the current imaging tools, including Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound (US). The benefits of using these non-invasive imaging technologies are to assess the current condition of an organ or tissue, which can be used to monitor a patient over time over time for accurate and timely diagnosis and treatment.With the development of imaging technologies, developing advanced artificial intelligence algorithms for automated image analysis has shown the potential to change many aspects of clinical applications within the next decade. Meanwhile, these advanced technologies have also brought new issues and challenges. Thus, there has been a growing demand for biomedical imaging computing to be a component of clinical trials and device improvement. Currently, Generative adversarial networks (GANs) have been attached growing interests in the computer vision community due to their capability of data generation or translation. GAN-based models are able to learn from a set of training data and generate new data with the same characteristics as the training ones, which have also proven to be the state of the art for generating sharp and realistic images. More importantly, GAN has been rapidly applied to many traditional and novel applications in the medical domain, such as image reconstruction, segmentation, diagnosis, synthesis, and so on. Despite GAN substantial progress in these areas, their application to medical image computing still faces challenges and unsolved problems remain.",,2022,10.1109/JBHI.2021.3134004,generative adversarial networks
122,Guest Editorial Generative Adversarial Networks in Biomedical Image Computing,H. Fu; T. Zhou; S. Li; A. F. Frangi,"The papers in this special section focus on generative adversarial networks in biomedical image computing. The field of biomedical imaging has obtained great progress from Roentgen’s original discovery of the X-ray to the current imaging tools, including Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound (US). The benefits of using these non-invasive imaging technologies are to assess the current condition of an organ or tissue, which can be used to monitor a patient over time over time for accurate and timely diagnosis and treatment.With the development of imaging technologies, developing advanced artificial intelligence algorithms for automated image analysis has shown the potential to change many aspects of clinical applications within the next decade. Meanwhile, these advanced technologies have also brought new issues and challenges. Thus, there has been a growing demand for biomedical imaging computing to be a component of clinical trials and device improvement. Currently, Generative adversarial networks (GANs) have been attached growing interests in the computer vision community due to their capability of data generation or translation. GAN-based models are able to learn from a set of training data and generate new data with the same characteristics as the training ones, which have also proven to be the state of the art for generating sharp and realistic images. More importantly, GAN has been rapidly applied to many traditional and novel applications in the medical domain, such as image reconstruction, segmentation, diagnosis, synthesis, and so on. Despite GAN substantial progress in these areas, their application to medical image computing still faces challenges and unsolved problems remain.",,2022,10.1109/JBHI.2021.3134004,feature extraction
122,Guest Editorial Generative Adversarial Networks in Biomedical Image Computing,H. Fu; T. Zhou; S. Li; A. F. Frangi,"The papers in this special section focus on generative adversarial networks in biomedical image computing. The field of biomedical imaging has obtained great progress from Roentgen’s original discovery of the X-ray to the current imaging tools, including Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound (US). The benefits of using these non-invasive imaging technologies are to assess the current condition of an organ or tissue, which can be used to monitor a patient over time over time for accurate and timely diagnosis and treatment.With the development of imaging technologies, developing advanced artificial intelligence algorithms for automated image analysis has shown the potential to change many aspects of clinical applications within the next decade. Meanwhile, these advanced technologies have also brought new issues and challenges. Thus, there has been a growing demand for biomedical imaging computing to be a component of clinical trials and device improvement. Currently, Generative adversarial networks (GANs) have been attached growing interests in the computer vision community due to their capability of data generation or translation. GAN-based models are able to learn from a set of training data and generate new data with the same characteristics as the training ones, which have also proven to be the state of the art for generating sharp and realistic images. More importantly, GAN has been rapidly applied to many traditional and novel applications in the medical domain, such as image reconstruction, segmentation, diagnosis, synthesis, and so on. Despite GAN substantial progress in these areas, their application to medical image computing still faces challenges and unsolved problems remain.",,2022,10.1109/JBHI.2021.3134004,special issues and sections
123,IEEE Access Special Section Editorial: Deep Learning for Computer-Aided Medical Diagnosis,Y. -D. Zhang; Z. Dong; S. -H. Wang; C. Cattani,"As neuroimaging scanners grow in popularity in hospitals and institutes, the tasks of radiologists are increasing. Emotion, fatigue, and other factors may influence the manual interpretation of results. This manual interpretation suffers from inter- and intra-radiologist variance. Computer-aided medical diagnosis (CAMD) are procedures in medicine that assist radiologists and doctors in the interpretation of medical images, which may come from CT, X-ray, ultrasound, thermography, MRI, PET, SPECT, etc. In practice, CAMD can help radiologists to interpret medical images within seconds. Conventional CAMD tools are built on top of handcrafted features. Recent progress on deep learning opens a new era in which features can be automatically built from a large amount of data. Many important medical projects were launched during the last decade (Human brain project, Blue brain project, Brain Initiative, etc.) that provide massive amounts of data. This emerging big medical data can support the use of deep learning.",1.0,2020,10.1109/ACCESS.2020.2996690,None
124,Table of contents,,The following topics are dealt with: imaging sensors and detectors; image analysis; image quality assessment; image segmentation; pattern recognition; image enhancement; MRI; CT; SPECT; PET; ET: microscopy and deep learning.,,2017,10.1109/IST.2017.8261437,None
